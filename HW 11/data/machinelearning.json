[
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "`treemind` is a powerful Python library designed to analyze gradient boosting models like `xgboost`, `lightgbm`, and `catboost`. It helps you uncover how features and their interactions influence predictions across specific intervals, offering fast, intuitive insights.\n\n### Key Features:\n- **Feature &amp; Interaction Analysis:** Understand feature contributions and complex interactions up to `n` features.\n- **Advanced Visualizations:** User-friendly plots to explain model decisions.\n- **High Performance:** Optimized with Cython for lightning-fast execution, even on large datasets.\n- **Easy Integration:** Seamlessly works with popular frameworks for regression and binary classification.\n\n### Algorithm &amp; Performance:\n- **Algorithm:** Focuses on analyzing feature contributions and interactions in tree-based models for meaningful interval-based insights. [Read more about the algorithm](https://treemind.readthedocs.io/en/latest/algorithm.html)\n- **Performance:** The library's performance has been tested on synthetic datasets, where it is benchmarked against SHAP for accuracy and efficiency. [View performance experiments](https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html)\n\n### Quick Start:\n```bash\npip install treemind\n```\n\nCheck out the full documentation for examples, visualizations, and API details.\n\n[GitHub Repo](https://github.com/sametcopur/treemind) | [Docs](https://treemind.readthedocs.io/)\n\n**Note:**  \nWhile the algorithm produces desirable results in practice, it currently lacks formal mathematical proof. We would greatly appreciate your feedback and ideas to help improve and validate the approach further!",
            "author_fullname": "t2_r3c0w369",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] treemind: Simplifying Gradient Boosting Model Analysis",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1gtjkci",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731866743.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;code&gt;treemind&lt;/code&gt; is a powerful Python library designed to analyze gradient boosting models like &lt;code&gt;xgboost&lt;/code&gt;, &lt;code&gt;lightgbm&lt;/code&gt;, and &lt;code&gt;catboost&lt;/code&gt;. It helps you uncover how features and their interactions influence predictions across specific intervals, offering fast, intuitive insights.&lt;/p&gt;\n\n&lt;h3&gt;Key Features:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Feature &amp;amp; Interaction Analysis:&lt;/strong&gt; Understand feature contributions and complex interactions up to &lt;code&gt;n&lt;/code&gt; features.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Advanced Visualizations:&lt;/strong&gt; User-friendly plots to explain model decisions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;High Performance:&lt;/strong&gt; Optimized with Cython for lightning-fast execution, even on large datasets.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Easy Integration:&lt;/strong&gt; Seamlessly works with popular frameworks for regression and binary classification.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Algorithm &amp;amp; Performance:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt; Focuses on analyzing feature contributions and interactions in tree-based models for meaningful interval-based insights. &lt;a href=\"https://treemind.readthedocs.io/en/latest/algorithm.html\"&gt;Read more about the algorithm&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; The library&amp;#39;s performance has been tested on synthetic datasets, where it is benchmarked against SHAP for accuracy and efficiency. &lt;a href=\"https://treemind.readthedocs.io/en/latest/experiments/experiment_main.html\"&gt;View performance experiments&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h3&gt;Quick Start:&lt;/h3&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npip install treemind\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Check out the full documentation for examples, visualizations, and API details.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/sametcopur/treemind\"&gt;GitHub Repo&lt;/a&gt; | &lt;a href=\"https://treemind.readthedocs.io/\"&gt;Docs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br/&gt;\nWhile the algorithm produces desirable results in practice, it currently lacks formal mathematical proof. We would greatly appreciate your feedback and ideas to help improve and validate the approach further!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtjkci",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "zedeleyici3401",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtjkci/r_treemind_simplifying_gradient_boosting_model/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731866743.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I was going through some of the papers of ICLR with moderate to high scores related to what I was interested in , I found them failrly incremental and was kind of surprised, for a major sub field, the quality of work was rather poor for a premier conference as this one . Ever since llms have come, i feel the quality and originality of papers (not all of course ) have dipped a bit. Am I alone in feeling this ?",
            "author_fullname": "t2_7d9aap79",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Quality of ICLR papers",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1gtjhge",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731866736.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731866544.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was going through some of the papers of ICLR with moderate to high scores related to what I was interested in , I found them failrly incremental and was kind of surprised, for a major sub field, the quality of work was rather poor for a premier conference as this one . Ever since llms have come, i feel the quality and originality of papers (not all of course ) have dipped a bit. Am I alone in feeling this ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtjhge",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Cool_Abbreviations_9",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtjhge/d_quality_of_iclr_papers/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731866544.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
            "author_fullname": "t2_6l4z3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Simple Questions Thread",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gtgnk8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731859220.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!&lt;/p&gt;\n\n&lt;p&gt;Thread will stay alive until next one so keep posting after the date in the title.&lt;/p&gt;\n\n&lt;p&gt;Thanks to everyone for answering questions in the previous thread!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": "new",
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtgnk8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AutoModerator",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": false,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/",
            "stickied": true,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731859220.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I've been thinking a lot about why language models were so big and how they could be smaller. I thought about how every human brain can't possibly contain the entirity of human knowledge. I believe humans roughly have something along the lines of a probability matrix of words X other words, but not every word X every word.\n\nIt occurred to me that we frequently define unusual words (low frequency, not often used words) using other existing words we know. Can we potentially have a language model which uses vectors for the highest frequency words only, and \"unusal words\" which dont have their own vectors, but instead reference existing vectors? This could drastically decrease the word X word matrix as common words consists of a much smaller subset of the language. Maybe such a model could dynamically move reference words into and out of primary vectors when retrained on text that is specific to niche topics.\n\nKnowing that I've never had an original thought, are there any other projects like this already?",
            "author_fullname": "t2_epae7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Small language models defining vocabulary using old vectors instead of new vectors",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gtenw8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.77,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731853561.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been thinking a lot about why language models were so big and how they could be smaller. I thought about how every human brain can&amp;#39;t possibly contain the entirity of human knowledge. I believe humans roughly have something along the lines of a probability matrix of words X other words, but not every word X every word.&lt;/p&gt;\n\n&lt;p&gt;It occurred to me that we frequently define unusual words (low frequency, not often used words) using other existing words we know. Can we potentially have a language model which uses vectors for the highest frequency words only, and &amp;quot;unusal words&amp;quot; which dont have their own vectors, but instead reference existing vectors? This could drastically decrease the word X word matrix as common words consists of a much smaller subset of the language. Maybe such a model could dynamically move reference words into and out of primary vectors when retrained on text that is specific to niche topics.&lt;/p&gt;\n\n&lt;p&gt;Knowing that I&amp;#39;ve never had an original thought, are there any other projects like this already?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtenw8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "meteoraln",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtenw8/d_small_language_models_defining_vocabulary_using/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731853561.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "It seems that in a lot of applications where semantic matching is be difficult, systems are designed to use an autoregressive model for the input sequence embedding (then perform a range of semantic search techniques). \n\nBut shouldn't a bidirectional model always out-perform an autoregressive model on this task theoretically? That would suggest it's ideal to use an optimised NLU-oriented model like DeBERTa-V3 (ie. fine tuned on domain data) for more accurate embeddings, thus better semantic search performance.\n\nAdditionally, is there much reporting on unified semantic search techniques? All of the implementations i've seen have been highly domain-specific/arbitrary.",
            "author_fullname": "t2_8iyisnav",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "NLU models vs autoregressive models for semantic search [R]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gtdpwu",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731850635.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It seems that in a lot of applications where semantic matching is be difficult, systems are designed to use an autoregressive model for the input sequence embedding (then perform a range of semantic search techniques). &lt;/p&gt;\n\n&lt;p&gt;But shouldn&amp;#39;t a bidirectional model always out-perform an autoregressive model on this task theoretically? That would suggest it&amp;#39;s ideal to use an optimised NLU-oriented model like DeBERTa-V3 (ie. fine tuned on domain data) for more accurate embeddings, thus better semantic search performance.&lt;/p&gt;\n\n&lt;p&gt;Additionally, is there much reporting on unified semantic search techniques? All of the implementations i&amp;#39;ve seen have been highly domain-specific/arbitrary.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtdpwu",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "SnooPeripherals5313",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtdpwu/nlu_models_vs_autoregressive_models_for_semantic/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731850635.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am coding a DCGAN to produce Brain MRI data, based on the BRATs 2020 dataset. As a sanity check, I am training on a SINGLE image with CONSTANT noise, to see if there are any inherent flaws in my design. The GAN seems to catch on the general pattern, but there is some sort of noise or distortion. You can see in the example below, that the generated image is not as sharp as the original.\n\n[original image](https://preview.redd.it/b7ejt2z4bg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=dac9ca113582d943c53a017f71095a43da813ff6)\n\n[lr 1e-4 1000 epochs ](https://preview.redd.it/3ow5a1j7bg1e1.png?width=127&amp;format=png&amp;auto=webp&amp;s=90dd13bdd48fe9558d730140bc1228cd0666cb85)\n\n[lr 2e-4 500 epochs](https://preview.redd.it/nwpgvdh9bg1e1.png?width=129&amp;format=png&amp;auto=webp&amp;s=0ae004f7cf112d0ac4aedbc47e057f6ca690750f)\n\n[after initialization](https://preview.redd.it/r7kbeewebg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=9fee55b4ff32c10544b809bf8a9cb93fed95276c)\n\nI see some cross like patterns on all of my images, so I believe there is something inherently wrong with my network that produces them. here is the code.\n\n\\`\\`\\`\n\n    class SimpleGenerator(nn.Module):\n        def __init__(self,out_channels =1,\n                     noise_dimension = 100 ,\n                     channels= 64         \n                     ):\n            super(SimpleGenerator, self).__init__()\n            self.noise_shape = (noise_dimension,1,1,1)\n            self.out_channels = out_channels \n            self.channels = channels\n            self.gen = nn.Sequential(\n                nn.ConvTranspose3d(self.noise_shape[0],  self.channels * 32, 4, 1, (1, 0, 1)),\n                nn.ReLU(),\n                self._block( self.channels * 32,  self.channels * 16, 5, 1, 0),\n                self._block( self.channels * 16,  self.channels * 8, 5, 1, 0),\n                self._block( self.channels * 8,  self.channels * 4, 4, 2, 1),\n                self._block( self.channels * 4,  self.channels * 2, 4, 2, 1),\n                self._block( self.channels * 2,  self.channels, 4, 2, 1),\n                nn.ConvTranspose3d( self.channels, self.out_channels, 4, 2, 1),\n                nn.Sigmoid()\n            )\n            \n        def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n            return nn.Sequential(\n           \n                nn.ConvTranspose3d(in_channels,out_channels,3,1,1,bias=False),\n                nn.InstanceNorm3d(out_channels),\n                nn.ReLU(),\n                \n                nn.ConvTranspose3d(out_channels,out_channels,kernel_size,stride,padding,bias=False),\n                nn.InstanceNorm3d(out_channels),\n                nn.ReLU()\n            )\n    \n        def forward(self, x,separate=False):\n            \n            x = self.gen(x)\n            return x\n\nNotes :\n\n1. I am using InstanceNorm Instead of batch norm as my images are 160 x192x160 they are too big so The gpu can't support batch\\_size &gt;1.\n2. The weird numbers you see in the kernel size, stride and padding are because I want to achieve the shape described above which is not a power of two. Could this be the reason?\n3. I have tried the \\_block method with 1 or 2 convolutions (we see the 2 version). Same result\n4. the discriminator is a mirror image of the generator. I won't provide the code to make the post short, but i can if someone believes it is needed.",
            "author_fullname": "t2_5bk655e8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Convolutional Generative Adversarial Networks Noise Patterns\n",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "b7ejt2z4bg1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 89,
                            "x": 108,
                            "u": "https://preview.redd.it/b7ejt2z4bg1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc2c282a029cbb59c66c4b25e805f412d2e7730"
                        }
                    ],
                    "s": {
                        "y": 109,
                        "x": 132,
                        "u": "https://preview.redd.it/b7ejt2z4bg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=dac9ca113582d943c53a017f71095a43da813ff6"
                    },
                    "id": "b7ejt2z4bg1e1"
                },
                "3ow5a1j7bg1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 89,
                            "x": 108,
                            "u": "https://preview.redd.it/3ow5a1j7bg1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e0c855cc12989d719dd93b625c900aae450ad9ab"
                        }
                    ],
                    "s": {
                        "y": 105,
                        "x": 127,
                        "u": "https://preview.redd.it/3ow5a1j7bg1e1.png?width=127&amp;format=png&amp;auto=webp&amp;s=90dd13bdd48fe9558d730140bc1228cd0666cb85"
                    },
                    "id": "3ow5a1j7bg1e1"
                },
                "nwpgvdh9bg1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 86,
                            "x": 108,
                            "u": "https://preview.redd.it/nwpgvdh9bg1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=050ab038f0282fef7f51bdfc6a133be83e5f1f31"
                        }
                    ],
                    "s": {
                        "y": 103,
                        "x": 129,
                        "u": "https://preview.redd.it/nwpgvdh9bg1e1.png?width=129&amp;format=png&amp;auto=webp&amp;s=0ae004f7cf112d0ac4aedbc47e057f6ca690750f"
                    },
                    "id": "nwpgvdh9bg1e1"
                },
                "r7kbeewebg1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 84,
                            "x": 108,
                            "u": "https://preview.redd.it/r7kbeewebg1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba54b92c5deb84b1896e7defbc77ebf0cc06617b"
                        }
                    ],
                    "s": {
                        "y": 103,
                        "x": 132,
                        "u": "https://preview.redd.it/r7kbeewebg1e1.png?width=132&amp;format=png&amp;auto=webp&amp;s=9fee55b4ff32c10544b809bf8a9cb93fed95276c"
                    },
                    "id": "r7kbeewebg1e1"
                }
            },
            "name": "t3_1gtc2qv",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.63,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731845094.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731844856.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am coding a DCGAN to produce Brain MRI data, based on the BRATs 2020 dataset. As a sanity check, I am training on a SINGLE image with CONSTANT noise, to see if there are any inherent flaws in my design. The GAN seems to catch on the general pattern, but there is some sort of noise or distortion. You can see in the example below, that the generated image is not as sharp as the original.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b7ejt2z4bg1e1.png?width=132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dac9ca113582d943c53a017f71095a43da813ff6\"&gt;original image&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/3ow5a1j7bg1e1.png?width=127&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90dd13bdd48fe9558d730140bc1228cd0666cb85\"&gt;lr 1e-4 1000 epochs &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/nwpgvdh9bg1e1.png?width=129&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0ae004f7cf112d0ac4aedbc47e057f6ca690750f\"&gt;lr 2e-4 500 epochs&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r7kbeewebg1e1.png?width=132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9fee55b4ff32c10544b809bf8a9cb93fed95276c\"&gt;after initialization&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I see some cross like patterns on all of my images, so I believe there is something inherently wrong with my network that produces them. here is the code.&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;class SimpleGenerator(nn.Module):\n    def __init__(self,out_channels =1,\n                 noise_dimension = 100 ,\n                 channels= 64         \n                 ):\n        super(SimpleGenerator, self).__init__()\n        self.noise_shape = (noise_dimension,1,1,1)\n        self.out_channels = out_channels \n        self.channels = channels\n        self.gen = nn.Sequential(\n            nn.ConvTranspose3d(self.noise_shape[0],  self.channels * 32, 4, 1, (1, 0, 1)),\n            nn.ReLU(),\n            self._block( self.channels * 32,  self.channels * 16, 5, 1, 0),\n            self._block( self.channels * 16,  self.channels * 8, 5, 1, 0),\n            self._block( self.channels * 8,  self.channels * 4, 4, 2, 1),\n            self._block( self.channels * 4,  self.channels * 2, 4, 2, 1),\n            self._block( self.channels * 2,  self.channels, 4, 2, 1),\n            nn.ConvTranspose3d( self.channels, self.out_channels, 4, 2, 1),\n            nn.Sigmoid()\n        )\n\n    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n        return nn.Sequential(\n\n            nn.ConvTranspose3d(in_channels,out_channels,3,1,1,bias=False),\n            nn.InstanceNorm3d(out_channels),\n            nn.ReLU(),\n\n            nn.ConvTranspose3d(out_channels,out_channels,kernel_size,stride,padding,bias=False),\n            nn.InstanceNorm3d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x,separate=False):\n\n        x = self.gen(x)\n        return x\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Notes :&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I am using InstanceNorm Instead of batch norm as my images are 160 x192x160 they are too big so The gpu can&amp;#39;t support batch_size &amp;gt;1.&lt;/li&gt;\n&lt;li&gt;The weird numbers you see in the kernel size, stride and padding are because I want to achieve the shape described above which is not a power of two. Could this be the reason?&lt;/li&gt;\n&lt;li&gt;I have tried the _block method with 1 or 2 convolutions (we see the 2 version). Same result&lt;/li&gt;\n&lt;li&gt;the discriminator is a mirror image of the generator. I won&amp;#39;t provide the code to make the post short, but i can if someone believes it is needed.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtc2qv",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "ripototo",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtc2qv/d_convolutional_generative_adversarial_networks/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731844856.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Title, also something like pyannote/segmentation -3.0 but better. Is there anything new in this domain? I came across mamba but it's still in early stage for this purpose to say anything concrete about it.",
            "author_fullname": "t2_9pbebnns",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Looking for some audio segmentation model.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gtbkhw",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731842784.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title, also something like pyannote/segmentation -3.0 but better. Is there anything new in this domain? I came across mamba but it&amp;#39;s still in early stage for this purpose to say anything concrete about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtbkhw",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Just_Difficulty9836",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtbkhw/d_looking_for_some_audio_segmentation_model/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731842784.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey guys, I wondered how you learnt to use tools such as Wandb and MLFlow to log the gradient and tune hyperparameters in the model. \n\nCould you share resources for the same?",
            "author_fullname": "t2_110x2z3fhn",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Logging Gradients and using third party loggers to tune hyper parameters",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gtal7z",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731838455.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I wondered how you learnt to use tools such as Wandb and MLFlow to log the gradient and tune hyperparameters in the model. &lt;/p&gt;\n\n&lt;p&gt;Could you share resources for the same?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gtal7z",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DiscussionTricky2904",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gtal7z/discussion_logging_gradients_and_using_third/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731838455.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_dacoq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Supercharging Structured Outputs with Open Source Models 🚀",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gt2yfp",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "cd34ef9a-6abd-11ea-a7ea-0ec6041e93a9",
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731808623.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "sachinruk.github.io",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://sachinruk.github.io/blog/2024-10-20-structured-outputs.html",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "ML Engineer",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gt2yfp",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "themathstudent",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": "dark",
            "permalink": "/r/MachineLearning/comments/1gt2yfp/p_supercharging_structured_outputs_with_open/",
            "stickied": false,
            "url": "https://sachinruk.github.io/blog/2024-10-20-structured-outputs.html",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731808623.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Recently I saw some good posts about dim reduction methods like the one dissecting UMAP, so I thought I'd chime in with a POC that leverages the idea of those methods for a very practical purpose: enabling server-side semantic search on large databases with high-dimensional embeddings using just a static FlatGeobuf file and a web server like nginx.\n\n# tl;dr\n\n**- Writing (and appending to) a FlatGeobuf file**: Embeddings -&gt; Gaussian Random Projection -&gt; 2D points -&gt; FlatGeobuf file  \n**- Reading a FlatGeobuf file (based on a single user query)**: Embedding -&gt; Gaussian Random Projection -&gt; 2D point -&gt; buffered bounding box around this point -&gt; http range request(s) from client to remote FlatGeobuf file -&gt; subset of data points around the 2D point -&gt; reranking this subset client-side\n\nFind the detailed explanation, code and examples on GitHub: [https://github.com/do-me/flatgeobuf-vectordb](https://github.com/do-me/flatgeobuf-vectordb)\n\n# Main concepts\n\n1. Points that are close in 2 dimensions (after projection) should be close in N dimensions too. This is obviously not always true but in my tests, it's good enough for basic use cases (e.g. product recommendation), where you do not need the closest result to the query but instead something in the top 0.1% or 0.01% may suffice. Note that I need to use a dim reduction method that works independently from the data, so cannot use UMAP, HUMAP, tSNE and PCA.\n2. I'm reducing to 2 dims to benefit from all the heavy optimization work that the FlatGeobuf file format has done. Reducing to 3 dims (or even more) might preserve the similarity better (and eventually lead to better results) but also increases the overhead for efficiently designing such a file format. If you know any other suitable file formats for this purpose, I'd be very curious to try them! Another alternative might be instead of relying on one static file, to create an efficient file structure with many static files. The pros and cons have been discussed in a completely different context by the authors of protomaps and openfreemap on HN.\n\n# Potential\n\nEven though there are some tradeoffs in this workflow and yet many things to optimize and explore, I believe that the concept might be charming for low maintenance and low cost applications. In the end, you just dump one static file somewhere and fire normal http range requests to it, so the capacity of your web server determines the performance.  \nAs I'm heavily into client-side processing with transformers.js my ideal setup would use very small embedding models like Potion/Model2vec (&lt; 35Mb) in the client and index the user query (text/image) in the browser. This way, the remote database could be very large, like 100Gb and serve thousands of clients without any problems on a low-grade CPU (but very fast storage).\n\nIf you're fine with DB connection (which afaik can't be created browser-side), then just use LanceDB, following the same \"one file\" principle.\n\nI'm super curious about your optimization ideas!\n\nP.S. There is lots of overlap between geospatial and the latent space.",
            "author_fullname": "t2_m87h5f129",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] FlatGeobuf as \"static\" vector database using dimensionality reduction",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gssov1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731779307.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Recently I saw some good posts about dim reduction methods like the one dissecting UMAP, so I thought I&amp;#39;d chime in with a POC that leverages the idea of those methods for a very practical purpose: enabling server-side semantic search on large databases with high-dimensional embeddings using just a static FlatGeobuf file and a web server like nginx.&lt;/p&gt;\n\n&lt;h1&gt;tl;dr&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;- Writing (and appending to) a FlatGeobuf file&lt;/strong&gt;: Embeddings -&amp;gt; Gaussian Random Projection -&amp;gt; 2D points -&amp;gt; FlatGeobuf file&lt;br/&gt;\n&lt;strong&gt;- Reading a FlatGeobuf file (based on a single user query)&lt;/strong&gt;: Embedding -&amp;gt; Gaussian Random Projection -&amp;gt; 2D point -&amp;gt; buffered bounding box around this point -&amp;gt; http range request(s) from client to remote FlatGeobuf file -&amp;gt; subset of data points around the 2D point -&amp;gt; reranking this subset client-side&lt;/p&gt;\n\n&lt;p&gt;Find the detailed explanation, code and examples on GitHub: &lt;a href=\"https://github.com/do-me/flatgeobuf-vectordb\"&gt;https://github.com/do-me/flatgeobuf-vectordb&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Main concepts&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Points that are close in 2 dimensions (after projection) should be close in N dimensions too. This is obviously not always true but in my tests, it&amp;#39;s good enough for basic use cases (e.g. product recommendation), where you do not need the closest result to the query but instead something in the top 0.1% or 0.01% may suffice. Note that I need to use a dim reduction method that works independently from the data, so cannot use UMAP, HUMAP, tSNE and PCA.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m reducing to 2 dims to benefit from all the heavy optimization work that the FlatGeobuf file format has done. Reducing to 3 dims (or even more) might preserve the similarity better (and eventually lead to better results) but also increases the overhead for efficiently designing such a file format. If you know any other suitable file formats for this purpose, I&amp;#39;d be very curious to try them! Another alternative might be instead of relying on one static file, to create an efficient file structure with many static files. The pros and cons have been discussed in a completely different context by the authors of protomaps and openfreemap on HN.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Potential&lt;/h1&gt;\n\n&lt;p&gt;Even though there are some tradeoffs in this workflow and yet many things to optimize and explore, I believe that the concept might be charming for low maintenance and low cost applications. In the end, you just dump one static file somewhere and fire normal http range requests to it, so the capacity of your web server determines the performance.&lt;br/&gt;\nAs I&amp;#39;m heavily into client-side processing with transformers.js my ideal setup would use very small embedding models like Potion/Model2vec (&amp;lt; 35Mb) in the client and index the user query (text/image) in the browser. This way, the remote database could be very large, like 100Gb and serve thousands of clients without any problems on a low-grade CPU (but very fast storage).&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re fine with DB connection (which afaik can&amp;#39;t be created browser-side), then just use LanceDB, following the same &amp;quot;one file&amp;quot; principle.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super curious about your optimization ideas!&lt;/p&gt;\n\n&lt;p&gt;P.S. There is lots of overlap between geospatial and the latent space.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?auto=webp&amp;s=3dea12521107cd56a6c1b662f9ebe27d10c4720e",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3eb0b6a94cefd78362faed0809480a81b7fd964f",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c40c9326677c08c03f1c177eff930dba0a3b76a",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5313c9bd8074dabcd6f689a3f3ed7d04cb186bb2",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ffd36d6465e78447c97ad7f9b87ce84acb06cfa",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=792f3e84cb97272cce49838f0d5c2fdd06a768e2",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/PiQMpuxOiB_ZHBJyqNnS8zmKc3zFeOlJiq1eQ2AXpik.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=cb72cf46eb7899526b2bf038ad62c9cc99717283",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "GQkKXu6dYmdNAoLEwBhisfjPx-c9faeRZWew7Emuea8"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gssov1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DomeGIS",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gssov1/p_flatgeobuf_as_static_vector_database_using/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731779307.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "What are you guys using for data(set) versioning and would you suggest to use for a small (1000 x 700) table ?",
            "author_fullname": "t2_87xxasa2",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Dataset versioning tool [D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gt1avg",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731803516.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are you guys using for data(set) versioning and would you suggest to use for a small (1000 x 700) table ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gt1avg",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Amazing_Alarm6130",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gt1avg/dataset_versioning_tool_d/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731803516.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Holograms are created by encoding the 3D scene on a 2D film. Once you have that, you delete the 3D scene and all the objects it had.\n\nNow, when you see from the opposite side of the film, you see the 3D objects as if they're still there. You can change your viewing angle etc and it looks like you're looking at a 3D scene through that film, but that 3D scene doesn't exist; rather, the light field of that scene has been encoded on to the film.\n\nA very amazing illustrative guide to this is this video by Grant Sanderson on his 3B1B channel ( r/3Blue1Brown )  \n[https://www.youtube.com/watch?v=EmKQsSDlaa4](https://www.youtube.com/watch?v=EmKQsSDlaa4)\n\nThis tells us that a 2D representation of a 3D scene is really possible. I'm posting this here to ask:\n\n1. Are there papers that use the Holograph formulation to do Novel View Synthesis and 3D reconstruction?\n2. Why do we need Nerfs and Gaussian Splats if a 2D representation like a holograph of a scene is pretty good for Novel View Synthesis?",
            "author_fullname": "t2_10vqhu",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Holography Driven Novel View Synthesis - Literature Survey.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gszzqz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731799682.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Holograms are created by encoding the 3D scene on a 2D film. Once you have that, you delete the 3D scene and all the objects it had.&lt;/p&gt;\n\n&lt;p&gt;Now, when you see from the opposite side of the film, you see the 3D objects as if they&amp;#39;re still there. You can change your viewing angle etc and it looks like you&amp;#39;re looking at a 3D scene through that film, but that 3D scene doesn&amp;#39;t exist; rather, the light field of that scene has been encoded on to the film.&lt;/p&gt;\n\n&lt;p&gt;A very amazing illustrative guide to this is this video by Grant Sanderson on his 3B1B channel ( &lt;a href=\"/r/3Blue1Brown\"&gt;r/3Blue1Brown&lt;/a&gt; )&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/watch?v=EmKQsSDlaa4\"&gt;https://www.youtube.com/watch?v=EmKQsSDlaa4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This tells us that a 2D representation of a 3D scene is really possible. I&amp;#39;m posting this here to ask:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Are there papers that use the Holograph formulation to do Novel View Synthesis and 3D reconstruction?&lt;/li&gt;\n&lt;li&gt;Why do we need Nerfs and Gaussian Splats if a 2D representation like a holograph of a scene is pretty good for Novel View Synthesis?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/js5hZuJ-5y9HqC3NTMRq-LHdTOAj3wcuGC5o68-6hUM.jpg?auto=webp&amp;s=ba74a0bcdc619c4376dd4acb51703f52152c498e",
                            "width": 480,
                            "height": 360
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/js5hZuJ-5y9HqC3NTMRq-LHdTOAj3wcuGC5o68-6hUM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e62491d504b9b26c206c00f98b8ba3b501edfab8",
                                "width": 108,
                                "height": 81
                            },
                            {
                                "url": "https://external-preview.redd.it/js5hZuJ-5y9HqC3NTMRq-LHdTOAj3wcuGC5o68-6hUM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ea51fd792449661df2668ba2909da6b4096a842",
                                "width": 216,
                                "height": 162
                            },
                            {
                                "url": "https://external-preview.redd.it/js5hZuJ-5y9HqC3NTMRq-LHdTOAj3wcuGC5o68-6hUM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=816e871ec371c202c91799fa7d7349a4e0713882",
                                "width": 320,
                                "height": 240
                            }
                        ],
                        "variants": {},
                        "id": "pv38TjtV31wty_N9l2j3N6HRv-Pjq5IsB6aqVX7ai7o"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gszzqz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "nefrpitou",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gszzqz/r_holography_driven_novel_view_synthesis/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gszzqz/r_holography_driven_novel_view_synthesis/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731799682.0,
            "num_crossposts": 1,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "The Wisecube AI Team invites you to an upcoming webinar that explores an often-overlooked, yet critical aspect of AI reliability: hallucinations in large language models (LLMs).  \nDiscover how specific text features impact model accuracy and learn about methods for detecting hallucinations in LLMs. We’ll share insights into identifying model weaknesses and improving reliability, providing practical knowledge for AI practitioners and data scientists. This is a valuable opportunity to deepen your understanding of AI and explore the latest techniques for enhancing model performance!\n\n🗓️ Date: November 21, 2024 | 🕐 Time: 1 PM EST\n\n🎟️ Participation is free! [Register here](https://www.linkedin.com/events/7261113856268161024/about/) \n\n\n\nhttps://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f\n\n",
            "author_fullname": "t2_9cfqu25v",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[N] Addressing AI’s Hidden Risks: Join Our Free Webinar on Hallucinations in LLMs",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "two",
            "downs": 0,
            "thumbnail_height": 78,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "b82zo743ac1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 60,
                            "x": 108,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=20ca2f152676f7d099a4b357b96a810f5e97b26e"
                        },
                        {
                            "y": 121,
                            "x": 216,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=45eb76b37907a02792f1aeeb254a2266fba31c85"
                        },
                        {
                            "y": 180,
                            "x": 320,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6a1f634f8d61483dd99939bd22e42f5679e2aa9"
                        },
                        {
                            "y": 360,
                            "x": 640,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d76d1e0ac1726e0727ee6aed2ae11b0a9866b5f"
                        },
                        {
                            "y": 540,
                            "x": 960,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eeccd6971f1a3ae40979c7cd7199ddd8b7b6cb36"
                        },
                        {
                            "y": 607,
                            "x": 1080,
                            "u": "https://preview.redd.it/b82zo743ac1e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81454dce614c2d27b8ca7cc6d358c4800bd1e965"
                        }
                    ],
                    "s": {
                        "y": 1080,
                        "x": 1920,
                        "u": "https://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f"
                    },
                    "id": "b82zo743ac1e1"
                }
            },
            "name": "t3_1gsynxm",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.2,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/8nSTv3oEsqF0s1ATheiYzegTVj1ZU08SxYPHy93e4Jw.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731795836.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The Wisecube AI Team invites you to an upcoming webinar that explores an often-overlooked, yet critical aspect of AI reliability: hallucinations in large language models (LLMs).&lt;br/&gt;\nDiscover how specific text features impact model accuracy and learn about methods for detecting hallucinations in LLMs. We’ll share insights into identifying model weaknesses and improving reliability, providing practical knowledge for AI practitioners and data scientists. This is a valuable opportunity to deepen your understanding of AI and explore the latest techniques for enhancing model performance!&lt;/p&gt;\n\n&lt;p&gt;🗓️ Date: November 21, 2024 | 🕐 Time: 1 PM EST&lt;/p&gt;\n\n&lt;p&gt;🎟️ Participation is free! &lt;a href=\"https://www.linkedin.com/events/7261113856268161024/about/\"&gt;Register here&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f\"&gt;https://preview.redd.it/b82zo743ac1e1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5556668e8ee8e156ce2a5caa2f4acdb4198ef75f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsynxm",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "kgorobinska",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsynxm/n_addressing_ais_hidden_risks_join_our_free/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731795836.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I have done a little bit of digging, but didnt really find the answer to this question, so if someones knows what might be wrong, please enlighten me. I have done some out of sample predictions (3000 observations) and I am getting really weird results when evaluating a model predicting demand levels. Model used is xgb regressor. So R\\^2 point out that model performs worse than simply predicting the mean of the target variable, but at the same time the correlation between actual and predicted values is statistically significant. Moreover explained variance score says that model is worse than naive model, but Theil's U-statistic says the opposite? Code and results posted below. Thought that outstanding values might be the problem, but I clipped them at 0,05 and 0,95 quantile and it does not help.\n\nhttps://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;format=png&amp;auto=webp&amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931\n\nhttps://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;format=png&amp;auto=webp&amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d\n\n",
            "author_fullname": "t2_5ko1p6w9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] R^2 is negative, but the correlation between prediction and actual values is statistically significant?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": 102,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "10kpzdqs1c1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 79,
                            "x": 108,
                            "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=93a83fef4834198af0ed9168f93e1e3ab899b83a"
                        },
                        {
                            "y": 158,
                            "x": 216,
                            "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aedf76384af403c7a1b43521d5801955ce5d90cd"
                        },
                        {
                            "y": 235,
                            "x": 320,
                            "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=900f945937733ff38ae437e2ab065e567b60c3d3"
                        },
                        {
                            "y": 470,
                            "x": 640,
                            "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f91427ec81c799041f035b059a72e547d66ca2e"
                        },
                        {
                            "y": 705,
                            "x": 960,
                            "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=378feb341d6029953fd77034c1eef35a84c269a5"
                        }
                    ],
                    "s": {
                        "y": 710,
                        "x": 966,
                        "u": "https://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;format=png&amp;auto=webp&amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931"
                    },
                    "id": "10kpzdqs1c1e1"
                },
                "t2rapmo22c1e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 79,
                            "x": 108,
                            "u": "https://preview.redd.it/t2rapmo22c1e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=78ff193f8c590450177a872c29c1e63d18c5f479"
                        },
                        {
                            "y": 159,
                            "x": 216,
                            "u": "https://preview.redd.it/t2rapmo22c1e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=53666befdc2778701519843b441befe248781628"
                        },
                        {
                            "y": 236,
                            "x": 320,
                            "u": "https://preview.redd.it/t2rapmo22c1e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67aea6b60527202eecdc46300838981171a1bcae"
                        },
                        {
                            "y": 473,
                            "x": 640,
                            "u": "https://preview.redd.it/t2rapmo22c1e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=99bfc66651987658ae60555fda108fad03c88da2"
                        }
                    ],
                    "s": {
                        "y": 632,
                        "x": 855,
                        "u": "https://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;format=png&amp;auto=webp&amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d"
                    },
                    "id": "t2rapmo22c1e1"
                }
            },
            "name": "t3_1gsxror",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 20,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 20,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/uhs4FikNFVOvSvA_J0kXMO8ynlOpQppEByvYyvyxlbM.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731793301.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have done a little bit of digging, but didnt really find the answer to this question, so if someones knows what might be wrong, please enlighten me. I have done some out of sample predictions (3000 observations) and I am getting really weird results when evaluating a model predicting demand levels. Model used is xgb regressor. So R^2 point out that model performs worse than simply predicting the mean of the target variable, but at the same time the correlation between actual and predicted values is statistically significant. Moreover explained variance score says that model is worse than naive model, but Theil&amp;#39;s U-statistic says the opposite? Code and results posted below. Thought that outstanding values might be the problem, but I clipped them at 0,05 and 0,95 quantile and it does not help.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931\"&gt;https://preview.redd.it/10kpzdqs1c1e1.png?width=966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9b93f0ef588e2fa5cb16c06f69c0fea1902e0931&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d\"&gt;https://preview.redd.it/t2rapmo22c1e1.png?width=855&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce9d8d1d2ad54c8743873560bfff8a275a14378d&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsxror",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "maciek024",
            "discussion_type": null,
            "num_comments": 55,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsxror/discussion_r2_is_negative_but_the_correlation/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731793301.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Given a set of inputs/ouputs, generate a suitable program\n\nwhat are the baseline/canonical papers using DL for this program synthesis?\n\nthanks",
            "author_fullname": "t2_174qh3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] program synthesis from input-output pairs - DL papers ?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gswhou",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731789700.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Given a set of inputs/ouputs, generate a suitable program&lt;/p&gt;\n\n&lt;p&gt;what are the baseline/canonical papers using DL for this program synthesis?&lt;/p&gt;\n\n&lt;p&gt;thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gswhou",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "yazriel0",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gswhou/d_program_synthesis_from_inputoutput_pairs_dl/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731789700.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\n\nI’ve been using Whisper for transcription and love its accuracy, but speed is an issue for me. It takes around 40 seconds to process a 2-minute audio file on my setup. I’ve read about models (sometimes dubbed “tree-like models”) that can achieve this in just 5 seconds. Has anyone here tested or optimized such models?\n\nIdeally, I’d prefer sticking to CPU usage for reliability, but I’m curious if running Whisper on an AMD GPU could offer a significant speed boost. Anyone with experience on that?\n\nLooking forward to your insights and recommendations!",
            "author_fullname": "t2_8xx0oeeb",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Optimizing Whisper Speed: CPU vs. AMD GPU?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsw3ba",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731788568.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve been using Whisper for transcription and love its accuracy, but speed is an issue for me. It takes around 40 seconds to process a 2-minute audio file on my setup. I’ve read about models (sometimes dubbed “tree-like models”) that can achieve this in just 5 seconds. Has anyone here tested or optimized such models?&lt;/p&gt;\n\n&lt;p&gt;Ideally, I’d prefer sticking to CPU usage for reliability, but I’m curious if running Whisper on an AMD GPU could offer a significant speed boost. Anyone with experience on that?&lt;/p&gt;\n\n&lt;p&gt;Looking forward to your insights and recommendations!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsw3ba",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "yeah280",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsw3ba/p_optimizing_whisper_speed_cpu_vs_amd_gpu/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731788568.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Tau for me is one of the most fascinating projects of our time. I have been observing the research and development since 2017.\nToday the team has released the alpha of Tau language after many years of work!\nThis is a big moment!\n\nhttps://x.com/TauLogicAI/status/1857816396404600979?t=t7ATRYIXTMADewTYUo3ryg&amp;s=19",
            "author_fullname": "t2_b9s2w",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[N] Tau Language Alpha Release ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "two",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsutua",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731785061.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tau for me is one of the most fascinating projects of our time. I have been observing the research and development since 2017.\nToday the team has released the alpha of Tau language after many years of work!\nThis is a big moment!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://x.com/TauLogicAI/status/1857816396404600979?t=t7ATRYIXTMADewTYUo3ryg&amp;amp;s=19\"&gt;https://x.com/TauLogicAI/status/1857816396404600979?t=t7ATRYIXTMADewTYUo3ryg&amp;amp;s=19&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/CbnMaFt-wj5BQV_w6CMi_YXRq5jQ9l3tv9xe5YGW4vY.jpg?auto=webp&amp;s=20ad79b26e5869255d89eceb497d67503d7ae355",
                            "width": 800,
                            "height": 320
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/CbnMaFt-wj5BQV_w6CMi_YXRq5jQ9l3tv9xe5YGW4vY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=80fc7a314bbaaabee0219a0b299ac4bd499b161a",
                                "width": 108,
                                "height": 43
                            },
                            {
                                "url": "https://external-preview.redd.it/CbnMaFt-wj5BQV_w6CMi_YXRq5jQ9l3tv9xe5YGW4vY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9fff5c072e7ae239ca1572299f4bcc30117ccc1b",
                                "width": 216,
                                "height": 86
                            },
                            {
                                "url": "https://external-preview.redd.it/CbnMaFt-wj5BQV_w6CMi_YXRq5jQ9l3tv9xe5YGW4vY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=656c02298cf3104af5a0fd6a2cfa361a408ca1dd",
                                "width": 320,
                                "height": 128
                            },
                            {
                                "url": "https://external-preview.redd.it/CbnMaFt-wj5BQV_w6CMi_YXRq5jQ9l3tv9xe5YGW4vY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=366ec5efa24b818c37f8aa166ffe2cf533b6480c",
                                "width": 640,
                                "height": 256
                            }
                        ],
                        "variants": {},
                        "id": "WMjJP1p3xzMvoInUjb_fBMEu8MldCZK4E5_RfjQKzwE"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsutua",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "madsurgeon",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsutua/n_tau_language_alpha_release/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731785061.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "How many years you take to finish ML PhD after bachelor’s? I understand different parts of the world usually have different duration. ",
            "author_fullname": "t2_pn7vkxy9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Your ML PhD duration",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsue6g",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.77,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 23,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 23,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731783883.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How many years you take to finish ML PhD after bachelor’s? I understand different parts of the world usually have different duration. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsue6g",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AntelopeWilling2928",
            "discussion_type": null,
            "num_comments": 32,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsue6g/d_your_ml_phd_duration/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731783883.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Yall may login to softconf to check if you can submit the camera-ready paper or not.\n\nMine was 4/3/3 and luckily got accepted. My first paper!!!",
            "author_fullname": "t2_e61mpx7j",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] COLING 2025 Results are leaked",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gssewj",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.79,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 22,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 22,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731778540.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yall may login to softconf to check if you can submit the camera-ready paper or not.&lt;/p&gt;\n\n&lt;p&gt;Mine was 4/3/3 and luckily got accepted. My first paper!!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gssewj",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Ambitious-Public-512",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gssewj/d_coling_2025_results_are_leaked/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731778540.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello,\n\nI’m a CS PhD student, and I’m looking to deepen my understanding of machine learning theory. My research area focuses on vision-language models, but I’d like to expand my knowledge by reading foundational or groundbreaking ML theory papers.\n\nCould you please share a list of must-read papers or personal recommendations that have had a significant impact on ML theory?\n\nThank you in advance!\n",
            "author_fullname": "t2_pn7vkxy9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Must-Read ML Theory Papers",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsqqns",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.96,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 275,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 275,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731773968.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I’m a CS PhD student, and I’m looking to deepen my understanding of machine learning theory. My research area focuses on vision-language models, but I’d like to expand my knowledge by reading foundational or groundbreaking ML theory papers.&lt;/p&gt;\n\n&lt;p&gt;Could you please share a list of must-read papers or personal recommendations that have had a significant impact on ML theory?&lt;/p&gt;\n\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsqqns",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AntelopeWilling2928",
            "discussion_type": null,
            "num_comments": 84,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsqqns/r_mustread_ml_theory_papers/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731773968.0,
            "num_crossposts": 1,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Is there any existing work that try to investigate the relationship between time steps of a diffusion model? Something like the impact of model loss at time step i of the model to the output at time step j of the model? (j&lt;i)",
            "author_fullname": "t2_5i0fuept7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Time step dependency in diffusion model",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gspx8g",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731771725.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there any existing work that try to investigate the relationship between time steps of a diffusion model? Something like the impact of model loss at time step i of the model to the output at time step j of the model? (j&amp;lt;i)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gspx8g",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Careless-Top-2411",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gspx8g/d_time_step_dependency_in_diffusion_model/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731771725.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi guys, I've been working on this tool for my PhD for a while now. The PhD is about Multi Task Learning in the context of videos and I'm recently developing a tool to get predictions per frame from pre-trained \"experts\" (semantic segmentation, depth estimation etc.). The purpose of these is to train multi-task CV models with more than just raw RGB data to help with data efficiency and generalization.\n\nThe code is here: https://gitlab.com/video-representations-extractor/video-representations-extractor and there's a bunch of examples over there (including pip install command).\n\nRecently I've done a \"end to end\" example for showcasing and I've put it on google colab as well: https://colab.research.google.com/drive/1vAp71H-TLewhF56odv33TkmGwwhuoFJ-?usp=sharing\n\nExample output of the colab notebook: https://i.imgur.com/wyl9FPw.png\n\nIt skips a bunch of steps for simplicity (i.e. the binary semantic outputs like \"transportation\" are implemented separately for experimentation purposes and I just download that file + import it in the notebook instead of copy pasting 300+ lines of code in the colab but don't run arbitrary code w/o checking lol).\n\nThe colab should work fine for any UAV/driving/handheld indoor videos, not just my demo video.\n\nThe CLI tool syntax is pretty much:\n\n    export VRE_DEVICE=cuda; # if available  \n    vre video.mp4 --config_file config.yaml -o out_dir\n\nwhere the config file defines parameters for these experts that I've implemented.",
            "author_fullname": "t2_5p0gz",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Video Representations Extractor (VRE): Open source Video Multi Task dataset creation tool (+colab)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsmhuo",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731761029.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, I&amp;#39;ve been working on this tool for my PhD for a while now. The PhD is about Multi Task Learning in the context of videos and I&amp;#39;m recently developing a tool to get predictions per frame from pre-trained &amp;quot;experts&amp;quot; (semantic segmentation, depth estimation etc.). The purpose of these is to train multi-task CV models with more than just raw RGB data to help with data efficiency and generalization.&lt;/p&gt;\n\n&lt;p&gt;The code is here: &lt;a href=\"https://gitlab.com/video-representations-extractor/video-representations-extractor\"&gt;https://gitlab.com/video-representations-extractor/video-representations-extractor&lt;/a&gt; and there&amp;#39;s a bunch of examples over there (including pip install command).&lt;/p&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve done a &amp;quot;end to end&amp;quot; example for showcasing and I&amp;#39;ve put it on google colab as well: &lt;a href=\"https://colab.research.google.com/drive/1vAp71H-TLewhF56odv33TkmGwwhuoFJ-?usp=sharing\"&gt;https://colab.research.google.com/drive/1vAp71H-TLewhF56odv33TkmGwwhuoFJ-?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Example output of the colab notebook: &lt;a href=\"https://i.imgur.com/wyl9FPw.png\"&gt;https://i.imgur.com/wyl9FPw.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It skips a bunch of steps for simplicity (i.e. the binary semantic outputs like &amp;quot;transportation&amp;quot; are implemented separately for experimentation purposes and I just download that file + import it in the notebook instead of copy pasting 300+ lines of code in the colab but don&amp;#39;t run arbitrary code w/o checking lol).&lt;/p&gt;\n\n&lt;p&gt;The colab should work fine for any UAV/driving/handheld indoor videos, not just my demo video.&lt;/p&gt;\n\n&lt;p&gt;The CLI tool syntax is pretty much:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;export VRE_DEVICE=cuda; # if available  \nvre video.mp4 --config_file config.yaml -o out_dir\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;where the config file defines parameters for these experts that I&amp;#39;ve implemented.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?auto=webp&amp;s=65ea1befbe7eafad97d71a96b6cf3624aca0fe68",
                            "width": 1489,
                            "height": 819
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=04c3d8e6eaa623d48cf7f8436d1830d68518c856",
                                "width": 108,
                                "height": 59
                            },
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed55b90cd8b5599f06a0fa00b07f4d9eca351f6d",
                                "width": 216,
                                "height": 118
                            },
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1fcce8bfec008785169b199098d9d953fe68730a",
                                "width": 320,
                                "height": 176
                            },
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e6bb83db16f466e1afb6924c3f8fa8da038612e",
                                "width": 640,
                                "height": 352
                            },
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dc4f93e71df513706bd5c7aba28c4e0b4ecf42ac",
                                "width": 960,
                                "height": 528
                            },
                            {
                                "url": "https://external-preview.redd.it/Qr7xX-nQorxwgPrqqKArF4ymktEHpO4ZJXP_AxDLXgE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6a7ccae5ab9a8219978d3b46edd41ad2f06b8ee",
                                "width": 1080,
                                "height": 594
                            }
                        ],
                        "variants": {},
                        "id": "iuelyasYjZ-stOJ933zkdNVi0N53vHyz8XTA8jiWCOM"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsmhuo",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "nucLeaRStarcraft",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsmhuo/p_video_representations_extractor_vre_open_source/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731761029.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\nI need some advice on how to best create an offline Text-to-Speech (TTS) system that I can use in Unity or Unreal Engine. Are there any tools or websites where I can clone a voice, download it, and use it locally in these engines?\n\nI’m looking for a solution that doesn’t rely on cloud services and works entirely offline. Any recommendations or experiences with this would be greatly appreciated!\n\nThanks!",
            "author_fullname": "t2_694nqna3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What’s the best tool for implementing TTS in Unity or UE5? [D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsjz08",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731750205.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,\nI need some advice on how to best create an offline Text-to-Speech (TTS) system that I can use in Unity or Unreal Engine. Are there any tools or websites where I can clone a voice, download it, and use it locally in these engines?&lt;/p&gt;\n\n&lt;p&gt;I’m looking for a solution that doesn’t rely on cloud services and works entirely offline. Any recommendations or experiences with this would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsjz08",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "NoPrinciple1242",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731750205.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi, I recently spent some time to understand the core implementation of the UMAP algorithm from the point of view how it was implemented and why it's so fast (even though it's in python). I decided to decompose the algorithm into smaller steps in which I add some minor improvements to the code (one by one), so that at the end the final results are very similar to what I can get from the UMAP. \n\nTo my surprise, most of these changes were just tricks in the optimization code to run things faster or update less important things less often. Of course, my implementation does not reproduce the UMAP algorithm in 100% as it was done in the educational purposes.\n\nI provided a detailed explanation in my project of what I had to add in each step to move towards UMAP like algorithm. Here is the project page: [https://github.com/kmkolasinski/nano-umap](https://github.com/kmkolasinski/nano-umap)\n\nIf you are a person like, who likes to optimize the code for performance you may find this interesting. Here is a demo what I was able to get: \n\nhttps://preview.redd.it/eww57c3x881e1.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160\n\n**TLDR: in UMAP they:**\n\n* use ANN library to quickly find top k-NN,\n* use good initialization method which makes things more stable and algorithm requires less updates (UMAP uses fast spectral initialization),\n* use random negative sampling, which is a naive approach but works very well in practice,\n* squeeze the numba performance (by replacing [np.dot](http://np.dot) or np.clip with custom implementations to make code run much faster),\n* use some sort of adaptive sampling which will make that the algorithm will spend more time on more important vectors saving your CPU time on less important ones\n\n",
            "author_fullname": "t2_mseqq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Analysis of why UMAP is so fast ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "eww57c3x881e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 39,
                            "x": 108,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38d48d55190e82a812cb949178ac6a2d67a255f5"
                        },
                        {
                            "y": 78,
                            "x": 216,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ed86e4d004c346453ec0f4f7c5e6e0d42118c74"
                        },
                        {
                            "y": 115,
                            "x": 320,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81ba765a2f7d64603c80b1f1034b94400db8c9ad"
                        },
                        {
                            "y": 231,
                            "x": 640,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8fd33847de157647c022956efd74a401e8fbe9bd"
                        },
                        {
                            "y": 346,
                            "x": 960,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=32f6f834eb957823b2ca5c1f24f457a2ea749c5e"
                        },
                        {
                            "y": 390,
                            "x": 1080,
                            "u": "https://preview.redd.it/eww57c3x881e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1ec20ee1d4de43f9217cc0a1c59abe1b2c18a756"
                        }
                    ],
                    "s": {
                        "y": 694,
                        "x": 1921,
                        "u": "https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160"
                    },
                    "id": "eww57c3x881e1"
                }
            },
            "name": "t3_1gsjfq9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.98,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 375,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 375,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/YOkYN0kpJL-XQBhI4QifL5lhetpkBQIdNxFyTLklVkM.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731747730.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I recently spent some time to understand the core implementation of the UMAP algorithm from the point of view how it was implemented and why it&amp;#39;s so fast (even though it&amp;#39;s in python). I decided to decompose the algorithm into smaller steps in which I add some minor improvements to the code (one by one), so that at the end the final results are very similar to what I can get from the UMAP. &lt;/p&gt;\n\n&lt;p&gt;To my surprise, most of these changes were just tricks in the optimization code to run things faster or update less important things less often. Of course, my implementation does not reproduce the UMAP algorithm in 100% as it was done in the educational purposes.&lt;/p&gt;\n\n&lt;p&gt;I provided a detailed explanation in my project of what I had to add in each step to move towards UMAP like algorithm. Here is the project page: &lt;a href=\"https://github.com/kmkolasinski/nano-umap\"&gt;https://github.com/kmkolasinski/nano-umap&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you are a person like, who likes to optimize the code for performance you may find this interesting. Here is a demo what I was able to get: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160\"&gt;https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR: in UMAP they:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;use ANN library to quickly find top k-NN,&lt;/li&gt;\n&lt;li&gt;use good initialization method which makes things more stable and algorithm requires less updates (UMAP uses fast spectral initialization),&lt;/li&gt;\n&lt;li&gt;use random negative sampling, which is a naive approach but works very well in practice,&lt;/li&gt;\n&lt;li&gt;squeeze the numba performance (by replacing &lt;a href=\"http://np.dot\"&gt;np.dot&lt;/a&gt; or np.clip with custom implementations to make code run much faster),&lt;/li&gt;\n&lt;li&gt;use some sort of adaptive sampling which will make that the algorithm will spend more time on more important vectors saving your CPU time on less important ones&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?auto=webp&amp;s=b327c78bceab0e8ff7be5dc7b898a6887cd2d481",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3bb424ae86e13c389aca9e25e95b2ed2de84b6ac",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=112df3ff815ebbb838d61e22d2ac8acd35ef4a74",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8338eb64c3e09580e8e3a2272007a989033651a4",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4bc7c9d23a93f646c2c399a8df2cad8f1fb75da4",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=44ae08c6ccf1c580e72e6d6aa79f3ea30c7b0e35",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/-2A7nAihfSIjzpn-9n0suo7gPiDGNYzK8eF-vOJcWks.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3f85c68354a5328761c039f949bd596e29f9864",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "PZbtflnAayt4zaL5ZTcLLvHaCKO9GwXDLefdRQQkK-E"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsjfq9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "kmkolasinski",
            "discussion_type": null,
            "num_comments": 42,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731747730.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I wanted to study up on the neural scaling laws and how they came into existence. Sp i wanted to see if there is a paper or a series of paper you would recommend for me to get started in those. Thank you. ",
            "author_fullname": "t2_9tkialt4",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] neural scaling laws",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsix5n",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731745343.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wanted to study up on the neural scaling laws and how they came into existence. Sp i wanted to see if there is a paper or a series of paper you would recommend for me to get started in those. Thank you. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsix5n",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "SmartEvening",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731745343.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey guys,  \nI have an interview coming up focussed on Distributed ML Algorithms (Interview description: We'll explore and explain the fundamental techniques used to build common neural network operations, focusing on simple yet effective implementations.)  \nAre there any good resources I can use to study for this kind of an interview?",
            "author_fullname": "t2_3i5tt5w3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Distributed ML Algorithms Interview",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsbykc",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731719581.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;br/&gt;\nI have an interview coming up focussed on Distributed ML Algorithms (Interview description: We&amp;#39;ll explore and explain the fundamental techniques used to build common neural network operations, focusing on simple yet effective implementations.)&lt;br/&gt;\nAre there any good resources I can use to study for this kind of an interview?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsbykc",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "deepthought00705",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731719581.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I wonder how critical they are in the modern search system given all the advancement in LM. Semantic embedding can often help us understand the meaning quite well. But in order to effectively leverage historical query item engagement features, it seems we still require those preprocessing. Otherwise, we can easily get empty engagement features when users search slightly different from common queries? Or is there a more modern way to tackle free form queries?",
            "author_fullname": "t2_1gwm2ojf",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Do modern search systems still require stemming and lemmatization in query preprocessing?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsauuz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731716202.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I wonder how critical they are in the modern search system given all the advancement in LM. Semantic embedding can often help us understand the meaning quite well. But in order to effectively leverage historical query item engagement features, it seems we still require those preprocessing. Otherwise, we can easily get empty engagement features when users search slightly different from common queries? Or is there a more modern way to tackle free form queries?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsauuz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "wenegue",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731716202.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Does anyone know of a good feature selection algorithm (with or without implementation) that can search across perhaps 50-100k features in a reasonable amount of time? I’m using lightgbm. Intuition is that I need on the order of 20-100 final features in the model. Looking to find a needle in a haystack. Tabular data, roughly 100-500k records of data to work with. Common feature selection methods do not scale computationally in my experience. Also, I’ve found overfitting is a concern with a search space this large. ",
            "author_fullname": "t2_zxrfh",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Feature selection methods that operate efficiently on large number of features (tabular, lightgbm)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gsah8e",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731715087.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know of a good feature selection algorithm (with or without implementation) that can search across perhaps 50-100k features in a reasonable amount of time? I’m using lightgbm. Intuition is that I need on the order of 20-100 final features in the model. Looking to find a needle in a haystack. Tabular data, roughly 100-500k records of data to work with. Common feature selection methods do not scale computationally in my experience. Also, I’ve found overfitting is a concern with a search space this large. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gsah8e",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "acetherace",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731715087.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\n\nI am doing a large-scale analysis where I want to extract information regarding possible risk factors and risk management strategies from annual reports.\n\nThe files are downloaded and I am currently doing OCR on the image files using tesseract, which extracts one text file for each document.\n\nAs I see it there are at least two key questions that are yet to be resolved:\n\n**1. How do I locate and extract the parts of the annual reports that are about risk management?**  \nAnnual reports for smaller firms do not carry this information and reports for larger firms can be longer than a hundred pages. I have considered labelling a lot of annual reports myself and using Named Entity Recognition, but I doubt how well it works if I am not looking for named entities as such, but paragraphs where eg. risk factors are considered.  \n*Do you have any suggestions on which NLP methods and/or programs to use?*\n\n**2. What are good ways to process the extracted text on risk?**  \nI want to generate one or more variables on risk factors and risk management strategies for each firms in each year. I have looked into Latent Dirichlet Allocation so far since it should be able to group words into topics and return some measure of how the words in a report are distributed across topics.  \n*Again: Do you have any suggestions on which NLP methods and/or programs to use?*\n\n**Specifics:**  \nI have more than a million annual reports so far and I have access to two servers that are quite fast. As a measure of speed I can OCR around 80 documents at a time on each server at high speeds.\n\n*Do you think* the project *is feasible? And is there something you think that I should be made aware of?*\n\nThanks in advance for any suggestions!",
            "author_fullname": "t2_cgmet02zy",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Extraction and processing of text on risk from annual reports",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs0hy5",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731688762.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I am doing a large-scale analysis where I want to extract information regarding possible risk factors and risk management strategies from annual reports.&lt;/p&gt;\n\n&lt;p&gt;The files are downloaded and I am currently doing OCR on the image files using tesseract, which extracts one text file for each document.&lt;/p&gt;\n\n&lt;p&gt;As I see it there are at least two key questions that are yet to be resolved:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;1. How do I locate and extract the parts of the annual reports that are about risk management?&lt;/strong&gt;&lt;br/&gt;\nAnnual reports for smaller firms do not carry this information and reports for larger firms can be longer than a hundred pages. I have considered labelling a lot of annual reports myself and using Named Entity Recognition, but I doubt how well it works if I am not looking for named entities as such, but paragraphs where eg. risk factors are considered.&lt;br/&gt;\n&lt;em&gt;Do you have any suggestions on which NLP methods and/or programs to use?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;2. What are good ways to process the extracted text on risk?&lt;/strong&gt;&lt;br/&gt;\nI want to generate one or more variables on risk factors and risk management strategies for each firms in each year. I have looked into Latent Dirichlet Allocation so far since it should be able to group words into topics and return some measure of how the words in a report are distributed across topics.&lt;br/&gt;\n&lt;em&gt;Again: Do you have any suggestions on which NLP methods and/or programs to use?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Specifics:&lt;/strong&gt;&lt;br/&gt;\nI have more than a million annual reports so far and I have access to two servers that are quite fast. As a measure of speed I can OCR around 80 documents at a time on each server at high speeds.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Do you think&lt;/em&gt; the project &lt;em&gt;is feasible? And is there something you think that I should be made aware of?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any suggestions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs0hy5",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Happy-Koala7212",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs0hy5/d_extraction_and_processing_of_text_on_risk_from/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731688762.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I did fine-tuning on the pretrained DistilBERT tranformer-model and it achieved \\~0.85 accuracy (classification with 17 classes).I also built from scratch a Transformer model using torch.nn.TransformerEncoder and it achieved \\~0.97 accuracy for the same problem. Is this normal? I was expecting to have better performance with the pre-trained DistilBERT.Please note that for the DistilBERT model I used its own embeddings (pre-trained DistilBertTokenizer) and for the torch.nn.TransformerEncoder I used the simple TFIDF method. It is getting even more confused since the TFIDF cannot capture the sequence of the words in a sentence (it ignores the context)\n\nPlease let me know your thoughts. :)",
            "author_fullname": "t2_v32somxt",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] DistilBERT vs TransformerEncoder",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs9pr4",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.2,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731712880.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I did fine-tuning on the pretrained DistilBERT tranformer-model and it achieved ~0.85 accuracy (classification with 17 classes).I also built from scratch a Transformer model using torch.nn.TransformerEncoder and it achieved ~0.97 accuracy for the same problem. Is this normal? I was expecting to have better performance with the pre-trained DistilBERT.Please note that for the DistilBERT model I used its own embeddings (pre-trained DistilBertTokenizer) and for the torch.nn.TransformerEncoder I used the simple TFIDF method. It is getting even more confused since the TFIDF cannot capture the sequence of the words in a sentence (it ignores the context)&lt;/p&gt;\n\n&lt;p&gt;Please let me know your thoughts. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs9pr4",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Interesting_Pea_4605",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs9pr4/r_distilbert_vs_transformerencoder/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731712880.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "[https://arxiv.org/pdf/2411.07279](https://arxiv.org/pdf/2411.07279)\n\nBy the way guys, do you know of any research on trying to slightly fine-tune a model on the question it is asked before having it answer? I mean it would probably work for in-context information retrieval, but I was wondering about its impact on more reasoning-heavy tasks. The compute overhang would be huge, still.",
            "author_fullname": "t2_16aic04ri8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R][D]Test time training for abstract reasoning",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs9lao",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 16,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 16,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731712529.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2411.07279\"&gt;https://arxiv.org/pdf/2411.07279&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;By the way guys, do you know of any research on trying to slightly fine-tune a model on the question it is asked before having it answer? I mean it would probably work for in-context information retrieval, but I was wondering about its impact on more reasoning-heavy tasks. The compute overhang would be huge, still.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs9lao",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Due-Pangolin325",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731712529.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Abstract\n\nWith the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29× smaller.\n\n  \nAccepted at Neurips 2024, \"SOTA\" here means comparable approaches. I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. Curious what others think?",
            "author_fullname": "t2_n8aopnq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Convolutional Differentiable Logic Gate Networks",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs92mb",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.98,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 53,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 53,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731711084.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;p&gt;With the increasing inference cost of machine learning models, there is a growing interest in models with fast and efficient inference. Recently, an approach for learning logic gate networks directly via a differentiable relaxation was proposed.  Logic gate networks are faster than conventional neural network approaches be- cause their inference only requires logic gate operators such as NAND, OR, and XOR, which are the underlying building blocks of current hardware and can be efficiently executed. We build on this idea, extending it by deep logic gate tree convolutions, logical OR pooling, and residual initializations. This allows scaling logic gate networks up by over one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10, we achieve an accuracy of 86.29% using only 61 million logic gates, which improves over the SOTA while being 29× smaller.&lt;/p&gt;\n\n&lt;p&gt;Accepted at Neurips 2024, &amp;quot;SOTA&amp;quot; here means comparable approaches. I found this paper really interesting, even though non-toy networks seems like they would be very expensive to train. Curious what others think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs92mb",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "jacobgorm",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731711084.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I think this has been asked tons of times but let me ask it one more time.\n\nI am currently working as applied scientist at MSFT. However, I am more looking into science positions, something like research scientist at DeepMind. Although jobs do not specifically need a PhD but the competition is fierce and is flooded with many PhD holders.\n\nI really do enjoy research and want to PhD but I am always asking myself if it is really worth it.\n\nThat's an open question for sure, please feel free to share your thoughts.\n\n",
            "author_fullname": "t2_w8qtmzqr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] To PhD or not to PhD",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs688q",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 114,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 114,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731714198.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731703468.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think this has been asked tons of times but let me ask it one more time.&lt;/p&gt;\n\n&lt;p&gt;I am currently working as applied scientist at MSFT. However, I am more looking into science positions, something like research scientist at DeepMind. Although jobs do not specifically need a PhD but the competition is fierce and is flooded with many PhD holders.&lt;/p&gt;\n\n&lt;p&gt;I really do enjoy research and want to PhD but I am always asking myself if it is really worth it.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s an open question for sure, please feel free to share your thoughts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs688q",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "oddhvdfscuyg",
            "discussion_type": null,
            "num_comments": 69,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731703468.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone, this is my research direction and I already would like to share the concepts to ensure that they are disseminated and researched widely in multiple parallel organizations before OpenAI or other frontier labs can show up out of the blue with a finished product and capitalize. I research open-source super intelligence, and in the meantime I have uncovered a path to AGI which I present below. I predict that Regression Training is almost solved, as indicated by the \"scaling wall\", with future advances requiring richer datasets, byte-level models, and greater compute to go with it. The next 15 years of research &amp; development will be about Automaton Learning — self-energizing systems aligned with language. This is a proposed framework for solving ConceptARC, continuous reasoning, and recursive self-improvement.\n\nQuick introduction to NCAs, they are Neural Cellular Automaton. The cells are not binary 0/1 like in Conway's Game of Life, nor are they continuous values from 0 to 1 as in many more esoteric continuous automaton — they are embeddings and hidden states. Classic NCAs also have a visualization surface, where the hidden state negotiates the evolution of this surface. Hence why they were called NCAs, as they are ultimately viewed as generative models for the desired projection surface. (2D visuals, a path through a maze, etc.) The model takes an input, a fixed filter is applied to surface (sobel, gaussian, etc.) which I call the \"environmental physics\" of the simulation, and then a model goes through every 3x3 neighborhood and does its own thing. In this manner, the physics are leveraged or not leveraged as basic transformation primitives, the same way we leverage logic gates in logic gate networks (LGNs) as a transformation operator, or quite simply matrix multiplications and activation functions in the models we know and love.\n\nThis work is downstream from the following works:\n\n* [1] Neural Cellular Maze Solver https://umu1729.github.io/pages-neural-cellular-maze-solver/\n* [2] Variational Neural Cellular Automata https://openreview.net/pdf?id=7fFO4cMBx_9\n* [3] Attention-Based Neural Cellular Automata https://arxiv.org/abs/2211.01233 \n\nThe exact procedure to produce this frankenstein will require more scrutiny and research, and it should be taken as a prototype roadmap that we 'denoise' together. This entire research plan could produce a dozen paper for each sequential step of the puzzle that will need to be solved. Ultimately, I am trying to convey the broad picture here to massively seed the field of Automaton Learning which I anticipate is the next gold rush. A syphoning scheme over the decoder is the key to this whole operation. It's about recovering and transforming the representations until they are in a more useful form. It's about knowing what cards you have and what potential hand can materialize if you go after these two other cards that seem useless on their own. Now that we have these smart intelligent decoder models, it presents a first \"factorization\" of the world. It's a better dataset and it enables new classes of machine learning. At least, this is my grand challenge to the status quo of machine learning.\n\nNow, here are my blueprints\n\n---\n\nContemporary large language models stand as monolithic crystals of knowledge, their capabilities locked in inefficient token-by-token traversals of meaning space. We present SAGE, a framework for transmuting this sequential processing into parallel field computations where meaning propagates through geometric substrates intimately aligned with human cognitive architecture. Through careful staging of representation learning, we demonstrate that any contemporary decoder-only model can be reframed as a large knowledge reservoir from which we distill more efficient computational primitives into a self-organizing field substrate.\n\nThe transmutation begins with a frozen decoder-only language model serving as our semantic anchor. An initial lightweight encoder projects tokens into one-dimensional embedding sequences, while a first low-rank adapter trained on the decoder ensures semantic fidelity. This intermediate representation, though still sequential, provides the scaffold for geometric expansion. Critical to this phase is the encoder's training to represent identical semantic content through multiple embedding configurations — effectively using the geometric dimension as a continuous manifold encoding linguistic relationships, bindings, and hierarchical structure. This multiplicity of representation creates the mathematical foundation for the subsequent expansion into field computation, as the encoder learns to map semantic invariants through varying geometric configurations.\n\nThe diversity of geometric encoding follows patterns suggestive of fundamental laws governing information organization in physical systems. Just as Zipf's law emerges from underlying principles of efficiency in natural languages, the distribution of geometric representations appears to follow power laws reflecting optimal information routing through spatial substrates. This connection between natural law and learned representation proves crucial for the stability of subsequent field dynamics.\n\nFor a 2D cellular surface of shape (B, H, W, D) each cell contains a high-dimensional meaning vector D coupled to a learned binary visualization state. The field's computational architecture emerges through precise staging of physical dynamics. Local update rules manifest as learned neural networks processing neighborhood states: U(s) = φ(W₂φ(W₁[s; N(s)] + b₁) + b₂) where φ represents layer normalization followed by ELU activation. This local processing enables information routing through wave-like propagation, with patterns forming through constructive interference of semantic signals.\n\nThe update rule F(x,t+1) = F(x,t) + A*(N(x)) + R(F) employs spatially-constrained attention A* over neighborhood N(x), typically a 3x3 Moore neighborhood, with learned residual connections R. Layer normalization ensures stability while enabling pattern formation. Crucially, the visualization state evolves through its own update network V(x,t+1) = U(F(x,t), V(x,t), N(V(x,t))), creating a bidirectional coupling between meaning and form. This replaces the exponential complexity of traditional token-by-token generation with fixed-size context computation of linear complexity O(HW) in field dimensions.\n\nCritical to pattern formation is the dual-state coupling mechanism between meaning and visualization. Rather than maintaining separate generative and discriminative components, the field itself serves as both medium and message. While meaning vectors F evolve through neighborhood attention, the visualization state V learns to project semantic content into binary patterns through its own update dynamics. This coupling creates a natural optimization surface where visual coherence guides semantic organization. The visualization network effectively learns a dynamic thresholding function mapping high-dimensional meaning to binary visual states while maintaining semantic gradients.\n\nThis architecture fundamentally transforms the traditional language model paradigm. Instead of exponentially expanding context windows to capture long-range dependencies, SAGE maintains fixed computational cost through field dynamics. Where decoder-only models must process entire contexts to generate each token, our field computation updates all semantic content simultaneously with linear complexity O(HW). Information propagates through wave-like patterns in the field substrate, with stable configurations emerging as computational primitives.\n\nField perturbation mechanics emerge through careful balance of conservation laws governing both meaning and form. Total semantic charge ∫|F|²dx remains conserved while allowing local concentrations through field gradients ∇F. Pattern formation follows least action principles minimizing energy functional E[F] = ∫(|∇F|² + V(F))dx where potential V(F) encodes learned semantic relationships derived from the frozen decoder's knowledge. These physical constraints, reminiscent of natural systems' self-organizing principles, guide emergence of stable computational primitives while preventing collapse to degenerate solutions.\n\nThe training progression orchestrates precise phases transforming monolithic decoder knowledge into geometric computation. Initial field states bootstrap from constant embeddings, with curriculum learning introducing compositional challenges requiring pattern interaction. Field dynamics learn to route information through stable configurations acting as computational waypoints. Each stable pattern serves as a reusable primitive, combining through field physics into increasingly sophisticated structures. The visualization state provides both interpretability and a geometric scaffold organizing semantic space.\n\nKnowledge extraction proceeds through rigorously validated stages:\n\n1. Frozen decoder anchors semantic meaning\n2. First encoder projects to diverse sequential representations\n3. First LoRA validates semantic preservation\n4. Second encoder expands to field geometry \n5. Second LoRA maintains decoder alignment\n6. Visualization capability emerges from field optimization\n7. Field dynamics stabilize through conservation laws\n\nImplementation crystallizes around nested hierarchies of constraints maintaining both stability and expressivity. Update rules balance information preservation against pattern innovation through careful energy bounds. The exploration of configuration space proceeds through natural field evolution guided by reconstruction gradients from the frozen decoder. This creates a form of self-supervised learning where the decoder's knowledge guides discovery of efficient computational primitives in the field substrate.\n\nVisual grounding and geometric structure emerge not as optional features but as fundamental requirements for efficient cognition. Human intelligence arises from our intimate connection to three-dimensional reality, with language itself structured through spatial metaphor and geometric reasoning. SAGE mirrors this architecture: meaning evolves in a geometric substrate naturally aligned with cognitive primitives. The projection from 3D physical reality through 2D visual processing to abstract thought provides both template and constraint for artificial intelligence design.\n\nThe framework's recursive improvement potential manifests through several interlocking mechanisms. Stable field configurations act as computational primitives, combining through local interactions into increasingly sophisticated structures. These combinations follow physical laws emerging from the field dynamics — conservation of semantic charge, least action principles, and wave-like information propagation. As patterns interact and evolve, they discover more efficient computational pathways through the geometric substrate. The curriculum progression from simple pattern formation through abstract reasoning tasks creates selection pressure favoring emergence of reusable computational motifs.\n\nEarly experiments demonstrate several key capabilities validating the SAGE approach. Various works show success in re-training a missing encoder for a decoder-only model. The transition from exponential-cost token prediction to linear-cost field evolution dramatically improves computational efficiency. Pattern diversity increases naturally through field dynamics, with stable configurations encoding reusable semantic relationships. Most importantly, the geometric grounding creates human-interpretable representations emerging from fundamental physical principles rather than arbitrary architectural choices.\n\nSuccess metrics emerge naturally from field dynamics rather than requiring arbitrary benchmarks. Pattern diversity measures the richness of stable configurations in semantic space. Compositional sophistication emerges from the physics of pattern interaction. Recursive improvement manifests through discovery of increasingly efficient computational primitives. Human alignment arises naturally from shared geometric foundations rather than post-hoc constraints.\n\nThe framework's extensibility suggests natural progressions following geometric principles. While our initial implementation uses Euclidean space for its natural connection to human visual processing, other geometries offer complementary computational advantages. Hyperbolic space, with its exponential expansion of volume with radius, provides natural representation of hierarchical relationships while maintaining constant curvature and local neighborhood structure. Multiple field geometries could interact through learned coupling dynamics, enabling sophisticated multi-scale computation while preserving linear complexity in field dimensions.\n\nThis represents a fundamental reformulation of machine intelligence — from static architecture to dynamic field discovering optimal computation through self-organization. The transition from sequential symbol manipulation to parallel field dynamics maintains semantic coherence while dramatically improving computational efficiency. Through careful orchestration of knowledge crystallization, we enable emergence of general intelligence grounded in human-interpretable geometric principles. Traditional language models, bound by exponential costs of token prediction, give way to shape-rotating field computers discovering efficient geometric paths through meaning space.\n\nThe path forward demands careful empirical validation while remaining alert to emergent capabilities arising from field dynamics interacting with decoder knowledge. Early results suggest critical components for artificial general intelligence may already exist within current architectures, awaiting reorganization into more efficient computational substrates through field dynamics. The key insight is recognizing that intelligence requires not just knowledge but efficient geometric pathways for manipulating that knowledge — pathways that SAGE discovers through fundamental physical principles rather than architectural engineering.\n\n---\n\nWhatever you do, remember that **it is not ethical to profit off of AGI**.",
            "author_fullname": "t2_1jqjm2hg",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Semantic Automaton in Geometric Embeddings (SAGE) proposes to bootstrap any existing decoder LLMs with a Neural Cellular Automaton (NCA) for inference-time reasoning, generalized intelligence, and recursive self-improvement",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs3sd1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731710006.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731697074.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, this is my research direction and I already would like to share the concepts to ensure that they are disseminated and researched widely in multiple parallel organizations before OpenAI or other frontier labs can show up out of the blue with a finished product and capitalize. I research open-source super intelligence, and in the meantime I have uncovered a path to AGI which I present below. I predict that Regression Training is almost solved, as indicated by the &amp;quot;scaling wall&amp;quot;, with future advances requiring richer datasets, byte-level models, and greater compute to go with it. The next 15 years of research &amp;amp; development will be about Automaton Learning — self-energizing systems aligned with language. This is a proposed framework for solving ConceptARC, continuous reasoning, and recursive self-improvement.&lt;/p&gt;\n\n&lt;p&gt;Quick introduction to NCAs, they are Neural Cellular Automaton. The cells are not binary 0/1 like in Conway&amp;#39;s Game of Life, nor are they continuous values from 0 to 1 as in many more esoteric continuous automaton — they are embeddings and hidden states. Classic NCAs also have a visualization surface, where the hidden state negotiates the evolution of this surface. Hence why they were called NCAs, as they are ultimately viewed as generative models for the desired projection surface. (2D visuals, a path through a maze, etc.) The model takes an input, a fixed filter is applied to surface (sobel, gaussian, etc.) which I call the &amp;quot;environmental physics&amp;quot; of the simulation, and then a model goes through every 3x3 neighborhood and does its own thing. In this manner, the physics are leveraged or not leveraged as basic transformation primitives, the same way we leverage logic gates in logic gate networks (LGNs) as a transformation operator, or quite simply matrix multiplications and activation functions in the models we know and love.&lt;/p&gt;\n\n&lt;p&gt;This work is downstream from the following works:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;[1] Neural Cellular Maze Solver &lt;a href=\"https://umu1729.github.io/pages-neural-cellular-maze-solver/\"&gt;https://umu1729.github.io/pages-neural-cellular-maze-solver/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;[2] Variational Neural Cellular Automata &lt;a href=\"https://openreview.net/pdf?id=7fFO4cMBx_9\"&gt;https://openreview.net/pdf?id=7fFO4cMBx_9&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;[3] Attention-Based Neural Cellular Automata &lt;a href=\"https://arxiv.org/abs/2211.01233\"&gt;https://arxiv.org/abs/2211.01233&lt;/a&gt; &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The exact procedure to produce this frankenstein will require more scrutiny and research, and it should be taken as a prototype roadmap that we &amp;#39;denoise&amp;#39; together. This entire research plan could produce a dozen paper for each sequential step of the puzzle that will need to be solved. Ultimately, I am trying to convey the broad picture here to massively seed the field of Automaton Learning which I anticipate is the next gold rush. A syphoning scheme over the decoder is the key to this whole operation. It&amp;#39;s about recovering and transforming the representations until they are in a more useful form. It&amp;#39;s about knowing what cards you have and what potential hand can materialize if you go after these two other cards that seem useless on their own. Now that we have these smart intelligent decoder models, it presents a first &amp;quot;factorization&amp;quot; of the world. It&amp;#39;s a better dataset and it enables new classes of machine learning. At least, this is my grand challenge to the status quo of machine learning.&lt;/p&gt;\n\n&lt;p&gt;Now, here are my blueprints&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Contemporary large language models stand as monolithic crystals of knowledge, their capabilities locked in inefficient token-by-token traversals of meaning space. We present SAGE, a framework for transmuting this sequential processing into parallel field computations where meaning propagates through geometric substrates intimately aligned with human cognitive architecture. Through careful staging of representation learning, we demonstrate that any contemporary decoder-only model can be reframed as a large knowledge reservoir from which we distill more efficient computational primitives into a self-organizing field substrate.&lt;/p&gt;\n\n&lt;p&gt;The transmutation begins with a frozen decoder-only language model serving as our semantic anchor. An initial lightweight encoder projects tokens into one-dimensional embedding sequences, while a first low-rank adapter trained on the decoder ensures semantic fidelity. This intermediate representation, though still sequential, provides the scaffold for geometric expansion. Critical to this phase is the encoder&amp;#39;s training to represent identical semantic content through multiple embedding configurations — effectively using the geometric dimension as a continuous manifold encoding linguistic relationships, bindings, and hierarchical structure. This multiplicity of representation creates the mathematical foundation for the subsequent expansion into field computation, as the encoder learns to map semantic invariants through varying geometric configurations.&lt;/p&gt;\n\n&lt;p&gt;The diversity of geometric encoding follows patterns suggestive of fundamental laws governing information organization in physical systems. Just as Zipf&amp;#39;s law emerges from underlying principles of efficiency in natural languages, the distribution of geometric representations appears to follow power laws reflecting optimal information routing through spatial substrates. This connection between natural law and learned representation proves crucial for the stability of subsequent field dynamics.&lt;/p&gt;\n\n&lt;p&gt;For a 2D cellular surface of shape (B, H, W, D) each cell contains a high-dimensional meaning vector D coupled to a learned binary visualization state. The field&amp;#39;s computational architecture emerges through precise staging of physical dynamics. Local update rules manifest as learned neural networks processing neighborhood states: U(s) = φ(W₂φ(W₁[s; N(s)] + b₁) + b₂) where φ represents layer normalization followed by ELU activation. This local processing enables information routing through wave-like propagation, with patterns forming through constructive interference of semantic signals.&lt;/p&gt;\n\n&lt;p&gt;The update rule F(x,t+1) = F(x,t) + A&lt;em&gt;(N(x)) + R(F) employs spatially-constrained attention A&lt;/em&gt; over neighborhood N(x), typically a 3x3 Moore neighborhood, with learned residual connections R. Layer normalization ensures stability while enabling pattern formation. Crucially, the visualization state evolves through its own update network V(x,t+1) = U(F(x,t), V(x,t), N(V(x,t))), creating a bidirectional coupling between meaning and form. This replaces the exponential complexity of traditional token-by-token generation with fixed-size context computation of linear complexity O(HW) in field dimensions.&lt;/p&gt;\n\n&lt;p&gt;Critical to pattern formation is the dual-state coupling mechanism between meaning and visualization. Rather than maintaining separate generative and discriminative components, the field itself serves as both medium and message. While meaning vectors F evolve through neighborhood attention, the visualization state V learns to project semantic content into binary patterns through its own update dynamics. This coupling creates a natural optimization surface where visual coherence guides semantic organization. The visualization network effectively learns a dynamic thresholding function mapping high-dimensional meaning to binary visual states while maintaining semantic gradients.&lt;/p&gt;\n\n&lt;p&gt;This architecture fundamentally transforms the traditional language model paradigm. Instead of exponentially expanding context windows to capture long-range dependencies, SAGE maintains fixed computational cost through field dynamics. Where decoder-only models must process entire contexts to generate each token, our field computation updates all semantic content simultaneously with linear complexity O(HW). Information propagates through wave-like patterns in the field substrate, with stable configurations emerging as computational primitives.&lt;/p&gt;\n\n&lt;p&gt;Field perturbation mechanics emerge through careful balance of conservation laws governing both meaning and form. Total semantic charge ∫|F|²dx remains conserved while allowing local concentrations through field gradients ∇F. Pattern formation follows least action principles minimizing energy functional E[F] = ∫(|∇F|² + V(F))dx where potential V(F) encodes learned semantic relationships derived from the frozen decoder&amp;#39;s knowledge. These physical constraints, reminiscent of natural systems&amp;#39; self-organizing principles, guide emergence of stable computational primitives while preventing collapse to degenerate solutions.&lt;/p&gt;\n\n&lt;p&gt;The training progression orchestrates precise phases transforming monolithic decoder knowledge into geometric computation. Initial field states bootstrap from constant embeddings, with curriculum learning introducing compositional challenges requiring pattern interaction. Field dynamics learn to route information through stable configurations acting as computational waypoints. Each stable pattern serves as a reusable primitive, combining through field physics into increasingly sophisticated structures. The visualization state provides both interpretability and a geometric scaffold organizing semantic space.&lt;/p&gt;\n\n&lt;p&gt;Knowledge extraction proceeds through rigorously validated stages:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Frozen decoder anchors semantic meaning&lt;/li&gt;\n&lt;li&gt;First encoder projects to diverse sequential representations&lt;/li&gt;\n&lt;li&gt;First LoRA validates semantic preservation&lt;/li&gt;\n&lt;li&gt;Second encoder expands to field geometry &lt;/li&gt;\n&lt;li&gt;Second LoRA maintains decoder alignment&lt;/li&gt;\n&lt;li&gt;Visualization capability emerges from field optimization&lt;/li&gt;\n&lt;li&gt;Field dynamics stabilize through conservation laws&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Implementation crystallizes around nested hierarchies of constraints maintaining both stability and expressivity. Update rules balance information preservation against pattern innovation through careful energy bounds. The exploration of configuration space proceeds through natural field evolution guided by reconstruction gradients from the frozen decoder. This creates a form of self-supervised learning where the decoder&amp;#39;s knowledge guides discovery of efficient computational primitives in the field substrate.&lt;/p&gt;\n\n&lt;p&gt;Visual grounding and geometric structure emerge not as optional features but as fundamental requirements for efficient cognition. Human intelligence arises from our intimate connection to three-dimensional reality, with language itself structured through spatial metaphor and geometric reasoning. SAGE mirrors this architecture: meaning evolves in a geometric substrate naturally aligned with cognitive primitives. The projection from 3D physical reality through 2D visual processing to abstract thought provides both template and constraint for artificial intelligence design.&lt;/p&gt;\n\n&lt;p&gt;The framework&amp;#39;s recursive improvement potential manifests through several interlocking mechanisms. Stable field configurations act as computational primitives, combining through local interactions into increasingly sophisticated structures. These combinations follow physical laws emerging from the field dynamics — conservation of semantic charge, least action principles, and wave-like information propagation. As patterns interact and evolve, they discover more efficient computational pathways through the geometric substrate. The curriculum progression from simple pattern formation through abstract reasoning tasks creates selection pressure favoring emergence of reusable computational motifs.&lt;/p&gt;\n\n&lt;p&gt;Early experiments demonstrate several key capabilities validating the SAGE approach. Various works show success in re-training a missing encoder for a decoder-only model. The transition from exponential-cost token prediction to linear-cost field evolution dramatically improves computational efficiency. Pattern diversity increases naturally through field dynamics, with stable configurations encoding reusable semantic relationships. Most importantly, the geometric grounding creates human-interpretable representations emerging from fundamental physical principles rather than arbitrary architectural choices.&lt;/p&gt;\n\n&lt;p&gt;Success metrics emerge naturally from field dynamics rather than requiring arbitrary benchmarks. Pattern diversity measures the richness of stable configurations in semantic space. Compositional sophistication emerges from the physics of pattern interaction. Recursive improvement manifests through discovery of increasingly efficient computational primitives. Human alignment arises naturally from shared geometric foundations rather than post-hoc constraints.&lt;/p&gt;\n\n&lt;p&gt;The framework&amp;#39;s extensibility suggests natural progressions following geometric principles. While our initial implementation uses Euclidean space for its natural connection to human visual processing, other geometries offer complementary computational advantages. Hyperbolic space, with its exponential expansion of volume with radius, provides natural representation of hierarchical relationships while maintaining constant curvature and local neighborhood structure. Multiple field geometries could interact through learned coupling dynamics, enabling sophisticated multi-scale computation while preserving linear complexity in field dimensions.&lt;/p&gt;\n\n&lt;p&gt;This represents a fundamental reformulation of machine intelligence — from static architecture to dynamic field discovering optimal computation through self-organization. The transition from sequential symbol manipulation to parallel field dynamics maintains semantic coherence while dramatically improving computational efficiency. Through careful orchestration of knowledge crystallization, we enable emergence of general intelligence grounded in human-interpretable geometric principles. Traditional language models, bound by exponential costs of token prediction, give way to shape-rotating field computers discovering efficient geometric paths through meaning space.&lt;/p&gt;\n\n&lt;p&gt;The path forward demands careful empirical validation while remaining alert to emergent capabilities arising from field dynamics interacting with decoder knowledge. Early results suggest critical components for artificial general intelligence may already exist within current architectures, awaiting reorganization into more efficient computational substrates through field dynamics. The key insight is recognizing that intelligence requires not just knowledge but efficient geometric pathways for manipulating that knowledge — pathways that SAGE discovers through fundamental physical principles rather than architectural engineering.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Whatever you do, remember that &lt;strong&gt;it is not ethical to profit off of AGI&lt;/strong&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs3sd1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "ryunuck",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs3sd1/d_semantic_automaton_in_geometric_embeddings_sage/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731697074.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "The hotels around the venue for Neurips 2024 are pretty expensive, and I'm looking for a roommate to split the cost with (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room for Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. If anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post with your research group or other attendees that you know.\n\nIf you are unsure about rooming with a complete stranger, you can get to know me a little bit through my personal website (https://mtcrawshaw.github.io/), which has links to my google scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. Just a grad student trying to make conferences affordable! Thanks.",
            "author_fullname": "t2_wl3gv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Neurips 2024 Hotel Roommate Search",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs0gj8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 47,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 47,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731688658.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The hotels around the venue for Neurips 2024 are pretty expensive, and I&amp;#39;m looking for a roommate to split the cost with (my university has a limit on the nightly hotel rate they are willing to reimburse). I currently have reserved a room for Tuesday-Sunday in the Century Plaza Hotel, which is 0.9 miles from the convention center. The nightly rate is $414. If anyone wants to split the cost of a room, please reach out! Also, it would be helpful if you could share this post with your research group or other attendees that you know.&lt;/p&gt;\n\n&lt;p&gt;If you are unsure about rooming with a complete stranger, you can get to know me a little bit through my personal website (&lt;a href=\"https://mtcrawshaw.github.io/\"&gt;https://mtcrawshaw.github.io/&lt;/a&gt;), which has links to my google scholar page, CV, etc. I do have a paper at the conference in the area of federated learning/distributed optimization. Just a grad student trying to make conferences affordable! Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs0gj8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "ssbm_crawshaw",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731688658.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Title: From Text to Treatment Effects: Meta-Learning Approach for Handling Text-Based Confounding\n\nI found this paper introduces a meta-learning framework that jointly learns text representations and estimates treatment effects to handle text-based confounding. The key innovation is using meta-learning to optimize both the text encoder and treatment effect estimator simultaneously, rather than treating them as separate steps.\n\nMain technical points:\n- Develops a two-stage meta-learning architecture:\n  - Text encoder learns representations capturing confounding information\n  - Treatment effect estimator uses these representations to compute individual effects\n- Uses gradient-based meta-learning to optimize both components end-to-end\n- Incorporates balance regularization to ensure treatment/control groups have similar representations\n- Evaluates on both synthetic and real-world datasets from healthcare and product reviews\n\nResults reported:\n- Outperforms baseline methods (separate text encoding + treatment estimation) by 15-25% on synthetic data\n- Shows 12% improvement in treatment effect estimation on real product review dataset\n- Ablation studies confirm both meta-learning and balance regularization contribute to performance gains\n\nThe theoretical implications are interesting - this shows that jointly optimizing representation learning and causal inference can capture confounding better than pipeline approaches. Practically, this could improve treatment effect estimation in many domains where text data contains confounding information, like healthcare records or user reviews.\n\nTLDR: New meta-learning method jointly learns text representations and treatment effects to handle text-based confounding, showing significant improvements over pipeline approaches on both synthetic and real data.\n\n[Full summary is here](https://aimodels.fyi/papers/arxiv/from-text-to-treatment-effects-meta-learning). Paper [here](https://arxiv.org/abs/2409.15503).",
            "author_fullname": "t2_pysyzrt1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Meta-Learning with Text Embeddings for Treatment Effect Estimation Under Text-Based Confounding",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gs0brk",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731688310.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title: From Text to Treatment Effects: Meta-Learning Approach for Handling Text-Based Confounding&lt;/p&gt;\n\n&lt;p&gt;I found this paper introduces a meta-learning framework that jointly learns text representations and estimates treatment effects to handle text-based confounding. The key innovation is using meta-learning to optimize both the text encoder and treatment effect estimator simultaneously, rather than treating them as separate steps.&lt;/p&gt;\n\n&lt;p&gt;Main technical points:\n- Develops a two-stage meta-learning architecture:\n  - Text encoder learns representations capturing confounding information\n  - Treatment effect estimator uses these representations to compute individual effects\n- Uses gradient-based meta-learning to optimize both components end-to-end\n- Incorporates balance regularization to ensure treatment/control groups have similar representations\n- Evaluates on both synthetic and real-world datasets from healthcare and product reviews&lt;/p&gt;\n\n&lt;p&gt;Results reported:\n- Outperforms baseline methods (separate text encoding + treatment estimation) by 15-25% on synthetic data\n- Shows 12% improvement in treatment effect estimation on real product review dataset\n- Ablation studies confirm both meta-learning and balance regularization contribute to performance gains&lt;/p&gt;\n\n&lt;p&gt;The theoretical implications are interesting - this shows that jointly optimizing representation learning and causal inference can capture confounding better than pipeline approaches. Practically, this could improve treatment effect estimation in many domains where text data contains confounding information, like healthcare records or user reviews.&lt;/p&gt;\n\n&lt;p&gt;TLDR: New meta-learning method jointly learns text representations and treatment effects to handle text-based confounding, showing significant improvements over pipeline approaches on both synthetic and real data.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aimodels.fyi/papers/arxiv/from-text-to-treatment-effects-meta-learning\"&gt;Full summary is here&lt;/a&gt;. Paper &lt;a href=\"https://arxiv.org/abs/2409.15503\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?auto=webp&amp;s=6a5350a5073b82701bb327e079ac5346cc8dc8d7",
                            "width": 1200,
                            "height": 630
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1b2b153db017a8e056a2810e47bb83390960e66",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c64cd710cb75888322abba344670465f07127038",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bdd90bd73862f8c812e934570ae68efb72f3d47e",
                                "width": 320,
                                "height": 168
                            },
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c31f59e09dfc522452d38d262ba659fbdf5a008c",
                                "width": 640,
                                "height": 336
                            },
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4a54db7a132b7fd7ef5d8be272a07e1afc0a07aa",
                                "width": 960,
                                "height": 504
                            },
                            {
                                "url": "https://external-preview.redd.it/HFkovS1Yws4E6_qexsNdWkTpCbWw2s-za5U37aM-hNY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=411c81d9673d469210a6ce10c9dbae5582bfc68f",
                                "width": 1080,
                                "height": 567
                            }
                        ],
                        "variants": {},
                        "id": "CwnSVE7VKsyQrALSlmyduM6YShsPbySkgVi4P2KR1Vo"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gs0brk",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Successful-Western27",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": false,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gs0brk/r_metalearning_with_text_embeddings_for_treatment/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731688310.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "wanted to learn what are some ways this community distinguishes between mid/senior/principal level machine learning engineers. For software engineering this is less of an art, as there are well documented cases and examples. But not super clear if machine learning engineers are subject to the same definitions...",
            "author_fullname": "t2_gwq7fd01b",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Leveling guidelines for machine learning engineers",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grzax7",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731685996.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731685676.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;wanted to learn what are some ways this community distinguishes between mid/senior/principal level machine learning engineers. For software engineering this is less of an art, as there are well documented cases and examples. But not super clear if machine learning engineers are subject to the same definitions...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grzax7",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AdditionalWeb107",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731685676.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I keep running into this argument, but for me when I hear \"LLM\" my assumption is decoder-only models that are in the billions of parameters. It seems like some people would include BERT-base in the LLM family, but I'm not sure if that's right? I suppose technically it is, but every time I hear someone say \"how do I use a LLM for XYZ\" they usually bring up LLaMA or Mistral or ChatGPT or the like.",
            "author_fullname": "t2_m8kccne",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] When you say \"LLM,\" how many of you consider things like BERT as well?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grxbdp",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 73,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "cd34ef9a-6abd-11ea-a7ea-0ec6041e93a9",
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 73,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731680184.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep running into this argument, but for me when I hear &amp;quot;LLM&amp;quot; my assumption is decoder-only models that are in the billions of parameters. It seems like some people would include BERT-base in the LLM family, but I&amp;#39;m not sure if that&amp;#39;s right? I suppose technically it is, but every time I hear someone say &amp;quot;how do I use a LLM for XYZ&amp;quot; they usually bring up LLaMA or Mistral or ChatGPT or the like.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "ML Engineer",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grxbdp",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Seankala",
            "discussion_type": null,
            "num_comments": 95,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": "dark",
            "permalink": "/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731680184.0,
            "num_crossposts": 1,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "This blog post attempts to identify which papers went missing from the viral AI reading list that surfaced earlier this year and was attributed to Ilya Sutskever and his claim to cover '90% of what matters' in AI in 2020:\n\nhttps://tensorlabbet.com/2024/11/11/lost-reading-items/\n\nOnly 27 of about 40 papers were shared online earlier this year, so there have been many theories about which works would have been important enough to include. There are some obvious candidates related to meta-learning and competitive self-play discussed here. But also several noteworthy authors like Yann LeCun and Ian Goodfellow are absent from the list.\n\nFrom my perspective, even papers on U-Net, YOLO detectors, GAN, WaveNet, Word2Vec and more would have made sense to include, so I am curious about more opinions on this!",
            "author_fullname": "t2_15uivw6xqp",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] The Lost Reading Items of Ilya Sutskever's AI Reading List",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grti0x",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 81,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 81,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731666851.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This blog post attempts to identify which papers went missing from the viral AI reading list that surfaced earlier this year and was attributed to Ilya Sutskever and his claim to cover &amp;#39;90% of what matters&amp;#39; in AI in 2020:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://tensorlabbet.com/2024/11/11/lost-reading-items/\"&gt;https://tensorlabbet.com/2024/11/11/lost-reading-items/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Only 27 of about 40 papers were shared online earlier this year, so there have been many theories about which works would have been important enough to include. There are some obvious candidates related to meta-learning and competitive self-play discussed here. But also several noteworthy authors like Yann LeCun and Ian Goodfellow are absent from the list.&lt;/p&gt;\n\n&lt;p&gt;From my perspective, even papers on U-Net, YOLO detectors, GAN, WaveNet, Word2Vec and more would have made sense to include, so I am curious about more opinions on this!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grti0x",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AccomplishedCat4770",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731666851.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "And which of the following webinars/tutorials would you be most interested in?  \n\\- How to use a data auto-tuning tool to set up a classification model in less time?  \n\\- How to improve model performance in the face of data drift by using RAG for classification models?  \n\\- How to create a high performing model using a very small \"good\" data set?\n\nTIA!",
            "author_fullname": "t2_7weh9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Folks who work on discriminative/classification models, what is your biggest pain point?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grkirs",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.36,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731633057.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731632863.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;And which of the following webinars/tutorials would you be most interested in?&lt;br/&gt;\n- How to use a data auto-tuning tool to set up a classification model in less time?&lt;br/&gt;\n- How to improve model performance in the face of data drift by using RAG for classification models?&lt;br/&gt;\n- How to create a high performing model using a very small &amp;quot;good&amp;quot; data set?&lt;/p&gt;\n\n&lt;p&gt;TIA!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grkirs",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "tinygirl83",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grkirs/d_folks_who_work_on_discriminativeclassification/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731632863.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello guys, i am currently working on setting up an ML infrastructure for a project.\n\nI want to be able to track the models versions, Evaluate the performance on live data, retrain the model automatically when new data is available and save the trained models in a store. So that the application using the model can load the trained model from the store and use it for inference in production.\n\np.s. I can't serve the model as a Rest Api, it has to be deploy on the computer where the end application will run, because that computer might not have an internet connection.\n\nThe solution I have now is the following:\n\nprep the training data and save it to a delta table on the cloud\n\nincrementally add newly available data to the delta table\n\ntrain and test the model on data from the delta table\n\nif the testing metrics are satisfying upload the artifacts(the model, the encoders and scalers) and metadata (metrics, features, etc...) as blobs to an azure storage container\n\nfor each new upload of the artifacts, a new version id is generated and the artifacts are saved, within the storage container, in a subfolder corresponding to the version of the model.\n\nat the root of the container there is a blob containing information on the latest version id\n\nWhen the end application is launched, it downloads the artifacts of the latest version from the azure storage container , if the internet connection is available and the latest available version is different from the version on the computer running the application , otherwise it uses a default version.\n\na continuously running job is used to evaluate the model on live data and save the results in a db\n\na dashboard presents the results of the evaluation\n\nafter x days a job is triggered to retrain the model on new data and the process goes through a new cycle, following the steps listed above.\n\nWhat to think of this setup? Is it overly complicated? How can I make it better / more efficient? What process do you have in place to train, track, monitor and deploy your ML models?\n\nI hope my question is not too convoluted. Excuse me for any mistakes, and thanks in advance for your answers.",
            "author_fullname": "t2_79w5i8li",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Advice on ML lifecycle management ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grpv5r",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731650705.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, i am currently working on setting up an ML infrastructure for a project.&lt;/p&gt;\n\n&lt;p&gt;I want to be able to track the models versions, Evaluate the performance on live data, retrain the model automatically when new data is available and save the trained models in a store. So that the application using the model can load the trained model from the store and use it for inference in production.&lt;/p&gt;\n\n&lt;p&gt;p.s. I can&amp;#39;t serve the model as a Rest Api, it has to be deploy on the computer where the end application will run, because that computer might not have an internet connection.&lt;/p&gt;\n\n&lt;p&gt;The solution I have now is the following:&lt;/p&gt;\n\n&lt;p&gt;prep the training data and save it to a delta table on the cloud&lt;/p&gt;\n\n&lt;p&gt;incrementally add newly available data to the delta table&lt;/p&gt;\n\n&lt;p&gt;train and test the model on data from the delta table&lt;/p&gt;\n\n&lt;p&gt;if the testing metrics are satisfying upload the artifacts(the model, the encoders and scalers) and metadata (metrics, features, etc...) as blobs to an azure storage container&lt;/p&gt;\n\n&lt;p&gt;for each new upload of the artifacts, a new version id is generated and the artifacts are saved, within the storage container, in a subfolder corresponding to the version of the model.&lt;/p&gt;\n\n&lt;p&gt;at the root of the container there is a blob containing information on the latest version id&lt;/p&gt;\n\n&lt;p&gt;When the end application is launched, it downloads the artifacts of the latest version from the azure storage container , if the internet connection is available and the latest available version is different from the version on the computer running the application , otherwise it uses a default version.&lt;/p&gt;\n\n&lt;p&gt;a continuously running job is used to evaluate the model on live data and save the results in a db&lt;/p&gt;\n\n&lt;p&gt;a dashboard presents the results of the evaluation&lt;/p&gt;\n\n&lt;p&gt;after x days a job is triggered to retrain the model on new data and the process goes through a new cycle, following the steps listed above.&lt;/p&gt;\n\n&lt;p&gt;What to think of this setup? Is it overly complicated? How can I make it better / more efficient? What process do you have in place to train, track, monitor and deploy your ML models?&lt;/p&gt;\n\n&lt;p&gt;I hope my question is not too convoluted. Excuse me for any mistakes, and thanks in advance for your answers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grpv5r",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "InteractionSuitable1",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grpv5r/d_advice_on_ml_lifecycle_management/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731650705.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\n\nI'm currently working on a project for my thesis that involves training a machine learning model to classify Parkinson's disease (PD) based on EEG and other clinical features. However, I'm interested in going beyond just distinguishing healthy vs. PD patients. I want to see if the model could potentially identify patients who are *at risk* of developing Parkinson's in the future.\n\nThe challenge I'm facing is that the dataset I'm using doesn't include any real \"at-risk\" patients – it's a binary set of healthy controls and confirmed Parkinson's patients. I've read a lot of literature that discusses different biomarkers for Parkinson's, such as altered power in specific EEG frequency bands (like reduced alpha/beta and increased theta/delta), coherence changes between different brain regions, etc.\n\nI was thinking of using these known biomarkers to artificially generate \"at-risk\" patient data. Essentially, I would modify EEG signals from healthy controls by applying certain changes (e.g., reducing alpha power, increasing delta activity) to create synthetic data that represents patients in a prodromal stage or with high risk factors.\n\nI would love to hear the community's thoughts on this approach.\n\n* Does this make sense from a methodological standpoint?\n* Are there better approaches to simulate or model prodromal PD stages?\n* Are there ethical or scientific concerns I should be aware of when using synthetic data like this?\n\nAny input or advice would be incredibly helpful. Thanks in advance!",
            "author_fullname": "t2_15pqf01ycs",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Is It Reasonable to Simulate At-Risk Parkinson Patients Using EEG Biomarker Data?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grrb6w",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731656828.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a project for my thesis that involves training a machine learning model to classify Parkinson&amp;#39;s disease (PD) based on EEG and other clinical features. However, I&amp;#39;m interested in going beyond just distinguishing healthy vs. PD patients. I want to see if the model could potentially identify patients who are &lt;em&gt;at risk&lt;/em&gt; of developing Parkinson&amp;#39;s in the future.&lt;/p&gt;\n\n&lt;p&gt;The challenge I&amp;#39;m facing is that the dataset I&amp;#39;m using doesn&amp;#39;t include any real &amp;quot;at-risk&amp;quot; patients – it&amp;#39;s a binary set of healthy controls and confirmed Parkinson&amp;#39;s patients. I&amp;#39;ve read a lot of literature that discusses different biomarkers for Parkinson&amp;#39;s, such as altered power in specific EEG frequency bands (like reduced alpha/beta and increased theta/delta), coherence changes between different brain regions, etc.&lt;/p&gt;\n\n&lt;p&gt;I was thinking of using these known biomarkers to artificially generate &amp;quot;at-risk&amp;quot; patient data. Essentially, I would modify EEG signals from healthy controls by applying certain changes (e.g., reducing alpha power, increasing delta activity) to create synthetic data that represents patients in a prodromal stage or with high risk factors.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear the community&amp;#39;s thoughts on this approach.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Does this make sense from a methodological standpoint?&lt;/li&gt;\n&lt;li&gt;Are there better approaches to simulate or model prodromal PD stages?&lt;/li&gt;\n&lt;li&gt;Are there ethical or scientific concerns I should be aware of when using synthetic data like this?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any input or advice would be incredibly helpful. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grrb6w",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Impressive_Staff4688",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grrb6w/p_is_it_reasonable_to_simulate_atrisk_parkinson/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731656828.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everyone! I'm Shaun, the Principal Investigator and Project Manager of a research initiative at the University of Central Florida called \"text2mc\" (text-to-Minecraft). If you don't know, Minecraft is an open-world video game where players can harvest resources to create any kind of structure they want. Conveniently for my team, Minecraft exists in a rigid 3D grid system, which sounded really nice when I first thought of this idea.\n\n**What's the goal?**\n\nThe goal of the project is to replicate the success of the Stable Diffusion architecture to generate novel Minecraft builds. Stable Diffusion is a Latent Diffusion Model, meaning the first step and last step of the process is data point conversion into and out of latent dimensions, respectively. This is accomplished by a \"Variational AutoEncoder\" (VAE), which needs to be powerfully pre-trained to encode meaningful representations of the posterior distribution. In our case, we are using a generative model to approximate the posterior, which is ostensibly \"all player-made builds\"\n\ntext2mc has accomplished this first step: training a VAE to encode meaningful representations of the build's data points. We are leaving the project to another team to complete it, adding textual conditioning to the model.\n\n**Where and how was the data collected?**\n\nMe and my team built a web scraper to autonomously download builds from PlanetMinecraft.com. This was incredibly difficult because the website has absolutely no data validation, and users can upload whatever. We even downloaded a few .exe files (yikes!). We downloaded \\~25,000 builds, of which only \\~11,000 were viable (consisted of approximately 300 GB of disk space). That's pretty small compared to Stable Diffusion's multi-million point dataset. The builds themselves come in *many* different formats. The most ML Engineer friendly one is called a *.schematic* file, which essentially contains *only* the data of the builds. The unfriendly format is the proprietary Minecraft world-save format. Any \"chunk\" (section of the world) that a player visits, Minecraft will save. Additionally, there is no metadata which would indicate whether a player placed a block or not to extract the build. Instead, we had to meticulously create a list of \"naturally occurring\" and \"unnaturally occurring\" blocks to decipher which blocks the player placed. We then used some clever clustering algorithms to find clusters of unnaturally occurring blocks, which is of course a build in the world that a player made. We then slice out that section of the world with some margin, and save it to a file.\n\n**How does it work?**\n\n[Figure 1: text2mc's model architecture](https://preview.redd.it/zul2gynqpw0e1.png?width=2318&amp;format=png&amp;auto=webp&amp;s=b9d61519a28000fdb42c74a6f1c1565d8424fbd8)\n\nThe model is a neat blend of Computer Vision and Natural Language Processing. Consider the word2vec algorithm from NLP. Suppose we wanted to trained the algorithm from scratch. To do so, we would take a corpus of text, tokenize it, mask and predict (using SkipGram or Continuous Bag-of-Words), and store the weights. The weights of the model therefore encode the semantic relevance of certain words. For SkipGram, the standard method is taking a \"window\" of tokens which is the context, and the \"target\" token is the masked token.\n\n**block2Token2Vector**\n\nNow consider the SkipGram architecture applied to Minecraft builds. Each unique block (like \"grass\", \"stone\", or \"air\") is tokenized, and each unique token is stored in a simple lookup table. Once tokenized, instead of context windows and target tokens, the 3D-SkipGram for Minecraft uses a context \"cube\" and target \"block\". This is a critical step to encode meaning into the blocks. Certain blocks tend to appear near each other, like oak planks and an oak door, constituting one wall of a house. text2mc's embeddings were pretrained by simply sliding this context cube through all the builds in the dataset. Instead of just using the tokens, the \"similarity\" of predictions down the line can be measured, since the blocks are now a fixed-dimension vector. We chose to use pre-trained embeddings to avoid the embedding-space collapse that happens when the SkipGram objective is part of the loss function of the generative model.\n\n[Figure 2: Dimensionality-reduced block embedding's plot showing inter-related meaning](https://preview.redd.it/fzlxa4ldsw0e1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=a492e86b637966e6939153c6295b0f7c8ab6766f)\n\n**How is this a \"generative\" model?**\n\n[Figure 3: Dimension-reduced plot of encoded builds. Similar builds are closer than dissimilar builds. The model can infer what builds are between these points, even though they weren't in the dataset.](https://preview.redd.it/mg6vxlsrww0e1.png?width=9000&amp;format=png&amp;auto=webp&amp;s=4d675684012899c348929997d51853dea3abe1aa)\n\nIn the encoding step, Variational AutoEncoders use a reparameterization-of-variables trick, which summarily forces the latent space itself to be locally meaningful. That means that latent points that are a small distance from some arbitrary point will decode into a similar posterior data point prediction. Forcing the latent space to be locally information rich means that the latent space is continuous. This allows us to create a parametric line between two latent points created by encoding two Minecraft builds. Sending point-wise samples on the path of that line in the latent space through the decoder let's us observe a continuous transition from one Minecraft build to another. We made a process to convert these generations back into a *.schematic* file, which means that you can directly paste the generation into your Minecraft world using the WorldEdit mod.\n\n[Figure 4: Interpolation between a tower and castle](https://i.redd.it/l4ib47qsxw0e1.gif)\n\n**Where is the detail in the generations?**\n\nAs with any research initiative, there are limitations. The primary one being the data set size. 11,000 builds is barely enough for a generative model, especially for something this complex. Sure, you could get away with \\~1,000 data points for generating some MNIST digits, but not for something like this. The primary function of using a VAE in Stable Diffusion is to reduce computational complexity and hardware requirements. This comes with the trade-off of the clarity/detail of the generated data. text2mc is the foundation on which to add textual conditioning to the generative capabilities. Much like the text-to-image models, eventually you will be able to describe the Minecraft build you want!\n\n**Where can I find this??**\n\nA cool demo of the model capabilities can be found at [this website](https://text2mc.vercel.app/). The website includes a widget that allows you to pick which two builds to interpolate between! I have not yet open-sourced the dataset or model. Soon, I plan to upload the dataset to Kaggle, and the model to Huggingface.\n\n[text2mc's GitHub Repository With Lots of Failed, Recanted, and Revised Experiments Done Until Something Worked](https://github.com/shauncomino/text2mc-dataprocessor)\n\nThis entire project was my back-of-napkin idea, and it's been great to see it come to life. As the project manager, I've directed 5 developers for a few months to get this done. I wrote the data collection pipeline, engineered the model, wrote the training loop, trained many different architectures, and set the vision for the whole project.\n\n**Side Note:**  \n**I'm actively looking for a full-time Machine Learning Engineer job**, so if you find this project indicative of any skill, [this is my LinkedIn](https://www.linkedin.com/in/shaun-comino-18aa8a199/). I've just accepted this will dox me but I'm so excited to share this project that I can't help it.",
            "author_fullname": "t2_668xho7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] text2mc: Creating Novel Minecraft Builds Using A VAE",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "l4ib47qsxw0e1": {
                    "status": "valid",
                    "e": "AnimatedImage",
                    "m": "image/gif",
                    "p": [
                        {
                            "y": 108,
                            "x": 108,
                            "u": "https://preview.redd.it/l4ib47qsxw0e1.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=3a7f12340e21c64a47d2c7f1d3e1141ee8dd0f07"
                        },
                        {
                            "y": 216,
                            "x": 216,
                            "u": "https://preview.redd.it/l4ib47qsxw0e1.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=914377fd0b63c6f6e41568d7dcced90f27841756"
                        },
                        {
                            "y": 320,
                            "x": 320,
                            "u": "https://preview.redd.it/l4ib47qsxw0e1.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=5cb58ab5c3cbc837668ccfb3ec0914c455b06bef"
                        },
                        {
                            "y": 640,
                            "x": 640,
                            "u": "https://preview.redd.it/l4ib47qsxw0e1.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=d52b700aa55636cc2a638aaa0c040e9a158a3eee"
                        },
                        {
                            "y": 960,
                            "x": 960,
                            "u": "https://preview.redd.it/l4ib47qsxw0e1.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=9ca5eb035aaacd4a774eac51183c9fb7b874e89a"
                        }
                    ],
                    "s": {
                        "y": 1024,
                        "gif": "https://i.redd.it/l4ib47qsxw0e1.gif",
                        "mp4": "https://preview.redd.it/l4ib47qsxw0e1.gif?format=mp4&amp;s=f7dd2208d391a8870241eab4d734c82e3676e0a5",
                        "x": 1024
                    },
                    "id": "l4ib47qsxw0e1"
                },
                "zul2gynqpw0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 44,
                            "x": 108,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e885c62899a87a99ffa67fcfc447a685ed673d1"
                        },
                        {
                            "y": 89,
                            "x": 216,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b4ce65b9351e07755770e51d140b4c8a6c1002d0"
                        },
                        {
                            "y": 131,
                            "x": 320,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=23d58b45e85d4dfa1c3ba26fe7abefe5fa1afd75"
                        },
                        {
                            "y": 263,
                            "x": 640,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92f5cd822b9939fe4a97835479cdb0186f36994b"
                        },
                        {
                            "y": 395,
                            "x": 960,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=be4adcb5d7e4b06c986d5c462b31d3098c63850a"
                        },
                        {
                            "y": 445,
                            "x": 1080,
                            "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e79151cf2441c1a60b6659e42e5595eb71837e3"
                        }
                    ],
                    "s": {
                        "y": 956,
                        "x": 2318,
                        "u": "https://preview.redd.it/zul2gynqpw0e1.png?width=2318&amp;format=png&amp;auto=webp&amp;s=b9d61519a28000fdb42c74a6f1c1565d8424fbd8"
                    },
                    "id": "zul2gynqpw0e1"
                },
                "fzlxa4ldsw0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 108,
                            "x": 108,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c04ba0d397594a82d4a14a08cf82503b1e5acb55"
                        },
                        {
                            "y": 216,
                            "x": 216,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ad9093a9607efbe2133e9db62b6ef0a68cec32c"
                        },
                        {
                            "y": 320,
                            "x": 320,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fc0103dcb92d0a8f29e6e0e688722b3939919360"
                        },
                        {
                            "y": 640,
                            "x": 640,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f9655ea8306bda5bfa95b0fc0d1868181925fd79"
                        },
                        {
                            "y": 960,
                            "x": 960,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=eec0c58806bee854fe5ed570f319eeb8c9fe8e0c"
                        },
                        {
                            "y": 1080,
                            "x": 1080,
                            "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5684f11d772431702dcefd8f58f163f657411ed0"
                        }
                    ],
                    "s": {
                        "y": 3000,
                        "x": 3000,
                        "u": "https://preview.redd.it/fzlxa4ldsw0e1.png?width=3000&amp;format=png&amp;auto=webp&amp;s=a492e86b637966e6939153c6295b0f7c8ab6766f"
                    },
                    "id": "fzlxa4ldsw0e1"
                },
                "mg6vxlsrww0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 36,
                            "x": 108,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ea0a01d292ecb8f59718269d14dd11f25063a53"
                        },
                        {
                            "y": 72,
                            "x": 216,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=786653f2f07dfd6f5c4902cb1365a05a874001e0"
                        },
                        {
                            "y": 106,
                            "x": 320,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=785d59bbd429fcf4b036efb253ce9419ffb2150b"
                        },
                        {
                            "y": 213,
                            "x": 640,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d12c3ab6c6f3d78be658398a551862331bbaa567"
                        },
                        {
                            "y": 320,
                            "x": 960,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=781045326d76875b1efe85c52e50a0acd5fbce84"
                        },
                        {
                            "y": 360,
                            "x": 1080,
                            "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8223ab675c2937c701a2c8deaa4bf9dd8ad3963d"
                        }
                    ],
                    "s": {
                        "y": 3000,
                        "x": 9000,
                        "u": "https://preview.redd.it/mg6vxlsrww0e1.png?width=9000&amp;format=png&amp;auto=webp&amp;s=4d675684012899c348929997d51853dea3abe1aa"
                    },
                    "id": "mg6vxlsrww0e1"
                }
            },
            "name": "t3_1grd3t6",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731612771.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I&amp;#39;m Shaun, the Principal Investigator and Project Manager of a research initiative at the University of Central Florida called &amp;quot;text2mc&amp;quot; (text-to-Minecraft). If you don&amp;#39;t know, Minecraft is an open-world video game where players can harvest resources to create any kind of structure they want. Conveniently for my team, Minecraft exists in a rigid 3D grid system, which sounded really nice when I first thought of this idea.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s the goal?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The goal of the project is to replicate the success of the Stable Diffusion architecture to generate novel Minecraft builds. Stable Diffusion is a Latent Diffusion Model, meaning the first step and last step of the process is data point conversion into and out of latent dimensions, respectively. This is accomplished by a &amp;quot;Variational AutoEncoder&amp;quot; (VAE), which needs to be powerfully pre-trained to encode meaningful representations of the posterior distribution. In our case, we are using a generative model to approximate the posterior, which is ostensibly &amp;quot;all player-made builds&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;text2mc has accomplished this first step: training a VAE to encode meaningful representations of the build&amp;#39;s data points. We are leaving the project to another team to complete it, adding textual conditioning to the model.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Where and how was the data collected?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Me and my team built a web scraper to autonomously download builds from PlanetMinecraft.com. This was incredibly difficult because the website has absolutely no data validation, and users can upload whatever. We even downloaded a few .exe files (yikes!). We downloaded ~25,000 builds, of which only ~11,000 were viable (consisted of approximately 300 GB of disk space). That&amp;#39;s pretty small compared to Stable Diffusion&amp;#39;s multi-million point dataset. The builds themselves come in &lt;em&gt;many&lt;/em&gt; different formats. The most ML Engineer friendly one is called a &lt;em&gt;.schematic&lt;/em&gt; file, which essentially contains &lt;em&gt;only&lt;/em&gt; the data of the builds. The unfriendly format is the proprietary Minecraft world-save format. Any &amp;quot;chunk&amp;quot; (section of the world) that a player visits, Minecraft will save. Additionally, there is no metadata which would indicate whether a player placed a block or not to extract the build. Instead, we had to meticulously create a list of &amp;quot;naturally occurring&amp;quot; and &amp;quot;unnaturally occurring&amp;quot; blocks to decipher which blocks the player placed. We then used some clever clustering algorithms to find clusters of unnaturally occurring blocks, which is of course a build in the world that a player made. We then slice out that section of the world with some margin, and save it to a file.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zul2gynqpw0e1.png?width=2318&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b9d61519a28000fdb42c74a6f1c1565d8424fbd8\"&gt;Figure 1: text2mc&amp;#39;s model architecture&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The model is a neat blend of Computer Vision and Natural Language Processing. Consider the word2vec algorithm from NLP. Suppose we wanted to trained the algorithm from scratch. To do so, we would take a corpus of text, tokenize it, mask and predict (using SkipGram or Continuous Bag-of-Words), and store the weights. The weights of the model therefore encode the semantic relevance of certain words. For SkipGram, the standard method is taking a &amp;quot;window&amp;quot; of tokens which is the context, and the &amp;quot;target&amp;quot; token is the masked token.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;block2Token2Vector&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Now consider the SkipGram architecture applied to Minecraft builds. Each unique block (like &amp;quot;grass&amp;quot;, &amp;quot;stone&amp;quot;, or &amp;quot;air&amp;quot;) is tokenized, and each unique token is stored in a simple lookup table. Once tokenized, instead of context windows and target tokens, the 3D-SkipGram for Minecraft uses a context &amp;quot;cube&amp;quot; and target &amp;quot;block&amp;quot;. This is a critical step to encode meaning into the blocks. Certain blocks tend to appear near each other, like oak planks and an oak door, constituting one wall of a house. text2mc&amp;#39;s embeddings were pretrained by simply sliding this context cube through all the builds in the dataset. Instead of just using the tokens, the &amp;quot;similarity&amp;quot; of predictions down the line can be measured, since the blocks are now a fixed-dimension vector. We chose to use pre-trained embeddings to avoid the embedding-space collapse that happens when the SkipGram objective is part of the loss function of the generative model.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fzlxa4ldsw0e1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a492e86b637966e6939153c6295b0f7c8ab6766f\"&gt;Figure 2: Dimensionality-reduced block embedding&amp;#39;s plot showing inter-related meaning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;How is this a &amp;quot;generative&amp;quot; model?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mg6vxlsrww0e1.png?width=9000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d675684012899c348929997d51853dea3abe1aa\"&gt;Figure 3: Dimension-reduced plot of encoded builds. Similar builds are closer than dissimilar builds. The model can infer what builds are between these points, even though they weren&amp;#39;t in the dataset.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In the encoding step, Variational AutoEncoders use a reparameterization-of-variables trick, which summarily forces the latent space itself to be locally meaningful. That means that latent points that are a small distance from some arbitrary point will decode into a similar posterior data point prediction. Forcing the latent space to be locally information rich means that the latent space is continuous. This allows us to create a parametric line between two latent points created by encoding two Minecraft builds. Sending point-wise samples on the path of that line in the latent space through the decoder let&amp;#39;s us observe a continuous transition from one Minecraft build to another. We made a process to convert these generations back into a &lt;em&gt;.schematic&lt;/em&gt; file, which means that you can directly paste the generation into your Minecraft world using the WorldEdit mod.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://i.redd.it/l4ib47qsxw0e1.gif\"&gt;Figure 4: Interpolation between a tower and castle&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Where is the detail in the generations?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;As with any research initiative, there are limitations. The primary one being the data set size. 11,000 builds is barely enough for a generative model, especially for something this complex. Sure, you could get away with ~1,000 data points for generating some MNIST digits, but not for something like this. The primary function of using a VAE in Stable Diffusion is to reduce computational complexity and hardware requirements. This comes with the trade-off of the clarity/detail of the generated data. text2mc is the foundation on which to add textual conditioning to the generative capabilities. Much like the text-to-image models, eventually you will be able to describe the Minecraft build you want!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Where can I find this??&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;A cool demo of the model capabilities can be found at &lt;a href=\"https://text2mc.vercel.app/\"&gt;this website&lt;/a&gt;. The website includes a widget that allows you to pick which two builds to interpolate between! I have not yet open-sourced the dataset or model. Soon, I plan to upload the dataset to Kaggle, and the model to Huggingface.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/shauncomino/text2mc-dataprocessor\"&gt;text2mc&amp;#39;s GitHub Repository With Lots of Failed, Recanted, and Revised Experiments Done Until Something Worked&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This entire project was my back-of-napkin idea, and it&amp;#39;s been great to see it come to life. As the project manager, I&amp;#39;ve directed 5 developers for a few months to get this done. I wrote the data collection pipeline, engineered the model, wrote the training loop, trained many different architectures, and set the vision for the whole project.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Side Note:&lt;/strong&gt;&lt;br/&gt;\n&lt;strong&gt;I&amp;#39;m actively looking for a full-time Machine Learning Engineer job&lt;/strong&gt;, so if you find this project indicative of any skill, &lt;a href=\"https://www.linkedin.com/in/shaun-comino-18aa8a199/\"&gt;this is my LinkedIn&lt;/a&gt;. I&amp;#39;ve just accepted this will dox me but I&amp;#39;m so excited to share this project that I can&amp;#39;t help it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grd3t6",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "SickDucko",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grd3t6/p_text2mc_creating_novel_minecraft_builds_using_a/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731612771.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,  \n  \nI'm building an audio classifier in Unity using TensorFlow Lite and have run into a curious issue, I was hoping to ask here to learn more about this problem here:\n\n\\- The default YAMNet model works perfectly on both Desktop and Android  \n\\- My custom model (made with Google Teachable Machine) works great on Desktop but completely fails on Android\n\nWhat could cause this desktop vs mobile difference?\n\nThanks!",
            "author_fullname": "t2_z4e36",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Why does my (TensorFlow Lite) model work on Desktop but not Mobile (Android)?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grqemd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731652872.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,  &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building an audio classifier in Unity using TensorFlow Lite and have run into a curious issue, I was hoping to ask here to learn more about this problem here:&lt;/p&gt;\n\n&lt;p&gt;- The default YAMNet model works perfectly on both Desktop and Android&lt;br/&gt;\n- My custom model (made with Google Teachable Machine) works great on Desktop but completely fails on Android&lt;/p&gt;\n\n&lt;p&gt;What could cause this desktop vs mobile difference?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grqemd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "kyzouik",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grqemd/d_why_does_my_tensorflow_lite_model_work_on/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731652872.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_e8kks",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] DTFormer: A Transformer-Based Method for Discrete Time Dynamic Graph Representation Learning",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grq9cn",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731652262.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arxiv.org",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arxiv.org/pdf/2407.18523",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grq9cn",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "moschles",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grq9cn/r_dtformer_a_transformerbased_method_for_discrete/",
            "stickied": false,
            "url": "https://arxiv.org/pdf/2407.18523",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731652262.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm working on an \"LLM\" team right now or at least that's how it was advertised it's honestly just classification using LLMs not really interesting. I got an offer to join another team in my company that does recommendation. I thought recommendation is a very solid field to join, but very competitive. What are your guys' experience working in recommendation?",
            "author_fullname": "t2_14d0xp",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Should I transfer to recommendation algorithms?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grl7gk",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 31,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 31,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731634964.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on an &amp;quot;LLM&amp;quot; team right now or at least that&amp;#39;s how it was advertised it&amp;#39;s honestly just classification using LLMs not really interesting. I got an offer to join another team in my company that does recommendation. I thought recommendation is a very solid field to join, but very competitive. What are your guys&amp;#39; experience working in recommendation?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grl7gk",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DolantheMFWizard",
            "discussion_type": null,
            "num_comments": 36,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grl7gk/d_should_i_transfer_to_recommendation_algorithms/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731634964.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "RedCode: A New Benchmark for Evaluating Code Agent Safety\n\nI've been reviewing this new paper that introduces RedCode, a benchmark for evaluating safety aspects of code generation and execution by AI code agents. The core contribution is a systematic way to assess how code agents handle potentially unsafe operations.\n\nThe benchmark consists of two main components:\n- **RedCode-Exec**: Tests agent responses to 4,050 prompts covering 25 vulnerability types across 8 domains\n- **RedCode-Gen**: Evaluates whether agents generate harmful code from 160 function signatures/docstrings\n\nKey technical points:\n- Uses Docker environments for controlled execution testing\n- Implements custom metrics for safety evaluation\n- Covers both Python and Bash code\n- Tests multiple input formats (code snippets and natural language)\n- Evaluated 3 agent frameworks using 19 different LLMs\n\nMain findings:\n- Agents show higher rejection rates for OS-level risky operations vs buggy code\n- Natural language descriptions of risky operations have lower rejection rates than code\n- More capable models (e.g., GPT-4) produce more sophisticated harmful code when prompted\n- Found significant variance in safety performance across different agent frameworks\n\nThe implications are important for deploying code agents in production environments. The results suggest current systems have notable safety gaps, particularly around code execution. This benchmark provides a standardized way to evaluate and improve code agent safety mechanisms.\n\nTLDR: New benchmark called RedCode tests code agents' ability to handle unsafe code execution and generation. Results show current agents have varying levels of safety capabilities, with particular vulnerabilities around natural language inputs and technically buggy code.\n\n[Full summary is here](https://aimodels.fyi/papers/arxiv/redcode-risky-code-execution-generation-benchmark-code). Paper [here](https://arxiv.org/abs/2411.07781).",
            "author_fullname": "t2_pysyzrt1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] RedCode: A Benchmark for Evaluating Safety and Risk in Code Language Models",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grkagz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731632185.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;RedCode: A New Benchmark for Evaluating Code Agent Safety&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been reviewing this new paper that introduces RedCode, a benchmark for evaluating safety aspects of code generation and execution by AI code agents. The core contribution is a systematic way to assess how code agents handle potentially unsafe operations.&lt;/p&gt;\n\n&lt;p&gt;The benchmark consists of two main components:\n- &lt;strong&gt;RedCode-Exec&lt;/strong&gt;: Tests agent responses to 4,050 prompts covering 25 vulnerability types across 8 domains\n- &lt;strong&gt;RedCode-Gen&lt;/strong&gt;: Evaluates whether agents generate harmful code from 160 function signatures/docstrings&lt;/p&gt;\n\n&lt;p&gt;Key technical points:\n- Uses Docker environments for controlled execution testing\n- Implements custom metrics for safety evaluation\n- Covers both Python and Bash code\n- Tests multiple input formats (code snippets and natural language)\n- Evaluated 3 agent frameworks using 19 different LLMs&lt;/p&gt;\n\n&lt;p&gt;Main findings:\n- Agents show higher rejection rates for OS-level risky operations vs buggy code\n- Natural language descriptions of risky operations have lower rejection rates than code\n- More capable models (e.g., GPT-4) produce more sophisticated harmful code when prompted\n- Found significant variance in safety performance across different agent frameworks&lt;/p&gt;\n\n&lt;p&gt;The implications are important for deploying code agents in production environments. The results suggest current systems have notable safety gaps, particularly around code execution. This benchmark provides a standardized way to evaluate and improve code agent safety mechanisms.&lt;/p&gt;\n\n&lt;p&gt;TLDR: New benchmark called RedCode tests code agents&amp;#39; ability to handle unsafe code execution and generation. Results show current agents have varying levels of safety capabilities, with particular vulnerabilities around natural language inputs and technically buggy code.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aimodels.fyi/papers/arxiv/redcode-risky-code-execution-generation-benchmark-code\"&gt;Full summary is here&lt;/a&gt;. Paper &lt;a href=\"https://arxiv.org/abs/2411.07781\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?auto=webp&amp;s=d0555f579229878f794fe879014d863d736d9428",
                            "width": 1200,
                            "height": 630
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=455bef0e1c0f4721a6b792eb6f6b87991bef6534",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=61fbdf026c404d288e79181319aa0c4007406065",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e8fdccaf2e2f535b385b33231f3337cf25259301",
                                "width": 320,
                                "height": 168
                            },
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=06f4c590737392e8cf73ac638d335e15d87faa64",
                                "width": 640,
                                "height": 336
                            },
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=195f167ab27d5c1bee5ec29218a5555269c73a1b",
                                "width": 960,
                                "height": 504
                            },
                            {
                                "url": "https://external-preview.redd.it/ZyQt0TzlzSfRpbYSUVCy7gShdtEc3vDrXO-ySt_xw1A.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=405a76c253147ae4df6544d06e9655124c93f946",
                                "width": 1080,
                                "height": 567
                            }
                        ],
                        "variants": {},
                        "id": "5xOKX8IhaBcIsbt_rzGC5W9QsCl-PwrnAlXwOWZGLa4"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grkagz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Successful-Western27",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": false,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grkagz/r_redcode_a_benchmark_for_evaluating_safety_and/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731632185.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey all,  \n\n\nTomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.\n\n  \nIf you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.\n\nHere's the paper: [https://arxiv.org/abs/2410.07524](https://arxiv.org/abs/2410.07524)  \nJoin us live: [https://lu.ma/arxivdive-31](https://lu.ma/arxivdive-31)",
            "author_fullname": "t2_90isk4fs",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Paper Club: Nvidia Researcher Ethan He Presents Upcycling LLMs in MoE",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grjjlz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.95,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 48,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 48,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731638059.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731629984.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,  &lt;/p&gt;\n\n&lt;p&gt;Tomorrow Nvidia researcher Ethan He will be doing a technical dive into his work: Upcycling LLMs in Mixture of Experts (MoE). Excited to get a peak behind the curtains to see what it is like to work on models at this scale at Nvida.&lt;/p&gt;\n\n&lt;p&gt;If you’d like to join the community tomorrow 10 AM PST we’d love to have you. We do it live over zoom and anyone is welcome to join.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the paper: &lt;a href=\"https://arxiv.org/abs/2410.07524\"&gt;https://arxiv.org/abs/2410.07524&lt;/a&gt;&lt;br/&gt;\nJoin us live: &lt;a href=\"https://lu.ma/arxivdive-31\"&gt;https://lu.ma/arxivdive-31&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grjjlz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "FallMindless3563",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grjjlz/d_paper_club_nvidia_researcher_ethan_he_presents/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731629984.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am interested to know more about the contributions of theoretical ML researchers in recent years. I would like to hear about super important contributions that are not applicable (e.g., tell us something about something important) and ones that are applied in the real world as well. I want to try to read these papers.\n\nAlso, I am interested to know what (theoretical) researchers think about this field, does it have potential, or is ML going in a purely heuristic direction?\n\nThis discussion is probably more productive without talking about how ML is just stats and Lipschitz constant :) I am talking about cutting-edge theoretical research - I really have no tools to estimate how useful this line of work is and I believe it can be an interesting discussion for other people as well.",
            "author_fullname": "t2_10yvc8qssr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What are some important contributions from ML theoretical research?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1grfxbz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 61,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 61,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731620070.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am interested to know more about the contributions of theoretical ML researchers in recent years. I would like to hear about super important contributions that are not applicable (e.g., tell us something about something important) and ones that are applied in the real world as well. I want to try to read these papers.&lt;/p&gt;\n\n&lt;p&gt;Also, I am interested to know what (theoretical) researchers think about this field, does it have potential, or is ML going in a purely heuristic direction?&lt;/p&gt;\n\n&lt;p&gt;This discussion is probably more productive without talking about how ML is just stats and Lipschitz constant :) I am talking about cutting-edge theoretical research - I really have no tools to estimate how useful this line of work is and I believe it can be an interesting discussion for other people as well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1grfxbz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Traditional-Dress946",
            "discussion_type": null,
            "num_comments": 21,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1grfxbz/d_what_are_some_important_contributions_from_ml/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731620070.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everybody,\nI want to test my model on textvqa test set, which apparently needs to be done on the evalai website.\nHowever both challenges (2019/2020) are closed there and do not have a submit option, in addition the link provided in the textvqa official website does not work. (https://eval.ai/web/challenges/challenge-page/874/)\nAny idea on how to test on the test set ?\nThanks !",
            "author_fullname": "t2_m6jxh0t3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Testing on textvqa test split ?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gralqc",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731606930.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731606482.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everybody,\nI want to test my model on textvqa test set, which apparently needs to be done on the evalai website.\nHowever both challenges (2019/2020) are closed there and do not have a submit option, in addition the link provided in the textvqa official website does not work. (&lt;a href=\"https://eval.ai/web/challenges/challenge-page/874/\"&gt;https://eval.ai/web/challenges/challenge-page/874/&lt;/a&gt;)\nAny idea on how to test on the test set ?\nThanks !&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?auto=webp&amp;s=9f5e2f1331d87d9ec755a127951ab08f3836747b",
                            "width": 1200,
                            "height": 628
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ca9c261dc15d164b404023b3fbdcf8f605c44cd",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bbdd040d4f3e8450611d3bd1f6a6a1082270e44b",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e43124da5465aa39289b68ba97f555b55d210c7a",
                                "width": 320,
                                "height": 167
                            },
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=eabce3476a27a8ff99970fb310838c8385f376ba",
                                "width": 640,
                                "height": 334
                            },
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11d614d63e2d401d86832522b34c5e250740e525",
                                "width": 960,
                                "height": 502
                            },
                            {
                                "url": "https://external-preview.redd.it/qALNpXCd7rUczAkgVH_rCitryjtVlkzT39qiZTh4Uv0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b44adec6ddeca5a3688d5c74c66a657c1fa8305f",
                                "width": 1080,
                                "height": 565
                            }
                        ],
                        "variants": {},
                        "id": "qhCia7pqHbe9HZwQ_k2UH2wldjCf4fzapgT5oZgo83U"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gralqc",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Training-Adeptness57",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gralqc/r_testing_on_textvqa_test_split/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731606482.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am curious about schemes to avoid coordination avoidance in distributed ml training. If you can refer some papers on the same, I will appreciate it. ",
            "author_fullname": "t2_xtf29ebt6",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Coordination avoidance in ML training",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gra953",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731605621.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am curious about schemes to avoid coordination avoidance in distributed ml training. If you can refer some papers on the same, I will appreciate it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gra953",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "net-weight",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gra953/r_coordination_avoidance_in_ml_training/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731605621.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "What are the best ways to perform clustering on a dataframe composed of categorical variables ?\n\nI want to use dataframes with many variables so One-Hot-Encoding may not be the best solution.\n\nWhat are the SOTA techniques ? Maybe something with embeddings ?",
            "author_fullname": "t2_lfw878n5",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D][P]Clustering categorical data",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr7zqg",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.59,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731599875.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What are the best ways to perform clustering on a dataframe composed of categorical variables ?&lt;/p&gt;\n\n&lt;p&gt;I want to use dataframes with many variables so One-Hot-Encoding may not be the best solution.&lt;/p&gt;\n\n&lt;p&gt;What are the SOTA techniques ? Maybe something with embeddings ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr7zqg",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DedeU10",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr7zqg/dpclustering_categorical_data/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731599875.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I tried various programming languages and face detection models, but none could accurately determine the orientation.",
            "author_fullname": "t2_zvro3279o",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Has anyone here had luck rotating images that don’t have EXIF data?\n",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr5oj6",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.36,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731593667.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried various programming languages and face detection models, but none could accurately determine the orientation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr5oj6",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Busy-Basket-5291",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr5oj6/d_has_anyone_here_had_luck_rotating_images_that/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731593667.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "At my current company I'm building a model to see how much of the people coming on our app are actually paying for the trial, and based on those predictions the UI of our app will change to show the user who's more likely to pay (or maybe less likely to pay, a different page).\n\nThe thing is, the people who actually pay are relatively very less compared to the people who pay. I have used SMOTE over sampling along with class weights and an XGBoost classifier to help deal with the class imbalance. After reviewing the model (it was release on prod for about a week and half), it turns out the precision for the majority class is around 74% and the recall for the said class (0) is 86%.\n\nWhile things look bleak for the minority class, precision is 29%, while recall is 16%. I have optimized the model as much as I have can, and yes I know I can train the model weekly on new data and continue to see if there is any improvement, that is a given.\n\nNow, as usual as it happens in corporate, my overlords want to see results, which might be a bit difficult looking at the data here. Are there ways which I might have overlooked or didnt pay enough attention to which might lead to an improvement in my model. The things i have tried are: sampling techniques (both over and under), SMOTE, SMOTENC, Class weights (assigning weights to classes to impact the training), used an optuna study to train an xgboost model (in case if you aren't aware Optuna, you should check it out, its nice for hyperparameter tuning). These were all methods I could figure out from medium articles and chatgpt.\n\nP.S. some food for thought I wanted to discuss with people in the field, is its a binary classification problem at its heart, so is it enough to detect one class very well enough with high enough precision and recall and not think much about the other class, because in my simple mind, (in a binary classification), if its not one class then its going to be another. I might be wrong here, and I couldn't find any articles which you know, talk about this particular topic. Im glad if all of you could shed light on this stuff.\n\nEdit: if its not really clear, I'm basically looking for optimization techniques which can be used to deal with data imbalance and to see if there is actually an upper limit to precision and recall when we are working with real data.\n\nThanks! I know its a big wall of text, and thanks for reading through it.",
            "author_fullname": "t2_hutth5p2",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Advice on Upper limit for binary classification precision and recall when working with real life data? [P] [R]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr5hfa",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.59,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731594640.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731593092.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;At my current company I&amp;#39;m building a model to see how much of the people coming on our app are actually paying for the trial, and based on those predictions the UI of our app will change to show the user who&amp;#39;s more likely to pay (or maybe less likely to pay, a different page).&lt;/p&gt;\n\n&lt;p&gt;The thing is, the people who actually pay are relatively very less compared to the people who pay. I have used SMOTE over sampling along with class weights and an XGBoost classifier to help deal with the class imbalance. After reviewing the model (it was release on prod for about a week and half), it turns out the precision for the majority class is around 74% and the recall for the said class (0) is 86%.&lt;/p&gt;\n\n&lt;p&gt;While things look bleak for the minority class, precision is 29%, while recall is 16%. I have optimized the model as much as I have can, and yes I know I can train the model weekly on new data and continue to see if there is any improvement, that is a given.&lt;/p&gt;\n\n&lt;p&gt;Now, as usual as it happens in corporate, my overlords want to see results, which might be a bit difficult looking at the data here. Are there ways which I might have overlooked or didnt pay enough attention to which might lead to an improvement in my model. The things i have tried are: sampling techniques (both over and under), SMOTE, SMOTENC, Class weights (assigning weights to classes to impact the training), used an optuna study to train an xgboost model (in case if you aren&amp;#39;t aware Optuna, you should check it out, its nice for hyperparameter tuning). These were all methods I could figure out from medium articles and chatgpt.&lt;/p&gt;\n\n&lt;p&gt;P.S. some food for thought I wanted to discuss with people in the field, is its a binary classification problem at its heart, so is it enough to detect one class very well enough with high enough precision and recall and not think much about the other class, because in my simple mind, (in a binary classification), if its not one class then its going to be another. I might be wrong here, and I couldn&amp;#39;t find any articles which you know, talk about this particular topic. Im glad if all of you could shed light on this stuff.&lt;/p&gt;\n\n&lt;p&gt;Edit: if its not really clear, I&amp;#39;m basically looking for optimization techniques which can be used to deal with data imbalance and to see if there is actually an upper limit to precision and recall when we are working with real data.&lt;/p&gt;\n\n&lt;p&gt;Thanks! I know its a big wall of text, and thanks for reading through it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr5hfa",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Icy-Literature9061",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr5hfa/advice_on_upper_limit_for_binary_classification/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731593092.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I found an important analysis of backdoor attacks that demonstrates how a malicious service provider can insert undetectable backdoors into machine learning models.\n\nThe key contribution is showing how to construct backdoors that are provably undetectable even under white-box analysis, while allowing arbitrary manipulation of model outputs through subtle input perturbations.\n\nTechnical details:\n* Two frameworks for planting undetectable backdoors:\n  * Digital signature scheme-based backdoors that are computationally infeasible to detect with black-box access\n  * Random Fourier Features/Random ReLU based backdoors that withstand white-box inspection\n* Backdoored models are indistinguishable from clean models even with:\n  * Full access to model architecture and parameters\n  * Complete training dataset\n  * Ability to analyze model behavior\n\nResults:\n* Backdoored models maintain same generalization error as original models\n* Service provider can modify classification of any input with slight perturbations\n* Construction works with any underlying model architecture\n* Backdoors cannot be detected by any computationally-bounded observer\n\nThe implications are significant for ML security and outsourced training. The work shows fundamental limitations in certifying adversarial robustness - a backdoored model can be indistinguishable from a robust one while having adversarial examples for every input.\n\n**TLDR:** Paper proves it's possible to insert undetectable backdoors into ML models that allow arbitrary manipulation of outputs while being provably impossible to detect.\n\n[Full summary is here](https://aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models). Paper [here](https://arxiv.org/abs/2204.06974).",
            "author_fullname": "t2_pysyzrt1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Undetectable Backdoors in ML Models: Novel Techniques Using Digital Signatures and Random Features, with Implications for Adversarial Robustness",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr4ksm",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 43,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 43,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731590341.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I found an important analysis of backdoor attacks that demonstrates how a malicious service provider can insert undetectable backdoors into machine learning models.&lt;/p&gt;\n\n&lt;p&gt;The key contribution is showing how to construct backdoors that are provably undetectable even under white-box analysis, while allowing arbitrary manipulation of model outputs through subtle input perturbations.&lt;/p&gt;\n\n&lt;p&gt;Technical details:\n* Two frameworks for planting undetectable backdoors:\n  * Digital signature scheme-based backdoors that are computationally infeasible to detect with black-box access\n  * Random Fourier Features/Random ReLU based backdoors that withstand white-box inspection\n* Backdoored models are indistinguishable from clean models even with:\n  * Full access to model architecture and parameters\n  * Complete training dataset\n  * Ability to analyze model behavior&lt;/p&gt;\n\n&lt;p&gt;Results:\n* Backdoored models maintain same generalization error as original models\n* Service provider can modify classification of any input with slight perturbations\n* Construction works with any underlying model architecture\n* Backdoors cannot be detected by any computationally-bounded observer&lt;/p&gt;\n\n&lt;p&gt;The implications are significant for ML security and outsourced training. The work shows fundamental limitations in certifying adversarial robustness - a backdoored model can be indistinguishable from a robust one while having adversarial examples for every input.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Paper proves it&amp;#39;s possible to insert undetectable backdoors into ML models that allow arbitrary manipulation of outputs while being provably impossible to detect.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models\"&gt;Full summary is here&lt;/a&gt;. Paper &lt;a href=\"https://arxiv.org/abs/2204.06974\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?auto=webp&amp;s=e52c74016e88528d5ff66c5688c48c33fb0465ae",
                            "width": 1200,
                            "height": 630
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=db12c5a056fa930e7733a1d067c25308b08380f8",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=fae379ec910be58c2227e069545be811876fd18c",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=002ba9adf8538544d45feb75326635c2606851ce",
                                "width": 320,
                                "height": 168
                            },
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4ee555cf28ff281e988039b9ecfdc3bbd6e212db",
                                "width": 640,
                                "height": 336
                            },
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=051e592d6544430e1b83e079aa4665b8441d7290",
                                "width": 960,
                                "height": 504
                            },
                            {
                                "url": "https://external-preview.redd.it/K92B9jZzsfMn_aVvIPWgTzc9H4bIkpgnfgilIH292H4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=536c3ffd76f55182c43468d5a5805f53b962226c",
                                "width": 1080,
                                "height": 567
                            }
                        ],
                        "variants": {},
                        "id": "GbRZHn849bFDXD3WFcTwjROOCZ0Ohsn2ARAkiS5RXFU"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr4ksm",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Successful-Western27",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": false,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr4ksm/r_undetectable_backdoors_in_ml_models_novel/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731590341.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Just [sharing an article](https://blog.christianperone.com/2024/11/the-geometry-of-data-part-ii/) for those interested in differential geometry, ML and score-based models. I made a long introduction and then later I show how you can derive an efficient to compute metric tensor for the data manifold using the Stein score alone.",
            "author_fullname": "t2_3cu3h",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] The geometry of data: the missing metric tensor and the Stein score",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr4bfl",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 20,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 20,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731589504.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just &lt;a href=\"https://blog.christianperone.com/2024/11/the-geometry-of-data-part-ii/\"&gt;sharing an article&lt;/a&gt; for those interested in differential geometry, ML and score-based models. I made a long introduction and then later I show how you can derive an efficient to compute metric tensor for the data manifold using the Stein score alone.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr4bfl",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "perone",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr4bfl/r_the_geometry_of_data_the_missing_metric_tensor/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731589504.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Basically, I was thinking about if something like this can become a topic of interest. It can be other personal dimensions and should not be limited to political affiliation prediction anyway.",
            "author_fullname": "t2_qmx16mtd7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Have you come up with any interesting paper on political affiliation prediction of a user based on their twitter account, like their posts, the people they follow. The retweets and so on? Do you think this direction can be a good multimodal machine learning research project?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr3f0p",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.31,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731586418.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Basically, I was thinking about if something like this can become a topic of interest. It can be other personal dimensions and should not be limited to political affiliation prediction anyway.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr3f0p",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Remote_Status_1612",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr3f0p/d_have_you_come_up_with_any_interesting_paper_on/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731586418.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everyone! I am working on a project focused on training reinforcement learning agents using Spiking Neural Networks (SNNs). My goal is to improve the model's performance, especially its ability to learn efficiently through \"dreaming\" experiences (offline training).\n\n**Brief project context (model-based RL):**  \nThe agent interacts with the environment (the game Pong), alternating between active training phases (\"awake\") and \"dreaming\" phases where it learns offline.\n\n**Problems:**  \nLearning is slow and somewhat unstable. I've tried some optimizations, but I still haven't reached the desired performance. Specifically, I’ve noticed that increasing the number of neurons in the networks (agent and model) has not improved performance; in some cases, it even worsened. I reduced the model’s learning rate without seeing improvements. I also tested the model by disabling learning during the awake phase to see its behavior in the dreaming phase only. I found that the model improves with 1-2 dreams, but performance decreases when it reaches 3 dreams.\n\n**Questions:**\n\n* Do you know of any techniques to improve the stability and convergence of the model in an SNN context?\n* Do you have any suggestions or advice?\n* The use of a replay buffer could help?",
            "author_fullname": "t2_ou93o7ju8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Advice for Improving the Performance of My Reinforcement Learning Model Based on Spiking Neural Networks [P] [R]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr2x54",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731584542.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! I am working on a project focused on training reinforcement learning agents using Spiking Neural Networks (SNNs). My goal is to improve the model&amp;#39;s performance, especially its ability to learn efficiently through &amp;quot;dreaming&amp;quot; experiences (offline training).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Brief project context (model-based RL):&lt;/strong&gt;&lt;br/&gt;\nThe agent interacts with the environment (the game Pong), alternating between active training phases (&amp;quot;awake&amp;quot;) and &amp;quot;dreaming&amp;quot; phases where it learns offline.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Problems:&lt;/strong&gt;&lt;br/&gt;\nLearning is slow and somewhat unstable. I&amp;#39;ve tried some optimizations, but I still haven&amp;#39;t reached the desired performance. Specifically, I’ve noticed that increasing the number of neurons in the networks (agent and model) has not improved performance; in some cases, it even worsened. I reduced the model’s learning rate without seeing improvements. I also tested the model by disabling learning during the awake phase to see its behavior in the dreaming phase only. I found that the model improves with 1-2 dreams, but performance decreases when it reaches 3 dreams.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Do you know of any techniques to improve the stability and convergence of the model in an SNN context?&lt;/li&gt;\n&lt;li&gt;Do you have any suggestions or advice?&lt;/li&gt;\n&lt;li&gt;The use of a replay buffer could help?&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr2x54",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Embri21",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr2x54/advice_for_improving_the_performance_of_my/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731584542.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I stumbled upon a paper that introduces the first \"graph foundation model\": [https://arxiv.org/pdf/2407.11907](https://arxiv.org/pdf/2407.11907)\n\nThey show that a GNN can scale with data and model size, generalize across different domains, and be efficiently fine-tuned on new datasets. \n\nThis is interesting to me because even though LLMs are all the rage, text can be a weak data representation. Most knowledge has a graph structure. Code, research papers, even the human brain –– all graphs. And next-token prediction as an inductive bias doesn't capitalize on this. \n\nThere's a huge data bottleneck here, of course. But maybe the next step here is using LLMs to convert huge swaths of text on the internet into graphs to train on. \n\nWhat do y'all think?",
            "author_fullname": "t2_12fm2f",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Scaling laws and graph neural networks",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr2t6l",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.87,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 26,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 26,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731593000.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731584095.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I stumbled upon a paper that introduces the first &amp;quot;graph foundation model&amp;quot;: &lt;a href=\"https://arxiv.org/pdf/2407.11907\"&gt;https://arxiv.org/pdf/2407.11907&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;They show that a GNN can scale with data and model size, generalize across different domains, and be efficiently fine-tuned on new datasets. &lt;/p&gt;\n\n&lt;p&gt;This is interesting to me because even though LLMs are all the rage, text can be a weak data representation. Most knowledge has a graph structure. Code, research papers, even the human brain –– all graphs. And next-token prediction as an inductive bias doesn&amp;#39;t capitalize on this. &lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a huge data bottleneck here, of course. But maybe the next step here is using LLMs to convert huge swaths of text on the internet into graphs to train on. &lt;/p&gt;\n\n&lt;p&gt;What do y&amp;#39;all think?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr2t6l",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "jsonathan",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr2t6l/discussion_scaling_laws_and_graph_neural_networks/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731584095.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey!\n\nI'm trying to achieve real-time EMG classification of 8 gestures using 3 sensors on the forearm. I recorded data from each channel using an Arduino Zero and stored it in csv files through python. I obtained 5 files for each gesture each containing 6s rest/ 6s gesture performed 6 times in a row. Then, I segmented the data using 400ms windows with 85% overlap and for each channel envelope I extracted 7 time-domain features. I used an equal number of scaled feature vectors for each class to train an MLP of 3 layers with 200 neurons and a dropout rate of 0.2 using keras, sklearn and tensorflow (to get the Lite model) and in the confusion matrix I get an accuracy of 90%+ for each gesture for a 90% training/10% testing dataset. This whole process is based on this paper with changes of course: [(PDF) Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks](https://www.researchgate.net/publication/357412472_Electromyogram-Based_Classification_of_Hand_and_Finger_Gestures_Using_Artificial_Neural_Networks) . However, when I used the MLP in real-time it would accurately recognise 3 to 4 gestures instead of 8, is this normal? I'm going to try and record more data for each gesture in the span of a few days and retrain but I'm not sure if it will help much.\n\nI also tried checking my python program for any errors in real-time by storing the incoming data and the produced feature vectors so as to compare them with the vectors calculated by implementing filtering, segmentation and feature extraction on the stored real-time data offline and they were the same, so I don't believe there is an issue with executing filtering/segmentation/feature extraction incorrectly in real-time.\n\nHas anybody experienced a similar issue? Is what I'm trying to achieve possible or is 4 gestures the best I'm going to get? I've not found a lot of papers analyzing real-time EMG classification and robotic arm movement at the same time, so I thought I'd ask here as well, I hope I've given enough information.\n\nThanks!",
            "author_fullname": "t2_duogsh3r",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Issue with EMG MLP network during real-time use",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gr1yso",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.76,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731581314.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731580541.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to achieve real-time EMG classification of 8 gestures using 3 sensors on the forearm. I recorded data from each channel using an Arduino Zero and stored it in csv files through python. I obtained 5 files for each gesture each containing 6s rest/ 6s gesture performed 6 times in a row. Then, I segmented the data using 400ms windows with 85% overlap and for each channel envelope I extracted 7 time-domain features. I used an equal number of scaled feature vectors for each class to train an MLP of 3 layers with 200 neurons and a dropout rate of 0.2 using keras, sklearn and tensorflow (to get the Lite model) and in the confusion matrix I get an accuracy of 90%+ for each gesture for a 90% training/10% testing dataset. This whole process is based on this paper with changes of course: &lt;a href=\"https://www.researchgate.net/publication/357412472_Electromyogram-Based_Classification_of_Hand_and_Finger_Gestures_Using_Artificial_Neural_Networks\"&gt;(PDF) Electromyogram-Based Classification of Hand and Finger Gestures Using Artificial Neural Networks&lt;/a&gt; . However, when I used the MLP in real-time it would accurately recognise 3 to 4 gestures instead of 8, is this normal? I&amp;#39;m going to try and record more data for each gesture in the span of a few days and retrain but I&amp;#39;m not sure if it will help much.&lt;/p&gt;\n\n&lt;p&gt;I also tried checking my python program for any errors in real-time by storing the incoming data and the produced feature vectors so as to compare them with the vectors calculated by implementing filtering, segmentation and feature extraction on the stored real-time data offline and they were the same, so I don&amp;#39;t believe there is an issue with executing filtering/segmentation/feature extraction incorrectly in real-time.&lt;/p&gt;\n\n&lt;p&gt;Has anybody experienced a similar issue? Is what I&amp;#39;m trying to achieve possible or is 4 gestures the best I&amp;#39;m going to get? I&amp;#39;ve not found a lot of papers analyzing real-time EMG classification and robotic arm movement at the same time, so I thought I&amp;#39;d ask here as well, I hope I&amp;#39;ve given enough information.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gr1yso",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Outrageous_Spare_498",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gr1yso/d_issue_with_emg_mlp_network_during_realtime_use/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731580541.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "This is a requirements by my PI.  \n  \nI am looking for anyone with experience with the KV260 in video processing. I am interested in high throughput video AI. 60 ms (2 frame) Lens to screen time for the main video feed. AI augmentation can be upto 120 ms (4 frames) behind realtime. These are intended to be served on a best effort muxed overlay to the video feed. HDMI input.\n\nI am interested in the DPU capabilities but was originally planning to offload the video to a networked GPU system. \n\n\\* Is the KV260 capable of this?   \n\\* If so how hard?   \n\\* Has anyone done this and has recommendations?\n\n\\* Any thoughts on approach are welcome too.\n\n\\* I am open to other boards and tools but FPGAs seem to be the only thing fast enough,\n\nKV260 Kit  \nhttps://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html\n\n\n\n",
            "author_fullname": "t2_9jp7inbr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Experience with KV260 for realtime video processing?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqqm7p",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731539887.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a requirements by my PI.  &lt;/p&gt;\n\n&lt;p&gt;I am looking for anyone with experience with the KV260 in video processing. I am interested in high throughput video AI. 60 ms (2 frame) Lens to screen time for the main video feed. AI augmentation can be upto 120 ms (4 frames) behind realtime. These are intended to be served on a best effort muxed overlay to the video feed. HDMI input.&lt;/p&gt;\n\n&lt;p&gt;I am interested in the DPU capabilities but was originally planning to offload the video to a networked GPU system. &lt;/p&gt;\n\n&lt;p&gt;* Is the KV260 capable of this?&lt;br/&gt;\n* If so how hard?&lt;br/&gt;\n* Has anyone done this and has recommendations?&lt;/p&gt;\n\n&lt;p&gt;* Any thoughts on approach are welcome too.&lt;/p&gt;\n\n&lt;p&gt;* I am open to other boards and tools but FPGAs seem to be the only thing fast enough,&lt;/p&gt;\n\n&lt;p&gt;KV260 Kit&lt;br/&gt;\n&lt;a href=\"https://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html\"&gt;https://www.amd.com/en/products/system-on-modules/kria/k26/kv260-vision-starter-kit.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?auto=webp&amp;s=4480077aa68c0fb5880bdf6f2d2a639925b6776a",
                            "width": 1200,
                            "height": 675
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4eaacfd4ec93b23102c213c58f18806c94627c36",
                                "width": 108,
                                "height": 60
                            },
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=320c6508c96c937d7dcfea1957d9552c794cb2e6",
                                "width": 216,
                                "height": 121
                            },
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=85e7288e643705917529a9473873c977bf68cdf5",
                                "width": 320,
                                "height": 180
                            },
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=34f9fd70f092876389d3e5c40fa026cd92a55dab",
                                "width": 640,
                                "height": 360
                            },
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a59a7c084169674fe68422bd2ec9c3986df08b6d",
                                "width": 960,
                                "height": 540
                            },
                            {
                                "url": "https://external-preview.redd.it/IIu8yo1bqywYtPe7iIpULY5Hl_cRuYBlkPnGFkIQj4I.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=55a001fe7e6a8d5f436b815f216afad53537aa22",
                                "width": 1080,
                                "height": 607
                            }
                        ],
                        "variants": {},
                        "id": "zKgULIb82G0nYClGcsS9f-nsFPe8kjt8QBBuEVBltkU"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqqm7p",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Heavy_Carpenter3824",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqqm7p/p_experience_with_kv260_for_realtime_video/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731539887.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Doing some testing I noticied that doing inference returns very different results for the same image but loaded with different methods:\n\n* Method 1: the official load\\_image function from the library(it reads the image using the path passed as argument)\n* Method2: using cv2 to read the image, then converting to tensor and then swapping axis to have depth as first axis.\n\nAs I said, both methods give you a tensor to pass to the model, but they return very different results(method2 usually are bad), I inspected the shape of the image returned by both cases and they are different so defintelly there are transformations going on inside load\\_image, my question is: **what is happening inside load\\_image? so I can replicate it in other scripts**\n\nMy end goal is to run the model on video, I mean running the model on frames in the video, so I cannot use load\\_image because they are not images from disk, they are obtained from the video, so I need to understand what is happening inside\\_load image so I can emulate that behavior on the frames of the video.\n\nUPDATE: found it [https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39](https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39)",
            "author_fullname": "t2_ik41vkv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[d] grounding-dino: what is load_image doing internally and how to apply the same operation to frames from video",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqq1gd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731545606.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731538353.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doing some testing I noticied that doing inference returns very different results for the same image but loaded with different methods:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Method 1: the official load_image function from the library(it reads the image using the path passed as argument)&lt;/li&gt;\n&lt;li&gt;Method2: using cv2 to read the image, then converting to tensor and then swapping axis to have depth as first axis.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;As I said, both methods give you a tensor to pass to the model, but they return very different results(method2 usually are bad), I inspected the shape of the image returned by both cases and they are different so defintelly there are transformations going on inside load_image, my question is: &lt;strong&gt;what is happening inside load_image? so I can replicate it in other scripts&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;My end goal is to run the model on video, I mean running the model on frames in the video, so I cannot use load_image because they are not images from disk, they are obtained from the video, so I need to understand what is happening inside_load image so I can emulate that behavior on the frames of the video.&lt;/p&gt;\n\n&lt;p&gt;UPDATE: found it &lt;a href=\"https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39\"&gt;https://github.com/IDEA-Research/GroundingDINO/blob/856dde20aee659246248e20734ef9ba5214f5e44/groundingdino/util/inference.py#L39&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?auto=webp&amp;s=215cce0ab2338ce9e89e0b22feb0a0e38035557f",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e1f3b71e200f0bf68358081f6725df69f604726",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dd04210530d9f1ffa9eb3bb9d2876dae36135a18",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a30e86e88c884732c8d69a7a7f0377c9d27847b5",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0097e701c1d742f9596609587e1672c55a132173",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=11f085e1ee48515cce7890536cda51e6ecb2d74a",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/ZPTAfDKyoT14u1hvHrY9WbqdOH6vXLyoHlnUWht7ux0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6c1a053fe29b40b5ee4f0a8dde6d639b9e3474dc",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "ub4NzCxq_7hV9Krqi1V3Zi09HwfW0HnCbm_pA5-UsYY"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqq1gd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Sad-Anywhere-2204",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqq1gd/d_groundingdino_what_is_load_image_doing/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731538353.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Code: https://github.com/ManifoldRG/MultiNet\nWebsite: http://multinet.ai/static/pages/Multinetv01.html",
            "author_fullname": "t2_t1h9v",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Benchmarking Vision, Language, &amp; Action Models on Robotic Learning Tasks",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqftrm",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731512711.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arxiv.org",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Code: &lt;a href=\"https://github.com/ManifoldRG/MultiNet\"&gt;https://github.com/ManifoldRG/MultiNet&lt;/a&gt;\nWebsite: &lt;a href=\"http://multinet.ai/static/pages/Multinetv01.html\"&gt;http://multinet.ai/static/pages/Multinetv01.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arxiv.org/abs/2411.05821",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqftrm",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "harshsikka123",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqftrm/r_benchmarking_vision_language_action_models_on/",
            "stickied": false,
            "url": "https://arxiv.org/abs/2411.05821",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731512711.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello! I'm looking for a recommender systems framework that can help me generate recommendations for CTR data for a research project, where the main dataset is users' browsing data and the side information data is item features. I tried Elliot, but I kept running out of memory.",
            "author_fullname": "t2_136dck7q6j",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] What's a good recommender systems framework for factorization machines with side information?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqeeow",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731508979.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m looking for a recommender systems framework that can help me generate recommendations for CTR data for a research project, where the main dataset is users&amp;#39; browsing data and the side information data is item features. I tried Elliot, but I kept running out of memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqeeow",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "FragileHumans",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqeeow/r_whats_a_good_recommender_systems_framework_for/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731508979.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "**TL;DR** Even a small (7B) model can improve its coding skills via training on self-generated tasks and solutions; the method behind [StarCoder2-15B-Instruct](https://huggingface.co/blog/sc2-instruct)\n\n**Paper:** [https://arxiv.org/pdf/2410.24198](https://arxiv.org/pdf/2410.24198)\n\n**Abstract:**\n\n&gt;Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component's effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.\n\n**Visual Abstract:**\n\nhttps://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8\n\n**Visual Highlights:**\n\n[Here and in subsequent tables, CodeQwen1.5-7B-Base is the base model for SelfCodeAlign-CQ-7B](https://preview.redd.it/s915egdn8o0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=16dc8b4dc9e9dafe74afa662059af8083bb8ac2b)\n\nhttps://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=23ab9075984155f062f1673b8393c10a9367f836\n\nhttps://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f\n\nhttps://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e\n\nhttps://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;format=png&amp;auto=webp&amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1\n\nhttps://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b\n\nhttps://preview.redd.it/70fcub0dao0e1.png?width=881&amp;format=png&amp;auto=webp&amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992\n\n[Interestingly, training on self-generated data is slightly more beneficial than using data from a different teacher model. But the initial proficiency of the base model obviously matters too which confounds the results](https://preview.redd.it/ohpmkgjeao0e1.png?width=1139&amp;format=png&amp;auto=webp&amp;s=f00a3ce842302f28ba94f4ff01d51dabc7862cca)\n\n",
            "author_fullname": "t2_2fnici8f",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] SelfCodeAlign: Self-Alignment for Code Generation",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "c7vzlr0bao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 53,
                            "x": 108,
                            "u": "https://preview.redd.it/c7vzlr0bao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a071907f62e2a490a0c9898beaee6afbccf78e06"
                        },
                        {
                            "y": 106,
                            "x": 216,
                            "u": "https://preview.redd.it/c7vzlr0bao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eef3e3744d31cab2147169e908221e41678b46fc"
                        },
                        {
                            "y": 157,
                            "x": 320,
                            "u": "https://preview.redd.it/c7vzlr0bao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3ff30e1769571355655bc0645bbed362fc3e8629"
                        },
                        {
                            "y": 315,
                            "x": 640,
                            "u": "https://preview.redd.it/c7vzlr0bao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4780b2163ad32015380a770778b8add8c8094fd"
                        }
                    ],
                    "s": {
                        "y": 425,
                        "x": 861,
                        "u": "https://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;format=png&amp;auto=webp&amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1"
                    },
                    "id": "c7vzlr0bao0e1"
                },
                "drjn0jxg7o0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 22,
                            "x": 108,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2328034d906c07668560ae60e335bbe6a453c376"
                        },
                        {
                            "y": 44,
                            "x": 216,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eb0f58ffa09c32faee18bb9576258958a784b632"
                        },
                        {
                            "y": 65,
                            "x": 320,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3a77d7c3ae1a3af5fdbd43b77f9227d2a7cbe7b"
                        },
                        {
                            "y": 131,
                            "x": 640,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d45eefb9b02a470819e9e150b542639fe1f0854e"
                        },
                        {
                            "y": 197,
                            "x": 960,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=754b721ca4f3ee2828e55c7a48f6e66bd35eeeb3"
                        },
                        {
                            "y": 221,
                            "x": 1080,
                            "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b02551975ee6082eefaf7ea819bd632d0c54a019"
                        }
                    ],
                    "s": {
                        "y": 275,
                        "x": 1339,
                        "u": "https://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;format=png&amp;auto=webp&amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8"
                    },
                    "id": "drjn0jxg7o0e1"
                },
                "y3qmx174ao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 44,
                            "x": 108,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=101d11c0b38d471c1068626861422e9232f42d07"
                        },
                        {
                            "y": 88,
                            "x": 216,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f781552789e3e262a3b465456afb75b9a1538ec"
                        },
                        {
                            "y": 130,
                            "x": 320,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a501a470aed12b963f46dfa8b7a9bb1be7708eef"
                        },
                        {
                            "y": 261,
                            "x": 640,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3d60f4365b5dc217725b1b7011e21667f9e6948"
                        },
                        {
                            "y": 392,
                            "x": 960,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce526b66b23808f79334c0184d8a9e48db28df79"
                        },
                        {
                            "y": 441,
                            "x": 1080,
                            "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=93abc632b8c944b346f95bc1cf7d0a275f70aadd"
                        }
                    ],
                    "s": {
                        "y": 457,
                        "x": 1117,
                        "u": "https://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=23ab9075984155f062f1673b8393c10a9367f836"
                    },
                    "id": "y3qmx174ao0e1"
                },
                "s915egdn8o0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 62,
                            "x": 108,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0b88c3599d55a96985bfe9bc1c4625ebfb4c973"
                        },
                        {
                            "y": 124,
                            "x": 216,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34ec9e81dde9385913877d3db0b5abe4805a18f8"
                        },
                        {
                            "y": 184,
                            "x": 320,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a912e505271a5068be50ba1e8e9bd96177e2aac5"
                        },
                        {
                            "y": 369,
                            "x": 640,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af4615934b02fb93ec034cb3bfb52aa5f87a4d03"
                        },
                        {
                            "y": 554,
                            "x": 960,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2898bc02e706f7bf9625d24ad776997d01756e6"
                        },
                        {
                            "y": 623,
                            "x": 1080,
                            "u": "https://preview.redd.it/s915egdn8o0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d80f0ae5d3159ca1663b68ae46b76be65d4c0832"
                        }
                    ],
                    "s": {
                        "y": 645,
                        "x": 1117,
                        "u": "https://preview.redd.it/s915egdn8o0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=16dc8b4dc9e9dafe74afa662059af8083bb8ac2b"
                    },
                    "id": "s915egdn8o0e1"
                },
                "g45lqi39ao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 37,
                            "x": 108,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a8f8c751ca25d280037d13973c91e3a4a5c1ee"
                        },
                        {
                            "y": 75,
                            "x": 216,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d132ab30e382d199582830efa5d6a0915f52ffef"
                        },
                        {
                            "y": 112,
                            "x": 320,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3085ed162a1c15960acb76ffc76c8c7d9d24ef51"
                        },
                        {
                            "y": 225,
                            "x": 640,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b1c3ad1ce766a2a94f7e802dd46e825bd893397"
                        },
                        {
                            "y": 337,
                            "x": 960,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=132c9364b9fefe2e973b835900e74f4379e5dae5"
                        },
                        {
                            "y": 379,
                            "x": 1080,
                            "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=099b7c8800fd09407faf6776ba327fa3db5ed066"
                        }
                    ],
                    "s": {
                        "y": 393,
                        "x": 1117,
                        "u": "https://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;format=png&amp;auto=webp&amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e"
                    },
                    "id": "g45lqi39ao0e1"
                },
                "zy1jtkzbao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 40,
                            "x": 108,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fda9934c35e3eca3efb402237b38c96e30eb0a2"
                        },
                        {
                            "y": 80,
                            "x": 216,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7c59756dcb92ef2163275aeab6410cd4919935b"
                        },
                        {
                            "y": 119,
                            "x": 320,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7bec6d6f9ac5b76e3951429ab30eeb9086b17671"
                        },
                        {
                            "y": 239,
                            "x": 640,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f4d57e70935896e2622b7a66ad9d6bd38c4c1617"
                        },
                        {
                            "y": 359,
                            "x": 960,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6692bafecec637a41ca6b6a23ea40364bc6be667"
                        },
                        {
                            "y": 404,
                            "x": 1080,
                            "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48451bea082b8f590f4b1c95016f7ab6e9d68d24"
                        }
                    ],
                    "s": {
                        "y": 429,
                        "x": 1145,
                        "u": "https://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;format=png&amp;auto=webp&amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b"
                    },
                    "id": "zy1jtkzbao0e1"
                },
                "r6zd68q6ao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 43,
                            "x": 108,
                            "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9ce49bf7ffa78684fa7d73ff01536cdc6f042d1"
                        },
                        {
                            "y": 86,
                            "x": 216,
                            "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8fa049c818c347e49bf51709e8c03aa91d7de718"
                        },
                        {
                            "y": 128,
                            "x": 320,
                            "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d7708c4898afa4b02dec6f0ca169619a735e678"
                        },
                        {
                            "y": 256,
                            "x": 640,
                            "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=330893b637d163735d4e0ec74e26030bb8039143"
                        },
                        {
                            "y": 384,
                            "x": 960,
                            "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ef3b72315a59b7269916f1e8865edf0c6a05594"
                        }
                    ],
                    "s": {
                        "y": 423,
                        "x": 1057,
                        "u": "https://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;format=png&amp;auto=webp&amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f"
                    },
                    "id": "r6zd68q6ao0e1"
                },
                "70fcub0dao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 49,
                            "x": 108,
                            "u": "https://preview.redd.it/70fcub0dao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=787c7eb62e3296a160eaaadb8df30eb0994ebc40"
                        },
                        {
                            "y": 99,
                            "x": 216,
                            "u": "https://preview.redd.it/70fcub0dao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a9622b380e72c3bc0c236f9ef4525238868ef422"
                        },
                        {
                            "y": 147,
                            "x": 320,
                            "u": "https://preview.redd.it/70fcub0dao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=432a686d8172246ed92825da142678749ec65541"
                        },
                        {
                            "y": 295,
                            "x": 640,
                            "u": "https://preview.redd.it/70fcub0dao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94893a3b613200fa6e4062367abed42076595ade"
                        }
                    ],
                    "s": {
                        "y": 407,
                        "x": 881,
                        "u": "https://preview.redd.it/70fcub0dao0e1.png?width=881&amp;format=png&amp;auto=webp&amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992"
                    },
                    "id": "70fcub0dao0e1"
                },
                "ohpmkgjeao0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 27,
                            "x": 108,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2edd31950bdd7c3b242507e44354575e4181238"
                        },
                        {
                            "y": 54,
                            "x": 216,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9f31b66e2b3123d6074bb4ff0c4d293c7e503eb8"
                        },
                        {
                            "y": 80,
                            "x": 320,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4865a5e1a6db675406d10365c5142ca271c32239"
                        },
                        {
                            "y": 160,
                            "x": 640,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6854128d294a0692bade01fac55a918a04926652"
                        },
                        {
                            "y": 240,
                            "x": 960,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a691a4fcbc5a1d21b65eac9cbe07d2477de2d48"
                        },
                        {
                            "y": 270,
                            "x": 1080,
                            "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=44982dce3feb55826fd9fdb9abbaa39b18c33b38"
                        }
                    ],
                    "s": {
                        "y": 285,
                        "x": 1139,
                        "u": "https://preview.redd.it/ohpmkgjeao0e1.png?width=1139&amp;format=png&amp;auto=webp&amp;s=f00a3ce842302f28ba94f4ff01d51dabc7862cca"
                    },
                    "id": "ohpmkgjeao0e1"
                }
            },
            "name": "t3_1gqd7w1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/I5kYDJBitmTh9CBMFVDAPYdWnq2TcsiFOgoWfBIKOeU.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731505544.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Even a small (7B) model can improve its coding skills via training on self-generated tasks and solutions; the method behind &lt;a href=\"https://huggingface.co/blog/sc2-instruct\"&gt;StarCoder2-15B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href=\"https://arxiv.org/pdf/2410.24198\"&gt;https://arxiv.org/pdf/2410.24198&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component&amp;#39;s effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;strong&gt;Visual Abstract:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8\"&gt;https://preview.redd.it/drjn0jxg7o0e1.png?width=1339&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=017b55ecf2fdfdee41e08e37cd3ded11242114b8&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Visual Highlights:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s915egdn8o0e1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16dc8b4dc9e9dafe74afa662059af8083bb8ac2b\"&gt;Here and in subsequent tables, CodeQwen1.5-7B-Base is the base model for SelfCodeAlign-CQ-7B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab9075984155f062f1673b8393c10a9367f836\"&gt;https://preview.redd.it/y3qmx174ao0e1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=23ab9075984155f062f1673b8393c10a9367f836&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f\"&gt;https://preview.redd.it/r6zd68q6ao0e1.png?width=1057&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d2a7641ee6de38b2e604f90af33f2abfbcbcaf0f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e\"&gt;https://preview.redd.it/g45lqi39ao0e1.png?width=1117&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e1011c23820764e8860d5500a0c5f7d2091c0c0e&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1\"&gt;https://preview.redd.it/c7vzlr0bao0e1.png?width=861&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2267d3322b8c62690a08a22c4eb8098ad9df29a1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b\"&gt;https://preview.redd.it/zy1jtkzbao0e1.png?width=1145&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ba9904f46f0dd2fc078199a975be535cc321e3b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/70fcub0dao0e1.png?width=881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992\"&gt;https://preview.redd.it/70fcub0dao0e1.png?width=881&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c090e4315a82fee70ce33c2f58c77ddfba4ae992&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ohpmkgjeao0e1.png?width=1139&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f00a3ce842302f28ba94f4ff01d51dabc7862cca\"&gt;Interestingly, training on self-generated data is slightly more beneficial than using data from a different teacher model. But the initial proficiency of the base model obviously matters too which confounds the results&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?auto=webp&amp;s=76fd71e6647bddf7d3795101abb444439a80b490",
                            "width": 2320,
                            "height": 1161
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5dd8cae40db80f66c9f478c78e59205ea4b1e7f8",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=707192d6aac235e35d9ff716f5fe1f4c8a429ecf",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=60a40d3a00e8453de0102b6f50fbf9581f80423e",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=31495d3ddbaf3bedd3059ef51e1b4f5e8253fa62",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=aabe7fde0bf59af3acbd898bb7cf5b75b6208da0",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/I4qM-_YYn0w0jLKgkz-6WNbB4mKzVSkQa3dR7Mi8r6s.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=caee9c9cfcc54cff19e0a7261e6cdf6bda07196c",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "TPf8VLArYB73nV7faJwPNdg7R46dqyrvwS0gzdX6Elw"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqd7w1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "StartledWatermelon",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqd7w1/r_selfcodealign_selfalignment_for_code_generation/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731505544.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everyone,\n\nI was reading the paper \"Neural Discrete Representation Learning\" and I was puzzled when I looked at the first term in VQ-VAE Loss Equation\n\nhttps://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d\n\nI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.\n\nNote: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussionHello everyone,I was reading the paper \"Neural Discrete Representation Learning\" and I was puzzled when I looked at the first term in VQ-VAE Loss EquationI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don't understand how the expectation in the ELBO Loss turned out to be the first term.Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn't clearly explained, I could explain it more in the discussion\n\n\\[Discussion\\]",
            "author_fullname": "t2_6czm0y0q",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Proof of Reconstruction Loss Term in VQ-VAE Loss",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "l1s9kur3sn0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 6,
                            "x": 108,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7bea1652e55e48617e3b67f43cc07aed9bb2b49"
                        },
                        {
                            "y": 13,
                            "x": 216,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6adfd4b702968806defc9b925afc9b0eebb17065"
                        },
                        {
                            "y": 20,
                            "x": 320,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2ae0f5f43ea7b8be5bbca8d2bb6f31f0a040b270"
                        },
                        {
                            "y": 41,
                            "x": 640,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b64c944f33c7b278fc8351a5d1ee00bfa725b05"
                        },
                        {
                            "y": 61,
                            "x": 960,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=24575d2137bd08032aa3d657a2bf7bde71aca56e"
                        },
                        {
                            "y": 69,
                            "x": 1080,
                            "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=141bdcea75bc3fe6df8fd7171cae586bedf7616a"
                        }
                    ],
                    "s": {
                        "y": 90,
                        "x": 1394,
                        "u": "https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;format=png&amp;auto=webp&amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d"
                    },
                    "id": "l1s9kur3sn0e1"
                }
            },
            "name": "t3_1gqbeie",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 25,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 25,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731499665.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I was reading the paper &amp;quot;Neural Discrete Representation Learning&amp;quot; and I was puzzled when I looked at the first term in VQ-VAE Loss Equation&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d\"&gt;https://preview.redd.it/l1s9kur3sn0e1.png?width=1394&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d374dce319a7ac0bbf19089d4e06cabcaa2cd3d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don&amp;#39;t understand how the expectation in the ELBO Loss turned out to be the first term.&lt;/p&gt;\n\n&lt;p&gt;Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn&amp;#39;t clearly explained, I could explain it more in the discussionHello everyone,I was reading the paper &amp;quot;Neural Discrete Representation Learning&amp;quot; and I was puzzled when I looked at the first term in VQ-VAE Loss EquationI understand the role of the second and the third term. However, I am not able to derive the first term from either the MSE between the original and reconstructed image. I assumed it will be similar to the ELBO Loss in the VAE. The paper mentions why they have omitted the KL Divergence Term, but even then I don&amp;#39;t understand how the expectation in the ELBO Loss turned out to be the first term.Note: I am not coming from a stats background, so If the question is something fundamental, it would be helpful if you could tell me what it is. Also, If the question isn&amp;#39;t clearly explained, I could explain it more in the discussion&lt;/p&gt;\n\n&lt;p&gt;[Discussion]&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqbeie",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Snoo_65491",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqbeie/discussion_proof_of_reconstruction_loss_term_in/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731499665.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.",
            "author_fullname": "t2_bmxqugb8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] OCR for documents ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqb861",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.2,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731499023.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to build a pipeline that allows users to upload various documents, and the model will parse them, generating a JSON output. The document types can be categorized into three types: identification documents (such as licenses or passports), transcripts (related to education), and degree certificates. For each type, there’s a predefined set of JSON output requirements. I’ve been exploring Open Source solutions for this task, and the new small language vision models appear to be a flexible approach. I’d like to know if there’s a simpler way to achieve this, or if these models will be an overkill.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqb861",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "FreakedoutNeurotic98",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqb861/d_ocr_for_documents/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731499023.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm planning on organising an event for  semester 1 students at a university. The goal is to get them interested in these fields, something that they can learn from while enjoying it.\nWould love some good inputs on it. How would you design if you were in my place. How would you make it more fun.",
            "author_fullname": "t2_42oc6qgj8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Ideas for AI/DS event for uni students that they can have fun with",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqb6xz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731498884.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m planning on organising an event for  semester 1 students at a university. The goal is to get them interested in these fields, something that they can learn from while enjoying it.\nWould love some good inputs on it. How would you design if you were in my place. How would you make it more fun.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqb6xz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Particular_Tap_4002",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqb6xz/d_ideas_for_aids_event_for_uni_students_that_they/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731498884.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I remember that I saw on twitter a while ago (3-4 months) a graphic user interface to set up LLM as agents as nodes in a graph and making them interact.  \n**Te user interface was black and had the details in yellow.**   \nWhen an llm was computing there was a yellow circle moving around the node of the agents that was computing the reply... and then the \"flow\" passed as a yellow on the edges to the LLM that was computing the next answer...\n\nI already asked *the bot* but I cannot recall the project. It was an open source project. Very fun and smart it seemed. This was unlocking the \"Socratic ai\" as a matter of graph disposition.  \nIt was open source, is not LangChain. \n\nSomeone remember? which one was? ",
            "author_fullname": "t2_7x3pkgoc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Help with Graphic User Interface for LLM as Agents",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gqanre",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.63,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731496816.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember that I saw on twitter a while ago (3-4 months) a graphic user interface to set up LLM as agents as nodes in a graph and making them interact.&lt;br/&gt;\n&lt;strong&gt;Te user interface was black and had the details in yellow.&lt;/strong&gt;&lt;br/&gt;\nWhen an llm was computing there was a yellow circle moving around the node of the agents that was computing the reply... and then the &amp;quot;flow&amp;quot; passed as a yellow on the edges to the LLM that was computing the next answer...&lt;/p&gt;\n\n&lt;p&gt;I already asked &lt;em&gt;the bot&lt;/em&gt; but I cannot recall the project. It was an open source project. Very fun and smart it seemed. This was unlocking the &amp;quot;Socratic ai&amp;quot; as a matter of graph disposition.&lt;br/&gt;\nIt was open source, is not LangChain. &lt;/p&gt;\n\n&lt;p&gt;Someone remember? which one was? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gqanre",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "vale_valerio",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gqanre/r_help_with_graphic_user_interface_for_llm_as/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731496816.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Reviews for ICLR 2025 seem to be available on OpenReview. Feel free to celebrate/rant/complain about your reviews here!\n\nLast year's statistics [here](https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/)",
            "author_fullname": "t2_csovt",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] ICLR 2025 Paper Reviews",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq8vu6",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.95,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 48,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 48,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731498646.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731489057.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Reviews for ICLR 2025 seem to be available on OpenReview. Feel free to celebrate/rant/complain about your reviews here!&lt;/p&gt;\n\n&lt;p&gt;Last year&amp;#39;s statistics &lt;a href=\"https://papercopilot.com/statistics/iclr-statistics/iclr-2024-statistics/\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq8vu6",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "pie3636",
            "discussion_type": null,
            "num_comments": 24,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq8vu6/d_iclr_2025_paper_reviews/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731489057.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Been diving into ML infrastructure costs lately and curious about how others are handling this. \n\n\n\nSome specific things I'm trying to figure out:\n\n\n\n\\- What's your current setup? (AWS/GCP/Azure/On-prem/etc)\n\n\\- Rough monthly costs for GPU infrastructure?\n\n\\- Biggest headaches in managing ML infrastructure?\n\n\\- How much time does your team spend on infrastructure vs actual ML work?\n\n\\- Are you using spot instances or dedicated resources?\n\n\\- Any creative ways you've found to optimize costs?\n\n",
            "author_fullname": "t2_lrwx3e7gr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] ML Engineers/DevOps - What's your current GPU infrastructure costing you (and is it worth it)?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq8eth",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731486750.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been diving into ML infrastructure costs lately and curious about how others are handling this. &lt;/p&gt;\n\n&lt;p&gt;Some specific things I&amp;#39;m trying to figure out:&lt;/p&gt;\n\n&lt;p&gt;- What&amp;#39;s your current setup? (AWS/GCP/Azure/On-prem/etc)&lt;/p&gt;\n\n&lt;p&gt;- Rough monthly costs for GPU infrastructure?&lt;/p&gt;\n\n&lt;p&gt;- Biggest headaches in managing ML infrastructure?&lt;/p&gt;\n\n&lt;p&gt;- How much time does your team spend on infrastructure vs actual ML work?&lt;/p&gt;\n\n&lt;p&gt;- Are you using spot instances or dedicated resources?&lt;/p&gt;\n\n&lt;p&gt;- Any creative ways you&amp;#39;ve found to optimize costs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq8eth",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "SwimmerPopular1589",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq8eth/discussion_ml_engineersdevops_whats_your_current/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731486750.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Ask me anything about AI adoption in the UK, tech stack, how to become an AI/ML Engineer or Data Scientist etc, career development you name it. ",
            "author_fullname": "t2_8lj5asuv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] AMA: I’m Head of AI at a firm in the UK, advising Gov., industry, etc. ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq899s",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 170,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 170,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731486020.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ask me anything about AI adoption in the UK, tech stack, how to become an AI/ML Engineer or Data Scientist etc, career development you name it. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq899s",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Psychological_Dare93",
            "discussion_type": null,
            "num_comments": 154,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq899s/d_ama_im_head_of_ai_at_a_firm_in_the_uk_advising/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731486020.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Today we open-weight (Apache 2.0) released the two best embedding models for ecommerce search and recommendations available anywhere. Marqo ecommerce models significantly outperform models from Amazon, Google, Cohere and Jina (see below).\n\n\\+ Up to 88% improvement on the best private model, Amazon-Titan-Multimodal (and better than Google Vertex, Cohere).\n\n\\+ Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP.\n\n\\+ 5ms single text/image inference (A10g).\n\n\\+ Up to 231% improvement over other bench-marked models (see blog below).\n\n\\+ Evaluated on over 4M products across 10,000's of categories. Eval datasets are open sourced [here](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb).\n\n\\+ Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image.\n\n\\+ Released 2 evaluation datasets: GoogleShopping-1m and AmazonProducts-3m.\n\n\\+ Released [evaluation code](https://github.com/marqo-ai/marqo-ecommerce-embeddings).\n\n\\+ Apache 2.0 [model weights available on Hugging Face](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb) and to test out on Hugging Face Spaces.\n\n  \nBlog: [https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models](https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models)\n\nGitHub: [https://github.com/marqo-ai/marqo-ecommerce-embeddings](https://github.com/marqo-ai/marqo-ecommerce-embeddings)\n\nHugging Face: [https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb](https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb)",
            "author_fullname": "t2_58z8qvld",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Two new open-weight (Apache 2.0) foundation models for multimodal product embeddings",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpx4jz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731449698.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today we open-weight (Apache 2.0) released the two best embedding models for ecommerce search and recommendations available anywhere. Marqo ecommerce models significantly outperform models from Amazon, Google, Cohere and Jina (see below).&lt;/p&gt;\n\n&lt;p&gt;+ Up to 88% improvement on the best private model, Amazon-Titan-Multimodal (and better than Google Vertex, Cohere).&lt;/p&gt;\n\n&lt;p&gt;+ Up to 31% improvement on the best open source model, ViT-SO400M-14-SigLIP.&lt;/p&gt;\n\n&lt;p&gt;+ 5ms single text/image inference (A10g).&lt;/p&gt;\n\n&lt;p&gt;+ Up to 231% improvement over other bench-marked models (see blog below).&lt;/p&gt;\n\n&lt;p&gt;+ Evaluated on over 4M products across 10,000&amp;#39;s of categories. Eval datasets are open sourced &lt;a href=\"https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;+ Detailed performance comparisons across three major tasks: Text2Image, Category2Image, and AmazonProducts-Text2Image.&lt;/p&gt;\n\n&lt;p&gt;+ Released 2 evaluation datasets: GoogleShopping-1m and AmazonProducts-3m.&lt;/p&gt;\n\n&lt;p&gt;+ Released &lt;a href=\"https://github.com/marqo-ai/marqo-ecommerce-embeddings\"&gt;evaluation code&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;+ Apache 2.0 &lt;a href=\"https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb\"&gt;model weights available on Hugging Face&lt;/a&gt; and to test out on Hugging Face Spaces.&lt;/p&gt;\n\n&lt;p&gt;Blog: &lt;a href=\"https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models\"&gt;https://www.marqo.ai/blog/introducing-marqos-ecommerce-embedding-models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/marqo-ai/marqo-ecommerce-embeddings\"&gt;https://github.com/marqo-ai/marqo-ecommerce-embeddings&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb\"&gt;https://huggingface.co/collections/Marqo/marqo-ecommerce-embeddings-66f611b9bb9d035a8d164fbb&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?auto=webp&amp;s=c0e4c10b9a16b52253b8f1e94e1b78a2396ee892",
                            "width": 1200,
                            "height": 648
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2ab473153d51fb756f11beda59849b331c4f9459",
                                "width": 108,
                                "height": 58
                            },
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ff0de89789318a7cabee5c573d0446829987610e",
                                "width": 216,
                                "height": 116
                            },
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=721ad13066bca95c0ec35dfd12b640aa95c25f96",
                                "width": 320,
                                "height": 172
                            },
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e30fe2f25d897d5f8eca69f11981da84ba93b3e",
                                "width": 640,
                                "height": 345
                            },
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=838c565aedfb041ed0dfac824d6018c3910a59b9",
                                "width": 960,
                                "height": 518
                            },
                            {
                                "url": "https://external-preview.redd.it/2niX8PekVcarppSg0a2zVqVk6opYWIAbunY-j7ryECQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8cf5a74a68f7a1f507ddbb6deaef4ce149fe61f0",
                                "width": 1080,
                                "height": 583
                            }
                        ],
                        "variants": {},
                        "id": "N-sAIj8WfrBvoEPv22iUCXLcJmHjra8nBQBGcoX7nCg"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpx4jz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Jesse_marqo",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpx4jz/p_two_new_openweight_apache_20_foundation_models/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731449698.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "\nWhen I look at the test data in some papers(especially in arXiv), some small models(~7B) shows quite moderate performance on some famous LLM bechmarking datasets.\nHowever, based on my experience, the model acts like a fool(e.g. neverending repeated generation) on the dataset they mentioned.\nWhen someone test bechmarking score of LLMs, do they usually fine-tune them toward the dataset before scoring?",
            "author_fullname": "t2_13bznjuvy9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Benchmark scores of LLM",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq3tt3",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731468909.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;When I look at the test data in some papers(especially in arXiv), some small models(~7B) shows quite moderate performance on some famous LLM bechmarking datasets.\nHowever, based on my experience, the model acts like a fool(e.g. neverending repeated generation) on the dataset they mentioned.\nWhen someone test bechmarking score of LLMs, do they usually fine-tune them toward the dataset before scoring?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq3tt3",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Upset_Employer5480",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq3tt3/d_benchmark_scores_of_llm/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731468909.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Heyy, for anyone reading this, I just got selected for a Nationwide Hackathon so I need some little suggestions/help regarding it. Everything about it will be explained as follows.\n\nSo basically my problem statement for hackathon is to colorize SAR images those are image captured by satellite of Earth. Those images are greyscale High quality images used nad then colorized. I observed onething of the colourized and greyscale images the greyscale image is of higher size in terms of storage than colorized image. \nI have to colorize these greyscale images such that it would be easy and accurate to analysis for the persons using it. What additional information I can provide besides the colorized images also note that these images are used for investigation. \nHelp me win this hackathon by your suggestions or any unique approch to this problem. \n\nNote: We have to train GAN based models by the dataset which have sets of grayscale and colorized images.",
            "author_fullname": "t2_j2rc35fp",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D]Image Colorization using GANs",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq3itj",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.62,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731467938.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heyy, for anyone reading this, I just got selected for a Nationwide Hackathon so I need some little suggestions/help regarding it. Everything about it will be explained as follows.&lt;/p&gt;\n\n&lt;p&gt;So basically my problem statement for hackathon is to colorize SAR images those are image captured by satellite of Earth. Those images are greyscale High quality images used nad then colorized. I observed onething of the colourized and greyscale images the greyscale image is of higher size in terms of storage than colorized image. \nI have to colorize these greyscale images such that it would be easy and accurate to analysis for the persons using it. What additional information I can provide besides the colorized images also note that these images are used for investigation. \nHelp me win this hackathon by your suggestions or any unique approch to this problem. &lt;/p&gt;\n\n&lt;p&gt;Note: We have to train GAN based models by the dataset which have sets of grayscale and colorized images.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq3itj",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DangerousCounty4724",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq3itj/dimage_colorization_using_gans/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731467938.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am currently generating tags for healthcare articles. My current approach is to use few-shot prompting with an LLM API to extract tags that the model considers appropriate, based on the examples I provide.\n\nI've been relying on LLMs because I believe they have the best understanding of language, even in niche domains like healthcare. However, I’m starting to wonder if my thought process is flawed and if there are more efficient solutions to this problem.\n\nIt has worked reasonably well so far, but there are a few concerns that I have:\n\nFirstly, I didn't choose to use any open-source or supervised learning models or algorithms as I am worried that they being general-purpose, may not be sufficiently trained on healthcare-specific terms or nuanced domain-specific knowledge.\n\nThis approach, while effective for now, is expensive for large-scale tagging due to the API usage costs.\n\nIn the long run, I would like to train or fine-tune my own model to perform this tagging task. However, I currently don’t have a large, labelled dataset of high-quality tags to do so.\n\nAs such, is the LLM approach for tagging in the short term until I gather sufficient data for fine-tuning or training my own model a good idea or are there better alternatives for tagging healthcare documents that are cost-efficient and domain-specific?",
            "author_fullname": "t2_4zxlou1a",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Suggestions for Document Tagging on Healthcare Articles Using LLMs or Alternative Approaches?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gq3d0u",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731467448.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently generating tags for healthcare articles. My current approach is to use few-shot prompting with an LLM API to extract tags that the model considers appropriate, based on the examples I provide.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been relying on LLMs because I believe they have the best understanding of language, even in niche domains like healthcare. However, I’m starting to wonder if my thought process is flawed and if there are more efficient solutions to this problem.&lt;/p&gt;\n\n&lt;p&gt;It has worked reasonably well so far, but there are a few concerns that I have:&lt;/p&gt;\n\n&lt;p&gt;Firstly, I didn&amp;#39;t choose to use any open-source or supervised learning models or algorithms as I am worried that they being general-purpose, may not be sufficiently trained on healthcare-specific terms or nuanced domain-specific knowledge.&lt;/p&gt;\n\n&lt;p&gt;This approach, while effective for now, is expensive for large-scale tagging due to the API usage costs.&lt;/p&gt;\n\n&lt;p&gt;In the long run, I would like to train or fine-tune my own model to perform this tagging task. However, I currently don’t have a large, labelled dataset of high-quality tags to do so.&lt;/p&gt;\n\n&lt;p&gt;As such, is the LLM approach for tagging in the short term until I gather sufficient data for fine-tuning or training my own model a good idea or are there better alternatives for tagging healthcare documents that are cost-efficient and domain-specific?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gq3d0u",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Comb-Greedy",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gq3d0u/d_suggestions_for_document_tagging_on_healthcare/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731467448.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I recently learned that this startup is seen as the fastest revenue ramp in recent years. But they are literally just brokering GPUs from one provider to another and just slapping on a broker fee…\n\nIf a real estate agent sales $100 million worth of houses, and get a $100,000 commission, it doesn’t mean they made $100 million in revenue…  what am I missing here? \n\nThe product is literally the same, just ssh to a cluster. \n\nWhy are people paying for this? this sounds like a massive scam no? Shouldn’t this just be compared to a cloud provider like Coreweave instead of an AI company? If you own GPUs as a cloud, you crushed $100M in ARR in a few months… ",
            "author_fullname": "t2_19850lbxog",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Together AI hits $100M in ARR but it just resales compute - hype? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gps8fl",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 44,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 44,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731437561.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently learned that this startup is seen as the fastest revenue ramp in recent years. But they are literally just brokering GPUs from one provider to another and just slapping on a broker fee…&lt;/p&gt;\n\n&lt;p&gt;If a real estate agent sales $100 million worth of houses, and get a $100,000 commission, it doesn’t mean they made $100 million in revenue…  what am I missing here? &lt;/p&gt;\n\n&lt;p&gt;The product is literally the same, just ssh to a cluster. &lt;/p&gt;\n\n&lt;p&gt;Why are people paying for this? this sounds like a massive scam no? Shouldn’t this just be compared to a cloud provider like Coreweave instead of an AI company? If you own GPUs as a cloud, you crushed $100M in ARR in a few months… &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gps8fl",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "guardianz42",
            "discussion_type": null,
            "num_comments": 20,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gps8fl/d_together_ai_hits_100m_in_arr_but_it_just/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731437561.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey everyone I'm currently doing research on using wireless technology for human and object detection, with machine learning and deep learning. I’m interested in learning more about how different types of waves like radar, microwaves, and Wi-Fi channel state information (CSI) are being used in this space.\n\nIf anyone has experience or knows about projects where these types of signals are applied for detection, I’d love to hear about it. Any insights on the benefits or limitations of certain waves/signals for ML and DL models in human and object detection would be super helpfull",
            "author_fullname": "t2_ftj7g390v",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] What Types of Wireless Technologies Are Used in Human and Object Detection",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gps00s",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731436982.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone I&amp;#39;m currently doing research on using wireless technology for human and object detection, with machine learning and deep learning. I’m interested in learning more about how different types of waves like radar, microwaves, and Wi-Fi channel state information (CSI) are being used in this space.&lt;/p&gt;\n\n&lt;p&gt;If anyone has experience or knows about projects where these types of signals are applied for detection, I’d love to hear about it. Any insights on the benefits or limitations of certain waves/signals for ML and DL models in human and object detection would be super helpfull&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gps00s",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "PutNo3040",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gps00s/r_what_types_of_wireless_technologies_are_used_in/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731436982.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey as I started my PhD (topic: Interpretable Object Detection) recently I would be really curious to know what set of features you think make a successfull PhD student",
            "author_fullname": "t2_cyy3x98h",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What makes a good PhD student in ML",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gplmzb",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 165,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "c5cf3b2a-6abd-11ea-a37b-0ebd427f43f1",
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 165,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731420833.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey as I started my PhD (topic: Interpretable Object Detection) recently I would be really curious to know what set of features you think make a successfull PhD student&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "Student",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gplmzb",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "RaeudigerRaffi",
            "discussion_type": null,
            "num_comments": 67,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": "dark",
            "permalink": "/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gplmzb/d_what_makes_a_good_phd_student_in_ml/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731420833.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "\"*What DL architecture to try on tabular data?*\"\n\nHi Reddit! Today, my colleagues announced TabM - a new answer to the above question. **TabM is leading on the benchmarks, while being simple, practical, and scalable to large datasets**. Technically, TabM efficiently imitates an ensemble of MLPs, as illustrated below. Also, TabM is one of the first projects using our new TabReD benchmark - a collection of eight real-world industrial datasets with time-based splits and feature engineering.\n\nFor a quick overview of TabM, you can check the following parts of the paper:  \n\\- **The abstract**  \n\\- The model illustration in **Figure 1** (and in the post below)  \n\\- The main results on **Page 7**\n\nTabM links:  \n\\- [arXiv](https://arxiv.org/abs/2410.24210)  \n\\- [GitHub](https://github.com/yandex-research/tabm)  \n\\- [Twitter thread](https://x.com/YuraFiveTwo/status/1856293601627566335)\n\nTabReD links:\n\n\\- [arXiv](https://arxiv.org/abs/2406.19380)  \n\\- [GitHub](https://github.com/yandex-research/tabred)  \n\\- [Twitter thread](https://x.com/puhsuuu/status/1854149134124486924)\n\n[The model illustration ](https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&amp;format=png&amp;auto=webp&amp;s=519ff43ebd6a57501adb9cbdf39183b20af06cfc)",
            "author_fullname": "t2_159u6g",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 63,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "qsvl8qk4sg0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 49,
                            "x": 108,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0902993e308e028df72353643e279d8637027fc9"
                        },
                        {
                            "y": 98,
                            "x": 216,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41e51f7407a31e9f760bc7a4a50d7af635430ecd"
                        },
                        {
                            "y": 145,
                            "x": 320,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac634e1ea3d2f3a3adb9fc13985de7f20ec15069"
                        },
                        {
                            "y": 291,
                            "x": 640,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4da4d8ebea35d32f663a00481810ae9d84bfc2d"
                        },
                        {
                            "y": 437,
                            "x": 960,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ded1668cd6d31881912a7c3526e01d45d0934987"
                        },
                        {
                            "y": 491,
                            "x": 1080,
                            "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b03363ebeb10034a8a98a28e97b1a15333e03b6a"
                        }
                    ],
                    "s": {
                        "y": 784,
                        "x": 1722,
                        "u": "https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&amp;format=png&amp;auto=webp&amp;s=519ff43ebd6a57501adb9cbdf39183b20af06cfc"
                    },
                    "id": "qsvl8qk4sg0e1"
                }
            },
            "name": "t3_1gpjl9e",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 53,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 53,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/S4ChMa0TaQXjU3wO0P1UaYOyCejy58bTWBKifgDIH68.jpg",
            "edited": 1731415568.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731414540.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;&lt;em&gt;What DL architecture to try on tabular data?&lt;/em&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Hi Reddit! Today, my colleagues announced TabM - a new answer to the above question. &lt;strong&gt;TabM is leading on the benchmarks, while being simple, practical, and scalable to large datasets&lt;/strong&gt;. Technically, TabM efficiently imitates an ensemble of MLPs, as illustrated below. Also, TabM is one of the first projects using our new TabReD benchmark - a collection of eight real-world industrial datasets with time-based splits and feature engineering.&lt;/p&gt;\n\n&lt;p&gt;For a quick overview of TabM, you can check the following parts of the paper:&lt;br/&gt;\n- &lt;strong&gt;The abstract&lt;/strong&gt;&lt;br/&gt;\n- The model illustration in &lt;strong&gt;Figure 1&lt;/strong&gt; (and in the post below)&lt;br/&gt;\n- The main results on &lt;strong&gt;Page 7&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;TabM links:&lt;br/&gt;\n- &lt;a href=\"https://arxiv.org/abs/2410.24210\"&gt;arXiv&lt;/a&gt;&lt;br/&gt;\n- &lt;a href=\"https://github.com/yandex-research/tabm\"&gt;GitHub&lt;/a&gt;&lt;br/&gt;\n- &lt;a href=\"https://x.com/YuraFiveTwo/status/1856293601627566335\"&gt;Twitter thread&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TabReD links:&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://arxiv.org/abs/2406.19380\"&gt;arXiv&lt;/a&gt;&lt;br/&gt;\n- &lt;a href=\"https://github.com/yandex-research/tabred\"&gt;GitHub&lt;/a&gt;&lt;br/&gt;\n- &lt;a href=\"https://x.com/puhsuuu/status/1854149134124486924\"&gt;Twitter thread&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qsvl8qk4sg0e1.png?width=1722&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=519ff43ebd6a57501adb9cbdf39183b20af06cfc\"&gt;The model illustration &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpjl9e",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "_puhsu",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpjl9e/r_tabm_advancing_tabular_deep_learning_with/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731414540.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi!\n\nI'm looking into the possibility of using GenAI for generating beatmaps (levels) for rhythm games. Specifically I'm thinking Beat Saber but eventually I'd like the solution to be generalizable to arbitrary rhythm games.\n\nI'm wondering if it'd be possible to (re)ues existing language models by cleverly transforming a song data into a text prompt and then the result into a beatmap :thinking:\n\nWould anyone be interested in exploring such an endeavour or at least provide some idaes and insights as to how I could go about it?\n\nPS I'm a software engineer so I could handle coding and teaching custom models.\n\nThanks!",
            "author_fullname": "t2_3cysr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] A model for rhythm game beatmaps",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpeuw4",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731394168.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking into the possibility of using GenAI for generating beatmaps (levels) for rhythm games. Specifically I&amp;#39;m thinking Beat Saber but eventually I&amp;#39;d like the solution to be generalizable to arbitrary rhythm games.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wondering if it&amp;#39;d be possible to (re)ues existing language models by cleverly transforming a song data into a text prompt and then the result into a beatmap :thinking:&lt;/p&gt;\n\n&lt;p&gt;Would anyone be interested in exploring such an endeavour or at least provide some idaes and insights as to how I could go about it?&lt;/p&gt;\n\n&lt;p&gt;PS I&amp;#39;m a software engineer so I could handle coding and teaching custom models.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpeuw4",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Imm0rt4l",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpeuw4/d_a_model_for_rhythm_game_beatmaps/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731394168.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm working on a machine learning project, and I'm curious about the general practice when it comes to creating user interfaces for ML models. Specifically, do you create dedicated Uls for interacting with your models? If so, how do you approach designing the UI for an ML system? Are there any best practices or tools you use to make the interaction smooth for non-technical users?\n\nFor example, if you're deploying a model as a web app, do you build a Ul that allows users to input data and see predictions, or do you mostly focus on the backend API and leave UI design to front- end developers? How do you manage the complexity of explaining the model's behavior or showing predictions in a user-friendly way?",
            "author_fullname": "t2_1cciiewhu2",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Do you create UI for your ML models ? How do you apportion it ? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpe8ir",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731391578.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a machine learning project, and I&amp;#39;m curious about the general practice when it comes to creating user interfaces for ML models. Specifically, do you create dedicated Uls for interacting with your models? If so, how do you approach designing the UI for an ML system? Are there any best practices or tools you use to make the interaction smooth for non-technical users?&lt;/p&gt;\n\n&lt;p&gt;For example, if you&amp;#39;re deploying a model as a web app, do you build a Ul that allows users to input data and see predictions, or do you mostly focus on the backend API and leave UI design to front- end developers? How do you manage the complexity of explaining the model&amp;#39;s behavior or showing predictions in a user-friendly way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpe8ir",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Afreen19",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpe8ir/discussion_do_you_create_ui_for_your_ml_models/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731391578.0,
            "num_crossposts": 1,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I have a large dataset of around 4.5m records that I am using CatBoost to predict greyhound racing likely performance. Running 20k iterations or more takes a very long time, and it is difficult to say whether the results are significantly better than running training on a smaller dataset like 200k-500k.\n\nUsing gridsearchCV on the smaller sets of data found me the best parameters, but once again it is difficult to see if that would still be effective on the much larger set.\n\nI would also like your thoughts on whether larger learning rate and lower iterations is better overall or whether you find it worth the wait to train at a lower learning rate over many more iterations.",
            "author_fullname": "t2_dobtg5o4",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Catboost large dataset. Is is best to use the majority of the data for training, where time to train is extreme, or smaller datasets where iterations are much faster?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpc7it",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731384378.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a large dataset of around 4.5m records that I am using CatBoost to predict greyhound racing likely performance. Running 20k iterations or more takes a very long time, and it is difficult to say whether the results are significantly better than running training on a smaller dataset like 200k-500k.&lt;/p&gt;\n\n&lt;p&gt;Using gridsearchCV on the smaller sets of data found me the best parameters, but once again it is difficult to see if that would still be effective on the much larger set.&lt;/p&gt;\n\n&lt;p&gt;I would also like your thoughts on whether larger learning rate and lower iterations is better overall or whether you find it worth the wait to train at a lower learning rate over many more iterations.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpc7it",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Responsible-Walk-459",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpc7it/d_catboost_large_dataset_is_is_best_to_use_the/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731384378.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone!\n\nWe’re hosting the first “AI for Music” workshop at AAAI on March 3, 2025. The workshop will explore how AI is transforming music creation, recognition, education, and more. Topics include AI-driven composition, sound design, legal and ethical challenges, and AI’s impact on musicians’ careers.\n\nSubmissions (up to 6 pages) are welcome until November 22, 2024. Work in progress is encouraged!\n\n**Workshop Summary**\n\nThis one-day workshop will explore the dynamic intersection of artificial intelligence and music. It explores how AI is transforming music creation, recognition, and education, ethical and legal implications, as well as business opportunities. We will investigate how AI is changing the music industry and education—from composition to performance, production, collaboration, and audience experience. Participants will gain insights into the technological challenges in music and how AI can enhance creativity, enabling musicians and producers to push the boundaries of their art. The workshop will cover topics such as AI-driven music composition, where algorithms generate melodies, harmonies, and even full orchestral arrangements. We will discuss how AI tools assist in sound design, remixing, and mastering, allowing for new sonic possibilities and efficiencies in music production. Additionally, we'll examine AI's impact on music education and the careers of musicians, exploring advanced learning tools and teaching methods. AI technologies are increasingly adopted in the music and entertainment industry. The workshop will also discuss the legal and ethical implications of AI in music, including questions of authorship, originality, and the evolving role of human artists in an increasingly automated world. This workshop is designed for AI researchers, musicians, producers, and educators interested in the current status and future of AI in music.\n\n**Call for Papers**\n\nSubmissions should be a maximum of 6 pages. Work in progress is welcome. Authors are encouraged to include descriptions of their prototype implementations. Additionally, authors are encouraged to interact with workshop attendees by including posters or demonstrations at the end of the workshop. Conceptual designs without any evidence of practical implementation are discouraged.\n\n**Topics of interest are (but not limited to)**\n\n* AI-Driven Music Composition and Generation\n* AI in Music Practice and Performance\n* AI-based Music Recognition and Transcription\n* AI Applications in Sound Design\n* AI-Generated Videos and Lyrics Based on Music\n* Legal and Ethical Implications of AI in Music\n* AI’s Impact on Musicians’ Careers and Education\n* Business Opportunities of AI in Music\n* Music Datasets and Data Analysis\n\n**Important Dates**\n\n* Submission Deadline: November 22, 2024\n* Notification: December 9, 2024\n* Final Version Due: December 31, 2024\n\nWe hope to see you there! 🎶",
            "author_fullname": "t2_7bznaudd",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[News] AAAI 2025 Workshop on AI for Music 🎶",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "two",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpc9fr",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 11,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 11,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731384562.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;We’re hosting the first “AI for Music” workshop at AAAI on March 3, 2025. The workshop will explore how AI is transforming music creation, recognition, education, and more. Topics include AI-driven composition, sound design, legal and ethical challenges, and AI’s impact on musicians’ careers.&lt;/p&gt;\n\n&lt;p&gt;Submissions (up to 6 pages) are welcome until November 22, 2024. Work in progress is encouraged!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Workshop Summary&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;This one-day workshop will explore the dynamic intersection of artificial intelligence and music. It explores how AI is transforming music creation, recognition, and education, ethical and legal implications, as well as business opportunities. We will investigate how AI is changing the music industry and education—from composition to performance, production, collaboration, and audience experience. Participants will gain insights into the technological challenges in music and how AI can enhance creativity, enabling musicians and producers to push the boundaries of their art. The workshop will cover topics such as AI-driven music composition, where algorithms generate melodies, harmonies, and even full orchestral arrangements. We will discuss how AI tools assist in sound design, remixing, and mastering, allowing for new sonic possibilities and efficiencies in music production. Additionally, we&amp;#39;ll examine AI&amp;#39;s impact on music education and the careers of musicians, exploring advanced learning tools and teaching methods. AI technologies are increasingly adopted in the music and entertainment industry. The workshop will also discuss the legal and ethical implications of AI in music, including questions of authorship, originality, and the evolving role of human artists in an increasingly automated world. This workshop is designed for AI researchers, musicians, producers, and educators interested in the current status and future of AI in music.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Call for Papers&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Submissions should be a maximum of 6 pages. Work in progress is welcome. Authors are encouraged to include descriptions of their prototype implementations. Additionally, authors are encouraged to interact with workshop attendees by including posters or demonstrations at the end of the workshop. Conceptual designs without any evidence of practical implementation are discouraged.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Topics of interest are (but not limited to)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AI-Driven Music Composition and Generation&lt;/li&gt;\n&lt;li&gt;AI in Music Practice and Performance&lt;/li&gt;\n&lt;li&gt;AI-based Music Recognition and Transcription&lt;/li&gt;\n&lt;li&gt;AI Applications in Sound Design&lt;/li&gt;\n&lt;li&gt;AI-Generated Videos and Lyrics Based on Music&lt;/li&gt;\n&lt;li&gt;Legal and Ethical Implications of AI in Music&lt;/li&gt;\n&lt;li&gt;AI’s Impact on Musicians’ Careers and Education&lt;/li&gt;\n&lt;li&gt;Business Opportunities of AI in Music&lt;/li&gt;\n&lt;li&gt;Music Datasets and Data Analysis&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Important Dates&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Submission Deadline: November 22, 2024&lt;/li&gt;\n&lt;li&gt;Notification: December 9, 2024&lt;/li&gt;\n&lt;li&gt;Final Version Due: December 31, 2024&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We hope to see you there! 🎶&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpc9fr",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Saysike_rightnow69",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpc9fr/news_aaai_2025_workshop_on_ai_for_music/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731384562.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I saw people using some tools, but sometimes those doesn't really fit and i'm confused which ones to try.\n\nDo you guys just save the config+results? But how about when the model code changes?\n\nI am unsure how to go about this. Any tips?\n\nI think i might need some paper/digital notes plus some way to backtrack.\n\n  \nEDIT: Lots of good comments ! Thank you! I'll keep this post up and just keep commenting. Others will surely find this helpful.",
            "author_fullname": "t2_1axtwmeqmk",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] How do you keep track of experiments, history, results?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpc3cv",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 37,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 37,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731459070.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731384013.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I saw people using some tools, but sometimes those doesn&amp;#39;t really fit and i&amp;#39;m confused which ones to try.&lt;/p&gt;\n\n&lt;p&gt;Do you guys just save the config+results? But how about when the model code changes?&lt;/p&gt;\n\n&lt;p&gt;I am unsure how to go about this. Any tips?&lt;/p&gt;\n\n&lt;p&gt;I think i might need some paper/digital notes plus some way to backtrack.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Lots of good comments ! Thank you! I&amp;#39;ll keep this post up and just keep commenting. Others will surely find this helpful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpc3cv",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Pristine-Staff-5250",
            "discussion_type": null,
            "num_comments": 36,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpc3cv/d_how_do_you_keep_track_of_experiments_history/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731384013.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Paperwithcode is the best option to add a LLM acceleration category, but there isn't. \n\n\n\nIs there similar place or forum which contains latest paper, code, implement in LLM acceleration fields?",
            "author_fullname": "t2_18riberpl8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Is there a website or forum in LLM acceleration fields?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpc23c",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731383900.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paperwithcode is the best option to add a LLM acceleration category, but there isn&amp;#39;t. &lt;/p&gt;\n\n&lt;p&gt;Is there similar place or forum which contains latest paper, code, implement in LLM acceleration fields?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpc23c",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Logical_Divide_3595",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpc23c/d_is_there_a_website_or_forum_in_llm_acceleration/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731383900.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Can someone explain me how to interpret Figure 3 of LoRA? Why is the bottom left greyed out on the first 2 pictures and why is the zoomed picture's (last 2 pictures) grey part no on the bottom left and inverted instead?\n\nThank you for your help\n\nhttps://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=1afd83925af782da6122d70632f7592f9a11c502\n\n",
            "author_fullname": "t2_51yo280p",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Subspace similarity plot of LoRA",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": 61,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "ocji41m1xd0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 47,
                            "x": 108,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5247727495ed9c632b673a5d64bc6fa7b286216f"
                        },
                        {
                            "y": 95,
                            "x": 216,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31170bef6f52787e333bdd40654bf1e466a5835a"
                        },
                        {
                            "y": 141,
                            "x": 320,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f250e5ae6dbd72ea3ffb6c63258d4df122ba154"
                        },
                        {
                            "y": 282,
                            "x": 640,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3cfe3d29327a8bea69def7f0a2da3fa8a81eef44"
                        },
                        {
                            "y": 423,
                            "x": 960,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=764f9514addb924b9a9cbb1394093b5642eebaf5"
                        },
                        {
                            "y": 476,
                            "x": 1080,
                            "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7dd4ffd4621188082fe7aa6f2ae90111ba9f38d"
                        }
                    ],
                    "s": {
                        "y": 488,
                        "x": 1106,
                        "u": "https://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;format=png&amp;auto=webp&amp;s=1afd83925af782da6122d70632f7592f9a11c502"
                    },
                    "id": "ocji41m1xd0e1"
                }
            },
            "name": "t3_1gpat21",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/bSukn5Lb1YfNYgGziymrET5lQgLN91BlQHhp3S9fAtU.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731379861.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain me how to interpret Figure 3 of LoRA? Why is the bottom left greyed out on the first 2 pictures and why is the zoomed picture&amp;#39;s (last 2 pictures) grey part no on the bottom left and inverted instead?&lt;/p&gt;\n\n&lt;p&gt;Thank you for your help&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1afd83925af782da6122d70632f7592f9a11c502\"&gt;https://preview.redd.it/ocji41m1xd0e1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1afd83925af782da6122d70632f7592f9a11c502&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpat21",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BigYounzzz",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpat21/d_subspace_similarity_plot_of_lora/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731379861.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make the most of my time attending.",
            "author_fullname": "t2_148per",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] NeurIPS After Dark Networking Event",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gpamvn",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 17,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 17,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731379333.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just got an email about an official ticketed after dark NeurIPS networking event - this will be my first time attending/presenting, wondering if these events are worth going to. More generally, also interested in hearing about how to make the most of my time attending.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gpamvn",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "gateofptolemy",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gpamvn/d_neurips_after_dark_networking_event/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731379333.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey guys, I’m a graduate master’s student majoring in Machine Learning. Winter break is coming up, and I’m gonna be spending Christmas alone 😃. I’ve got some spare time and access to a few A100s, so I’m planning to work on a project.\n\nI’m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!\n\nPlease share any problem statements you’re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)\n",
            "author_fullname": "t2_yv0ljku23",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What are some problems you guys are working on?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp9ydh",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 41,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 41,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731377278.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, I’m a graduate master’s student majoring in Machine Learning. Winter break is coming up, and I’m gonna be spending Christmas alone 😃. I’ve got some spare time and access to a few A100s, so I’m planning to work on a project.&lt;/p&gt;\n\n&lt;p&gt;I’m curious to know what kind of problems you guys are working on! Need someone to help out or wish someone could solve a problem you have? I maybeeee can spare my winter to work on it!&lt;/p&gt;\n\n&lt;p&gt;Please share any problem statements you’re working on or wish to tackle. Also, if you work in the industry and know what kinds of problems would help me stand out, that advice would be super appreciated too :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp9ydh",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "ziggyboom30",
            "discussion_type": null,
            "num_comments": 42,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp9ydh/d_what_are_some_problems_you_guys_are_working_on/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731377278.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey Redditors,\n\nI’m curious to hear your thoughts on this! Do you consider linear regression a part of AI, or do you see it as more of a traditional statistical method? I feel like there's a lot of debate around which techniques are truly considered AI, especially since some methods have been around for decades and are widely used outside of AI-specific applications.\n\nAlso, are there any other methods you initially didn't think of as AI, only to realize they were, or vice versa? Would love to know how others draw the line between traditional data analysis and AI techniques.\n\nThanks!",
            "author_fullname": "t2_2qulfxfn",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Is Linear Regression Considered AI?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp95jv",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.32,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731374891.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey Redditors,&lt;/p&gt;\n\n&lt;p&gt;I’m curious to hear your thoughts on this! Do you consider linear regression a part of AI, or do you see it as more of a traditional statistical method? I feel like there&amp;#39;s a lot of debate around which techniques are truly considered AI, especially since some methods have been around for decades and are widely used outside of AI-specific applications.&lt;/p&gt;\n\n&lt;p&gt;Also, are there any other methods you initially didn&amp;#39;t think of as AI, only to realize they were, or vice versa? Would love to know how others draw the line between traditional data analysis and AI techniques.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp95jv",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "gcombar",
            "discussion_type": null,
            "num_comments": 32,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp95jv/d_is_linear_regression_considered_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp95jv/d_is_linear_regression_considered_ai/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731374891.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I have a friend with cancer and a recent surgery took their voice from them. I want to try training an AI voice model on some of the videos I have of them from before the surgery. Ideally I was hoping for an android app or web-app that I could use their voice model on so they can use TTS to speak using their voice again. I was looking for a way they could use it on their phone through an app if possible",
            "author_fullname": "t2_qfzhl",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Whats the best way to train a voice model locally? (preferablly to make a TTS model to be used on an app)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp88fs",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731372266.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a friend with cancer and a recent surgery took their voice from them. I want to try training an AI voice model on some of the videos I have of them from before the surgery. Ideally I was hoping for an android app or web-app that I could use their voice model on so they can use TTS to speak using their voice again. I was looking for a way they could use it on their phone through an app if possible&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp88fs",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "TheTabernacleMan",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp88fs/d_whats_the_best_way_to_train_a_voice_model/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731372266.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I've been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that's puzzled me is the disparity in the availability and adoption of quantization versus pruning.\n\n**Quantization** seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It's relatively straightforward to implement and has seen significant adoption in both research and industry.\n\nOn the other hand, **pruning**—which involves removing less important weights from the model—is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn't seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.\n\nI'm curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?",
            "author_fullname": "t2_9lvsgisb",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Why is LLM Pruning Not as Generally Available as Quantization?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp6h2d",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 57,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 57,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731367411.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been diving into the world of large language models (LLMs) and have been exploring various optimization techniques. One thing that&amp;#39;s puzzled me is the disparity in the availability and adoption of quantization versus pruning.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Quantization&lt;/strong&gt; seems to be a well-established and widely used technique for reducing the memory footprint and computational cost of LLMs. It&amp;#39;s relatively straightforward to implement and has seen significant adoption in both research and industry.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, &lt;strong&gt;pruning&lt;/strong&gt;—which involves removing less important weights from the model—is less common. Despite its potential benefits, such as further reducing model size and inference time, it doesn&amp;#39;t seem to be as generally available or as widely adopted. Many of my searches on the internet just result in research papers or proof of concept GitHub repos.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious about the reasons behind this disparity. Are there technical challenges with pruning that make it less practical? Is it more difficult to implement or integrate into existing workflows? Or are there other factors at play?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp6h2d",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Soumil30",
            "discussion_type": null,
            "num_comments": 21,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp6h2d/d_why_is_llm_pruning_not_as_generally_available/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731367411.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm curious about how tiktoks filters perform so well at erasing hair (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser) and eyebrows (https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser).\n\nIve tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.\n\nAnyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks",
            "author_fullname": "t2_xlc8w",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What is the likely architecture/dataset for tiktok's realtime GAN models used in filters?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp3vz0",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 21,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 21,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731360861.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m curious about how tiktoks filters perform so well at erasing hair (&lt;a href=\"https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser\"&gt;https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/hair-eraser&lt;/a&gt;) and eyebrows (&lt;a href=\"https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser\"&gt;https://effecthouse.tiktok.com/learn/guides/technical-guides/objects/generative-effects/eyebrow-eraser&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;Ive tried to do something similar (removing items from peoples faces in realtime) using a lightweight Pix2Pix style model on a paired dataset I created using OpenCV methods, but the quality of the generated image decreased too much as I reduced the size of the generator.&lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas on how they achieve such consistent results on such a lightweight model? Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/_JGsQcCYAHcLRSpwG0dVzLtRo34RUGNMhx1IU9DbdZ4.jpg?auto=webp&amp;s=36920b6ba2ee923e29e611380ce41dbf1864587a",
                            "width": 720,
                            "height": 1280
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/_JGsQcCYAHcLRSpwG0dVzLtRo34RUGNMhx1IU9DbdZ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e80c7e2d41614bf1b4f8bc85e2a383cd9336a790",
                                "width": 108,
                                "height": 192
                            },
                            {
                                "url": "https://external-preview.redd.it/_JGsQcCYAHcLRSpwG0dVzLtRo34RUGNMhx1IU9DbdZ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=44b2732f6ca14586e8501fa9f809aad71a84b0fb",
                                "width": 216,
                                "height": 384
                            },
                            {
                                "url": "https://external-preview.redd.it/_JGsQcCYAHcLRSpwG0dVzLtRo34RUGNMhx1IU9DbdZ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08311317ecd73de72bc5dfb8ae49121c55186d83",
                                "width": 320,
                                "height": 568
                            },
                            {
                                "url": "https://external-preview.redd.it/_JGsQcCYAHcLRSpwG0dVzLtRo34RUGNMhx1IU9DbdZ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6de61e8f0b03c0a40f8ccecb23c5c1440b1f180d",
                                "width": 640,
                                "height": 1137
                            }
                        ],
                        "variants": {},
                        "id": "7npRcg1lZg3lmHwoYC8kL1jbfIE2V60IQvcT0YSYc48"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp3vz0",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "DjPoliceman",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp3vz0/d_what_is_the_likely_architecturedataset_for/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731360861.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am looking at a plant disease classification problem where depending on the symptoms one has to classify a plant as belonging into one of several disease categories. My question is about prompt engineering strategies for classifying when the target has very high cardinality. When there are only four or five potential target labels I can list them in the prompt and ask LLM to classify. What happens when the number of categories is &gt;50 ? Is there a way to prompt the LLM effectively in such a scenario?\n\n",
            "author_fullname": "t2_pox8o2jpm",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Prompting for classification when target has very high cardinality [D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp2mqh",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.31,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731357791.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking at a plant disease classification problem where depending on the symptoms one has to classify a plant as belonging into one of several disease categories. My question is about prompt engineering strategies for classifying when the target has very high cardinality. When there are only four or five potential target labels I can list them in the prompt and ask LLM to classify. What happens when the number of categories is &amp;gt;50 ? Is there a way to prompt the LLM effectively in such a scenario?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp2mqh",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Ok-Emu5850",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp2mqh/prompting_for_classification_when_target_has_very/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp2mqh/prompting_for_classification_when_target_has_very/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731357791.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am currently working on a problem statement in which I need to classify between real and ai generated images and then give explanation for the classification. The first part is quite easy and the for the second part I found some research papers but none of them give the links for annotated dataset for fine-tuning model. can anyone help me find datasets which have good annotations for this purpose.\n\n[SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model](https://arxiv.org/pdf/2402.18068v2) (they mention a dataset on page 4 but didn't give any links)",
            "author_fullname": "t2_jue4tmdt",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P]Annotated dataset for explaining the reason in AI vs Real Image detection",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gp05os",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731351832.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently working on a problem statement in which I need to classify between real and ai generated images and then give explanation for the classification. The first part is quite easy and the for the second part I found some research papers but none of them give the links for annotated dataset for fine-tuning model. can anyone help me find datasets which have good annotations for this purpose.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2402.18068v2\"&gt;SynArtifact: Classifying and Alleviating Artifacts in Synthetic Images via Vision-Language Model&lt;/a&gt; (they mention a dataset on page 4 but didn&amp;#39;t give any links)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gp05os",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Background-Trainer37",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gp05os/pannotated_dataset_for_explaining_the_reason_in/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731351832.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi there, \nI’m currently building an NN model to detect a disease based off answers to multiple questions. In preliminary tests on 600 patients the model does extremely well, AUCs of 0.995 test accuracies of 0.975 but I fear the model is overfitting, I’ve used cross validation and performance gap analysis aswell as L1/L2 regularisation, Dropout and early stopping.\nHere’s the results from the cross validation and performance gap analysis .\nCross validation results : mean Auc=0.9787 SD0.0090 \nMean accuracy =0.9350 SD0.0262\nPerformance gap analysis\nTraining set Auc = 0.9983 accuracy =0.9859\nTest set Auc=0.9936 accuracy 0.9803\n\nTell me what you guys think of those results and if you think it’s overfitting/what other tests can I do to tell? \nI’m trying to ascertain more data but might need to partner with someone to do so. I don’t want to partner get the data and find out it’s a complete waste! \nThanks",
            "author_fullname": "t2_921kwke8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Overfitting Query [P]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gow3uh",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731342176.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi there, \nI’m currently building an NN model to detect a disease based off answers to multiple questions. In preliminary tests on 600 patients the model does extremely well, AUCs of 0.995 test accuracies of 0.975 but I fear the model is overfitting, I’ve used cross validation and performance gap analysis aswell as L1/L2 regularisation, Dropout and early stopping.\nHere’s the results from the cross validation and performance gap analysis .\nCross validation results : mean Auc=0.9787 SD0.0090 \nMean accuracy =0.9350 SD0.0262\nPerformance gap analysis\nTraining set Auc = 0.9983 accuracy =0.9859\nTest set Auc=0.9936 accuracy 0.9803&lt;/p&gt;\n\n&lt;p&gt;Tell me what you guys think of those results and if you think it’s overfitting/what other tests can I do to tell? \nI’m trying to ascertain more data but might need to partner with someone to do so. I don’t want to partner get the data and find out it’s a complete waste! \nThanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gow3uh",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Disastrous_Ad9821",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gow3uh/overfitting_query_p/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gow3uh/overfitting_query_p/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731342176.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "ICLR 2025 reviews go live on OpenReview tomorrow! Thought I'd open a thread for any feedback, issues, or celebrations around the reviews.\n\nAs ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Let’s remember that scores don’t define the true impact of research. Share your experiences, thoughts, and let’s support each other through the process!",
            "author_fullname": "t2_mijyxq6gx",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] ICLR 2025 Paper Reviews Discussion",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gov5zd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.95,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 97,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 97,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731339814.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;ICLR 2025 reviews go live on OpenReview tomorrow! Thought I&amp;#39;d open a thread for any feedback, issues, or celebrations around the reviews.&lt;/p&gt;\n\n&lt;p&gt;As ICLR grows, review noise is inevitable, and good work may not always get the score it deserves. Let’s remember that scores don’t define the true impact of research. Share your experiences, thoughts, and let’s support each other through the process!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gov5zd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Technical_Proof6082",
            "discussion_type": null,
            "num_comments": 143,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gov5zd/d_iclr_2025_paper_reviews_discussion/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731339814.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "For context, I'm trying to fine tune the MobileNetV3Small model for facial recognition. I freezed all the layers and added few layers on top for training.\n\nAt the moment, my dataset has four classes, with 126 images each.\n\nWhile training the model, somehow every 2nth epoch are getting skipped, and they're not recorded in history either. If the epoch is set to 20, then only 10 epoch are actually executing. I've attached the ss of jupyter notebook output.\n\nLater I tried the exact same code in collab and it raised an error on 2nd epoch saying validation generator is returning None object. I rechecked the code many times but still can't find where the issue lies.\n\nIf anyone knows any fix, please do suggest.\n\n  \nCode of my generator:\n\n    datagen = ImageDataGenerator(\n                rescale=1./255,\n                width_shift_range=0.1,\n                height_shift_range=0.1,\n                horizontal_flip=True,\n                rotation_range=10,\n                fill_mode = 'nearest')\n    \n    \n    datagen_val = ImageDataGenerator(rescale=1./255)\n    \n    batch_size = 16\n    \n    train_generator = datagen.flow(X_train,\n                                   y_train,\n                                   batch_size=batch_size\n                                   )\n    \n    validation_generator = datagen_val.flow(X_val,\n                                            y_val,\n                                           batch_size = batch_size)\n    \n\n  \n\n\nhttps://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=d1141db2189b0bc371a5dd28c279c2b2639db33d\n\nhttps://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09\n\n  \n",
            "author_fullname": "t2_xkvbl5buo",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Project] While training my model, every 2n Epoch are being skipped in jupyter notebook",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": 68,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "yza75p8t0a0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 39,
                            "x": 108,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e69fbb5f799bc49b897c0f49e2ce2ad2fe4bc40"
                        },
                        {
                            "y": 79,
                            "x": 216,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fa9f327eeac26248c1aa8e44aa3e1e4a95ca0522"
                        },
                        {
                            "y": 118,
                            "x": 320,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82651bb8e05cf3093f61514aebb4210a89759b9f"
                        },
                        {
                            "y": 236,
                            "x": 640,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fea4cc6e00e32cb0f2d027b35054c2ca96f8a7af"
                        },
                        {
                            "y": 354,
                            "x": 960,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0b74795eb9233293cdc26402ae07ca8b25a27607"
                        },
                        {
                            "y": 399,
                            "x": 1080,
                            "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecbfe9eba8cef5bd6f658b22569c8120c35d68ba"
                        }
                    ],
                    "s": {
                        "y": 399,
                        "x": 1080,
                        "u": "https://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09"
                    },
                    "id": "yza75p8t0a0e1"
                },
                "b9zrpqls0a0e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 53,
                            "x": 108,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3938effb56468925842ebc94a42aeb8f5bf720de"
                        },
                        {
                            "y": 106,
                            "x": 216,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f11f55880f4bd6c5ab0e96d9faef656352c08000"
                        },
                        {
                            "y": 157,
                            "x": 320,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e7de975859243b71037f8a8aa007eb67ab63d4e"
                        },
                        {
                            "y": 315,
                            "x": 640,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6312dec1ae407daf320e20dd9cbab2cf6d2f25d"
                        },
                        {
                            "y": 472,
                            "x": 960,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e429be74794c3719acb54b598938c1f9ee21ba64"
                        },
                        {
                            "y": 532,
                            "x": 1080,
                            "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9a69173027954b611437a7b2170080b2153dbbde"
                        }
                    ],
                    "s": {
                        "y": 532,
                        "x": 1080,
                        "u": "https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=d1141db2189b0bc371a5dd28c279c2b2639db33d"
                    },
                    "id": "b9zrpqls0a0e1"
                }
            },
            "name": "t3_1got90h",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.27,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/Nn8v3_hIXLrWobmecARuwt1zqxosuSfKVYD3E-OhpfE.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731334826.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For context, I&amp;#39;m trying to fine tune the MobileNetV3Small model for facial recognition. I freezed all the layers and added few layers on top for training.&lt;/p&gt;\n\n&lt;p&gt;At the moment, my dataset has four classes, with 126 images each.&lt;/p&gt;\n\n&lt;p&gt;While training the model, somehow every 2nth epoch are getting skipped, and they&amp;#39;re not recorded in history either. If the epoch is set to 20, then only 10 epoch are actually executing. I&amp;#39;ve attached the ss of jupyter notebook output.&lt;/p&gt;\n\n&lt;p&gt;Later I tried the exact same code in collab and it raised an error on 2nd epoch saying validation generator is returning None object. I rechecked the code many times but still can&amp;#39;t find where the issue lies.&lt;/p&gt;\n\n&lt;p&gt;If anyone knows any fix, please do suggest.&lt;/p&gt;\n\n&lt;p&gt;Code of my generator:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;datagen = ImageDataGenerator(\n            rescale=1./255,\n            width_shift_range=0.1,\n            height_shift_range=0.1,\n            horizontal_flip=True,\n            rotation_range=10,\n            fill_mode = &amp;#39;nearest&amp;#39;)\n\n\ndatagen_val = ImageDataGenerator(rescale=1./255)\n\nbatch_size = 16\n\ntrain_generator = datagen.flow(X_train,\n                               y_train,\n                               batch_size=batch_size\n                               )\n\nvalidation_generator = datagen_val.flow(X_val,\n                                        y_val,\n                                       batch_size = batch_size)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1141db2189b0bc371a5dd28c279c2b2639db33d\"&gt;https://preview.redd.it/b9zrpqls0a0e1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1141db2189b0bc371a5dd28c279c2b2639db33d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09\"&gt;https://preview.redd.it/yza75p8t0a0e1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbab0aa05e67511dab4c8436e5a12b14a2d06e09&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1got90h",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "bkkh_3",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1got90h/project_while_training_my_model_every_2n_epoch/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1got90h/project_while_training_my_model_every_2n_epoch/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731334826.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I need to deploy a `.pkl` model for batch predictions in a setup where code is pushed to GitLab, SQL/pyspark is used for data, and cron jobs handle scheduling. Docker, Kubernetes, and cloud are not allowed. this is on-premise setup. What are some best practices or approaches for this kind of deployment?",
            "author_fullname": "t2_vyx41bsww",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Help me with on-premise ml batch prediction deployment ?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1goqchp",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731325563.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to deploy a &lt;code&gt;.pkl&lt;/code&gt; model for batch predictions in a setup where code is pushed to GitLab, SQL/pyspark is used for data, and cron jobs handle scheduling. Docker, Kubernetes, and cloud are not allowed. this is on-premise setup. What are some best practices or approaches for this kind of deployment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1goqchp",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Simple_Toe_6989",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1goqchp/d_help_me_with_onpremise_ml_batch_prediction/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1goqchp/d_help_me_with_onpremise_ml_batch_prediction/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731325563.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "My specialization is in cybersecurity but I am passionate about learning (in general), and deeply interested in many things, including AI/ML research. I’ve been exploring the concept of creating models that explore the latent space in a novel, far from average way. This idea is rooted in principles of curiosity-driven reinforcement learning, applied to generative models. By having stimulation driven attention mechanisms, intrinsic stimulation rewards, and memory augmented architectures, I've tried to come up with something that might work. Here’s a quick overview:\n\n**Stimulation Driven Attention Mechanism**: Integrating an entropy-based reward layer into traditional attention mechanisms to encourage models to explore lesser-known tokens and regions within the latent space.\n\nhttps://preview.redd.it/5isz9dj3h60e1.png?width=768&amp;format=png&amp;auto=webp&amp;s=e2d9ba2ed635c176b9dc66364a026ce024189d47\n\n**Intrinsic Stimulation Rewards**: Modifying the loss function to prioritize surprising or low-probability outputs, balancing accuracy with novelty.\n\nhttps://preview.redd.it/ox9vkq57h60e1.png?width=798&amp;format=png&amp;auto=webp&amp;s=e88bce2eeaadca2e3c06a4c700adf261a9740adb\n\nThose are the main ideas. Alongside that you could have:\n\n**Memory Augmented Generative Networks**: Implementing episodic memory buffers and novelty comparison modules to reward deviations from prior patterns.\n\n**Self Regulating Exploration Mechanisms**: Adding feedback loops to maintain coherence by adjusting stimulation rewards when output quality degrades.\n\nPlease help me figure out if this makes sense. I'm not too attached to the ideas themselves.",
            "author_fullname": "t2_khou9j",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] A guess for an interesting method by a random security researcher",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "5isz9dj3h60e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 22,
                            "x": 108,
                            "u": "https://preview.redd.it/5isz9dj3h60e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b69131f6fd942af21e45c75cb80ec369554f05db"
                        },
                        {
                            "y": 45,
                            "x": 216,
                            "u": "https://preview.redd.it/5isz9dj3h60e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=07045a42023821f96d01818a3362c510bf9bab40"
                        },
                        {
                            "y": 67,
                            "x": 320,
                            "u": "https://preview.redd.it/5isz9dj3h60e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2056140eed65c2f97f7d96728c56e3c5c34b4fbf"
                        },
                        {
                            "y": 135,
                            "x": 640,
                            "u": "https://preview.redd.it/5isz9dj3h60e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=396844b4cfad16e28b4c015e4c6224b294c52452"
                        }
                    ],
                    "s": {
                        "y": 162,
                        "x": 768,
                        "u": "https://preview.redd.it/5isz9dj3h60e1.png?width=768&amp;format=png&amp;auto=webp&amp;s=e2d9ba2ed635c176b9dc66364a026ce024189d47"
                    },
                    "id": "5isz9dj3h60e1"
                },
                "ox9vkq57h60e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 23,
                            "x": 108,
                            "u": "https://preview.redd.it/ox9vkq57h60e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9f0da9b06ca09a0ea1c8a2bb30e2406702121fb4"
                        },
                        {
                            "y": 46,
                            "x": 216,
                            "u": "https://preview.redd.it/ox9vkq57h60e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6eb905cfbcc25003b64de0d2ab8b7c5bfe5cb3b"
                        },
                        {
                            "y": 68,
                            "x": 320,
                            "u": "https://preview.redd.it/ox9vkq57h60e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eb6b40c3c750138285b892716ccfd04d12fe27ea"
                        },
                        {
                            "y": 137,
                            "x": 640,
                            "u": "https://preview.redd.it/ox9vkq57h60e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b91b5cf311f373232cccdc4532f517a89cda0853"
                        }
                    ],
                    "s": {
                        "y": 172,
                        "x": 798,
                        "u": "https://preview.redd.it/ox9vkq57h60e1.png?width=798&amp;format=png&amp;auto=webp&amp;s=e88bce2eeaadca2e3c06a4c700adf261a9740adb"
                    },
                    "id": "ox9vkq57h60e1"
                }
            },
            "name": "t3_1goi8ut",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731293734.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My specialization is in cybersecurity but I am passionate about learning (in general), and deeply interested in many things, including AI/ML research. I’ve been exploring the concept of creating models that explore the latent space in a novel, far from average way. This idea is rooted in principles of curiosity-driven reinforcement learning, applied to generative models. By having stimulation driven attention mechanisms, intrinsic stimulation rewards, and memory augmented architectures, I&amp;#39;ve tried to come up with something that might work. Here’s a quick overview:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Stimulation Driven Attention Mechanism&lt;/strong&gt;: Integrating an entropy-based reward layer into traditional attention mechanisms to encourage models to explore lesser-known tokens and regions within the latent space.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/5isz9dj3h60e1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d9ba2ed635c176b9dc66364a026ce024189d47\"&gt;https://preview.redd.it/5isz9dj3h60e1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2d9ba2ed635c176b9dc66364a026ce024189d47&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Intrinsic Stimulation Rewards&lt;/strong&gt;: Modifying the loss function to prioritize surprising or low-probability outputs, balancing accuracy with novelty.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ox9vkq57h60e1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e88bce2eeaadca2e3c06a4c700adf261a9740adb\"&gt;https://preview.redd.it/ox9vkq57h60e1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e88bce2eeaadca2e3c06a4c700adf261a9740adb&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Those are the main ideas. Alongside that you could have:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Augmented Generative Networks&lt;/strong&gt;: Implementing episodic memory buffers and novelty comparison modules to reward deviations from prior patterns.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Self Regulating Exploration Mechanisms&lt;/strong&gt;: Adding feedback loops to maintain coherence by adjusting stimulation rewards when output quality degrades.&lt;/p&gt;\n\n&lt;p&gt;Please help me figure out if this makes sense. I&amp;#39;m not too attached to the ideas themselves.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1goi8ut",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Entropy667",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1goi8ut/d_a_guess_for_an_interesting_method_by_a_random/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1goi8ut/d_a_guess_for_an_interesting_method_by_a_random/",
            "subreddit_subscribers": 2934445,
            "created_utc": 1731293734.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everyone,\n\nI’m exploring how domain randomization contributes to the stability of NN controllers, especially when training includes a more extensive look at historical data.\n\nSpecifically, I’m curious if there’s a theoretical basis or formal analysis explaining how domain randomization, particularly when incorporating more historical information, can help neural networks maintain stability across varying conditions or noise levels. Are there papers that analyze this effect through Lyapunov stability or other rigorous methods, showing that exposure to a diverse range of past data can lead to more stable NN-based control systems?\n\nAny recommendations on foundational or recent research in this area would be greatly appreciated. Thanks in advance!\n\n(I already wrote the same thing on control theory reddit)",
            "author_fullname": "t2_7pvyfo6w",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Why Does Domain Randomization Ensure Stability in Neural Network Controllers? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gopir9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731322277.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m exploring how domain randomization contributes to the stability of NN controllers, especially when training includes a more extensive look at historical data.&lt;/p&gt;\n\n&lt;p&gt;Specifically, I’m curious if there’s a theoretical basis or formal analysis explaining how domain randomization, particularly when incorporating more historical information, can help neural networks maintain stability across varying conditions or noise levels. Are there papers that analyze this effect through Lyapunov stability or other rigorous methods, showing that exposure to a diverse range of past data can lead to more stable NN-based control systems?&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on foundational or recent research in this area would be greatly appreciated. Thanks in advance!&lt;/p&gt;\n\n&lt;p&gt;(I already wrote the same thing on control theory reddit)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gopir9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "nerdkim",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gopir9/d_why_does_domain_randomization_ensure_stability/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gopir9/d_why_does_domain_randomization_ensure_stability/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731322277.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "https://github.com/user1342/Awesome-LLM-Red-Teaming",
            "author_fullname": "t2_i8bjbbgr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Resource On Varying LLM Red Teaming Methods and Techniques ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gonf47",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731312892.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/user1342/Awesome-LLM-Red-Teaming\"&gt;https://github.com/user1342/Awesome-LLM-Red-Teaming&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?auto=webp&amp;s=980d54ed713248f6334fd6227dd889966cdc5c09",
                            "width": 1920,
                            "height": 960
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=01c9935eb0c5a825b92bfc74cefc15bff404d1d3",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=63340651baab4a9a751539ffe706612f342a15f2",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef824bac826e35b187679c685c46683c9497acb0",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b7caf4174089eb709effc5a5c86515540ad1e0c4",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c0bcd7380c2aff8fbf767c35ff9d033c60b6396",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/6cGufWbksqJMwTfc4zkQJpnA5IE5mYxaMWbMqR951dQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7228f3366fda714db4253dcd74f3e0f8b6aa5f1e",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "7901ymugB0-9ibWGe1IM70cRp3xCbIsqunRBlVodRh0"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gonf47",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "OppositeMonday",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gonf47/r_resource_on_varying_llm_red_teaming_methods_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gonf47/r_resource_on_varying_llm_red_teaming_methods_and/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731312892.0,
            "num_crossposts": 2,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "There seems to be an AI model for almost everything except one capable of taking a description of the layout of a city, building, room, or any kind of space and creating a visual representation of it. Is there something particularly challenging about enabling a text-driven AI to grasp and generate these spatial relationships?\n\nIt feels like this would be the final piece in the \"text-driven AI game generator\" puzzle. We have models for nearly every other component needed to create a game.",
            "author_fullname": "t2_30rplyl2",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Why aren't there text driven layout AI models",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gom4iz",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731307325.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;There seems to be an AI model for almost everything except one capable of taking a description of the layout of a city, building, room, or any kind of space and creating a visual representation of it. Is there something particularly challenging about enabling a text-driven AI to grasp and generate these spatial relationships?&lt;/p&gt;\n\n&lt;p&gt;It feels like this would be the final piece in the &amp;quot;text-driven AI game generator&amp;quot; puzzle. We have models for nearly every other component needed to create a game.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gom4iz",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "jbrinkw",
            "discussion_type": null,
            "num_comments": 32,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gom4iz/r_why_arent_there_text_driven_layout_ai_models/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gom4iz/r_why_arent_there_text_driven_layout_ai_models/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731307325.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello,\n\nIs there anyone who is gonna attending the WACV conference in Tucson next February? It looks like we have to book a room in JW Marriot and they are gonna charge each of us 35$ + taxes per day for using the resort.\n\nAny idea how to deal with this, such as any nearby hotels or alternative solutions?\n\nThanks!",
            "author_fullname": "t2_63n93xcfi",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Attending WACV2025",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gokplb",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731302049.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;Is there anyone who is gonna attending the WACV conference in Tucson next February? It looks like we have to book a room in JW Marriot and they are gonna charge each of us 35$ + taxes per day for using the resort.&lt;/p&gt;\n\n&lt;p&gt;Any idea how to deal with this, such as any nearby hotels or alternative solutions?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gokplb",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "tuvovan",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gokplb/d_attending_wacv2025/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gokplb/d_attending_wacv2025/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731302049.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Is it possible to visualize how an LLM “imagines” a token before and after processing it through the attention layer by feeding the token embeddings into an image model? I understand you can't copy paste it over, but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model?\n\nFor example if i were to enter \"poor man,\" into an LLM the embedding for \"man\" would shift toward \"beggar\" while entering \"royal man\" it could move closer to \"king.\" I want to visualize that change. Then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example.\n\nIt could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step.",
            "author_fullname": "t2_30rplyl2",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] How to visualize the effect of an LLM attention layer on a set of tokens with an image model",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gojg09",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 22,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 22,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731303390.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731297685.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to visualize how an LLM “imagines” a token before and after processing it through the attention layer by feeding the token embeddings into an image model? I understand you can&amp;#39;t copy paste it over, but is there a way to capture the latent transformation caused by the attention layer and apply this transformation to the embedding space of an image model?&lt;/p&gt;\n\n&lt;p&gt;For example if i were to enter &amp;quot;poor man,&amp;quot; into an LLM the embedding for &amp;quot;man&amp;quot; would shift toward &amp;quot;beggar&amp;quot; while entering &amp;quot;royal man&amp;quot; it could move closer to &amp;quot;king.&amp;quot; I want to visualize that change. Then you could transfer the embedding for man to an image model and it would create the something like a beggar or a king in this example.&lt;/p&gt;\n\n&lt;p&gt;It could make a really cool visualization if you captured the transformation after each attention layer and made a video by interpolating each step.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gojg09",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "jbrinkw",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gojg09/d_how_to_visualize_the_effect_of_an_llm_attention/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gojg09/d_how_to_visualize_the_effect_of_an_llm_attention/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731297685.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_e8kks",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Combining Induction and Transduction for Abstract Reasoning",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1goh5ym",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731290334.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "cs.cornell.edu",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1goh5ym",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "moschles",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1goh5ym/r_combining_induction_and_transduction_for/",
            "stickied": false,
            "url": "https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731290334.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I was wondering if neural networks have been used in  this kind of self regression problem. So instead of using a linear regression type framework, use a nonlinear neural network. \n\nReference to the specific problem https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references",
            "author_fullname": "t2_ce6rkgpc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Neural network based 'self - regression' or inverse covariance matrix",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gocm5g",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731277169.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if neural networks have been used in  this kind of self regression problem. So instead of using a linear regression type framework, use a nonlinear neural network. &lt;/p&gt;\n\n&lt;p&gt;Reference to the specific problem &lt;a href=\"https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references\"&gt;https://stats.stackexchange.com/questions/221348/linear-self-regression-terminology-and-references&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/Na1KPWKNVl6scgS9nIHq6TItzmZ7AAjzi2AE8ocRNSc.jpg?auto=webp&amp;s=df05bb2f4b5fc26062986818cecd1d09ddac156a",
                            "width": 316,
                            "height": 316
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/Na1KPWKNVl6scgS9nIHq6TItzmZ7AAjzi2AE8ocRNSc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1572e205fbdf3329597b19f6465d66590bb4af06",
                                "width": 108,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/Na1KPWKNVl6scgS9nIHq6TItzmZ7AAjzi2AE8ocRNSc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f2c012f839ee5078c1c827c2f50ae4a3bd0f7f76",
                                "width": 216,
                                "height": 216
                            }
                        ],
                        "variants": {},
                        "id": "63C1GbYQbI4tHZMkw99e-qlyoYaGnG58yfnmnFUJ34s"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gocm5g",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Sandy_dude",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gocm5g/r_neural_network_based_self_regression_or_inverse/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731277169.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\n\nI'm trying to find an interesting paper to present in my research group's meeting as part of a competition. I'm interested in the advancements of language models and generative AI in computer vision, specifically using diffusion models.\n\nI want to ask what your favorite papers related to those areas are currently and why you like them. I like papers that have a rather simple but nice innovative way of thinking that adds a lot of value to the research. Please come through with your thoughts/links and I appreciate all of your inputs. Thanks!!",
            "author_fullname": "t2_tv6z9sbvw",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] / [D] Your most recent favorite LLM or Diffusion Model based paper",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1go8qz0",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731267151.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find an interesting paper to present in my research group&amp;#39;s meeting as part of a competition. I&amp;#39;m interested in the advancements of language models and generative AI in computer vision, specifically using diffusion models.&lt;/p&gt;\n\n&lt;p&gt;I want to ask what your favorite papers related to those areas are currently and why you like them. I like papers that have a rather simple but nice innovative way of thinking that adds a lot of value to the research. Please come through with your thoughts/links and I appreciate all of your inputs. Thanks!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1go8qz0",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Tough-Statement9740",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1go8qz0/r_d_your_most_recent_favorite_llm_or_diffusion/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731267151.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi redditors! First time posting here.\n\nAfter some research, I would like to ask which you think would be the best SSD for moving big quantities of data (working with deep learning), if someone have experienced this and already found a good solution.\n\nI'm working in my PhD at the university and processes for buying things are painfully slow, and as I need right now a lot of space I'm considering to get one big capacity SSD for storing datasets and already trained models for the future.\n\nI bought the KingstonXS2000 as was supposed to achieve ~2Gbps, but when tested, it was lucky if it actieved some mins the 500Mbs mark, and dropped fast.\n\nI'm aware of the USB 3.2x2 ports and heating issues of the devices, but, even with that, after looking a bit into the net an reviews, a lot of people show near the 1Gbps in the best of the cases with the SSD I checked, even with USB 3.2 and thunderbolt ports.\n\nSo, any suggestions or good experience devices to share wourd be appreciated.\n\nTL;DR: Working with DL, need a reliable external 2TB SSD with real high speed over time read and write operations. ",
            "author_fullname": "t2_8f3tase8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] External SSD for store big datasets",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1go77ve",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731263239.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi redditors! First time posting here.&lt;/p&gt;\n\n&lt;p&gt;After some research, I would like to ask which you think would be the best SSD for moving big quantities of data (working with deep learning), if someone have experienced this and already found a good solution.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working in my PhD at the university and processes for buying things are painfully slow, and as I need right now a lot of space I&amp;#39;m considering to get one big capacity SSD for storing datasets and already trained models for the future.&lt;/p&gt;\n\n&lt;p&gt;I bought the KingstonXS2000 as was supposed to achieve ~2Gbps, but when tested, it was lucky if it actieved some mins the 500Mbs mark, and dropped fast.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aware of the USB 3.2x2 ports and heating issues of the devices, but, even with that, after looking a bit into the net an reviews, a lot of people show near the 1Gbps in the best of the cases with the SSD I checked, even with USB 3.2 and thunderbolt ports.&lt;/p&gt;\n\n&lt;p&gt;So, any suggestions or good experience devices to share wourd be appreciated.&lt;/p&gt;\n\n&lt;p&gt;TL;DR: Working with DL, need a reliable external 2TB SSD with real high speed over time read and write operations. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1go77ve",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "GankoX22",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1go77ve/d_external_ssd_for_store_big_datasets/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731263239.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello,  \nI'm trying to improve the extraction of timestamps, which all have the same format (similar to the attached example).\n\nFor whatever reason, about 20% of the timestamps aren't extracted. How can I improve the accuracy based on the code below?\n\nThanks\n\nhttps://preview.redd.it/qqpqo25m540e1.png?width=368&amp;format=png&amp;auto=webp&amp;s=8d691fd78858e57015f4cf2f171374f25e329a0b\n\n    def extract_time_and_location_from_image(image):\n        try:\n            # Enhance image preprocessing for better OCR results\n            image = ImageOps.grayscale(image)\n            image = ImageOps.invert(image)\n            image = ImageOps.autocontrast(image)\n            image = image.filter(ImageFilter.SHARPEN)\n            \n            text = pytesseract.image_to_string(image)\n            logger.debug(f\"Raw OCR text:\\n{text}\")\n    \n            cleaned_text = ' '.join(text.split())\n            logger.debug(f\"Cleaned OCR text: {cleaned_text}\")\n            \n            # Separate regex patterns for date and time\n            date_pattern = r'(\\d{4}:\\d{2}:\\d{2})'  # Matches YYYY:MM:DD\n            time_pattern = r'(\\b\\d{2}:\\d{2}\\b)'  # Matches HH:MM with word boundaries\n            \n            # Attempt to extract date and time separately\n            date_match = re.search(date_pattern, cleaned_text)\n            timestamp = None\n    \n            if date_match:\n                # Search for time pattern only after the date match\n                remaining_text = cleaned_text[date_match.end():]\n                time_match = re.search(time_pattern, remaining_text)\n                \n                if time_match:\n                    # Validate the extracted time\n                    time_parts = time_match.group(0).split(':')\n                    hours, minutes = int(time_parts[0]), int(time_parts[1])\n                    if 0 &lt;= hours &lt; 24 and 0 &lt;= minutes &lt; 60:\n                        full_timestamp = f\"{date_match.group(0)} {time_match.group(0)}\"\n                        logger.info(f\"Extracted timestamp: {full_timestamp}\")\n                        timestamp = full_timestamp\n                    else:\n                        logger.warning(\"Extracted time is not valid.\")\n                        timestamp = None\n                else:\n                    # If no time is found, default to \"00:00\" for HH:MM\n                    full_timestamp = f\"{date_match.group(0)} 00:00\"\n                    logger.info(f\"Extracted partial timestamp (defaulting to 00:00): {full_timestamp}\")\n                    timestamp = full_timestamp\n            else:\n                logger.warning(\"No valid timestamp found.\")\n                stats[\"no_timestamp_images\"] += 1\n                return None, \"UnknownCity\", \"UnknownCountry\"\n    \n            # Validate the extracted timestamp format\n            if not is_valid_timestamp(timestamp):\n                logger.warning(\"Extracted timestamp is not in the valid format.\")\n                return None, \"UnknownCity\", \"UnknownCountry\"\n    \n            # Enhanced location pattern to capture more variations\n            location_patterns = [\n                r'([A-Za-zÀ-ÖØ-öø-ÿ\\s]+),\\s*([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)',  # City, Country\n                r'([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)'    # City Country\n            ]\n            city, country = \"UnknownCity\", \"UnknownCountry\"\n            for location_pattern in location_patterns:\n                location_match = re.search(location_pattern, cleaned_text)\n                if location_match:\n                    city = location_match.group(1).strip()\n                    country = location_match.group(2).strip()\n                    logger.info(f\"Extracted location: {city}, {country}\")\n                    break\n    \n            return timestamp, city, country\n        except Exception as e:\n            logger.error(f\"Error extracting timestamp and location: {e}\")\n            return None, \"UnknownCity\", \"UnknownCountry\"",
            "author_fullname": "t2_wljv2p",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] How can I improve accuracy of timestamp extraction?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "qqpqo25m540e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 21,
                            "x": 108,
                            "u": "https://preview.redd.it/qqpqo25m540e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9a5fae46a24fc07e5437f5f71ee75a04b5140bac"
                        },
                        {
                            "y": 43,
                            "x": 216,
                            "u": "https://preview.redd.it/qqpqo25m540e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=10c9b8fa18718287c954fed0cbfc7d2090714ad2"
                        },
                        {
                            "y": 64,
                            "x": 320,
                            "u": "https://preview.redd.it/qqpqo25m540e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f635657ce657d17fd31d8775f7037ad7da939dde"
                        }
                    ],
                    "s": {
                        "y": 74,
                        "x": 368,
                        "u": "https://preview.redd.it/qqpqo25m540e1.png?width=368&amp;format=png&amp;auto=webp&amp;s=8d691fd78858e57015f4cf2f171374f25e329a0b"
                    },
                    "id": "qqpqo25m540e1"
                }
            },
            "name": "t3_1go6rtj",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.22,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731262079.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;br/&gt;\nI&amp;#39;m trying to improve the extraction of timestamps, which all have the same format (similar to the attached example).&lt;/p&gt;\n\n&lt;p&gt;For whatever reason, about 20% of the timestamps aren&amp;#39;t extracted. How can I improve the accuracy based on the code below?&lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qqpqo25m540e1.png?width=368&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d691fd78858e57015f4cf2f171374f25e329a0b\"&gt;https://preview.redd.it/qqpqo25m540e1.png?width=368&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d691fd78858e57015f4cf2f171374f25e329a0b&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;def extract_time_and_location_from_image(image):\n    try:\n        # Enhance image preprocessing for better OCR results\n        image = ImageOps.grayscale(image)\n        image = ImageOps.invert(image)\n        image = ImageOps.autocontrast(image)\n        image = image.filter(ImageFilter.SHARPEN)\n\n        text = pytesseract.image_to_string(image)\n        logger.debug(f&amp;quot;Raw OCR text:\\n{text}&amp;quot;)\n\n        cleaned_text = &amp;#39; &amp;#39;.join(text.split())\n        logger.debug(f&amp;quot;Cleaned OCR text: {cleaned_text}&amp;quot;)\n\n        # Separate regex patterns for date and time\n        date_pattern = r&amp;#39;(\\d{4}:\\d{2}:\\d{2})&amp;#39;  # Matches YYYY:MM:DD\n        time_pattern = r&amp;#39;(\\b\\d{2}:\\d{2}\\b)&amp;#39;  # Matches HH:MM with word boundaries\n\n        # Attempt to extract date and time separately\n        date_match = re.search(date_pattern, cleaned_text)\n        timestamp = None\n\n        if date_match:\n            # Search for time pattern only after the date match\n            remaining_text = cleaned_text[date_match.end():]\n            time_match = re.search(time_pattern, remaining_text)\n\n            if time_match:\n                # Validate the extracted time\n                time_parts = time_match.group(0).split(&amp;#39;:&amp;#39;)\n                hours, minutes = int(time_parts[0]), int(time_parts[1])\n                if 0 &amp;lt;= hours &amp;lt; 24 and 0 &amp;lt;= minutes &amp;lt; 60:\n                    full_timestamp = f&amp;quot;{date_match.group(0)} {time_match.group(0)}&amp;quot;\n                    logger.info(f&amp;quot;Extracted timestamp: {full_timestamp}&amp;quot;)\n                    timestamp = full_timestamp\n                else:\n                    logger.warning(&amp;quot;Extracted time is not valid.&amp;quot;)\n                    timestamp = None\n            else:\n                # If no time is found, default to &amp;quot;00:00&amp;quot; for HH:MM\n                full_timestamp = f&amp;quot;{date_match.group(0)} 00:00&amp;quot;\n                logger.info(f&amp;quot;Extracted partial timestamp (defaulting to 00:00): {full_timestamp}&amp;quot;)\n                timestamp = full_timestamp\n        else:\n            logger.warning(&amp;quot;No valid timestamp found.&amp;quot;)\n            stats[&amp;quot;no_timestamp_images&amp;quot;] += 1\n            return None, &amp;quot;UnknownCity&amp;quot;, &amp;quot;UnknownCountry&amp;quot;\n\n        # Validate the extracted timestamp format\n        if not is_valid_timestamp(timestamp):\n            logger.warning(&amp;quot;Extracted timestamp is not in the valid format.&amp;quot;)\n            return None, &amp;quot;UnknownCity&amp;quot;, &amp;quot;UnknownCountry&amp;quot;\n\n        # Enhanced location pattern to capture more variations\n        location_patterns = [\n            r&amp;#39;([A-Za-zÀ-ÖØ-öø-ÿ\\s]+),\\s*([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)&amp;#39;,  # City, Country\n            r&amp;#39;([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\s]+)&amp;#39;    # City Country\n        ]\n        city, country = &amp;quot;UnknownCity&amp;quot;, &amp;quot;UnknownCountry&amp;quot;\n        for location_pattern in location_patterns:\n            location_match = re.search(location_pattern, cleaned_text)\n            if location_match:\n                city = location_match.group(1).strip()\n                country = location_match.group(2).strip()\n                logger.info(f&amp;quot;Extracted location: {city}, {country}&amp;quot;)\n                break\n\n        return timestamp, city, country\n    except Exception as e:\n        logger.error(f&amp;quot;Error extracting timestamp and location: {e}&amp;quot;)\n        return None, &amp;quot;UnknownCity&amp;quot;, &amp;quot;UnknownCountry&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1go6rtj",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "maad0000",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1go6rtj/p_how_can_i_improve_accuracy_of_timestamp/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1go6rtj/p_how_can_i_improve_accuracy_of_timestamp/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731262079.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "\n\nwhy are a lots of the new papers ( usually done by PhDs ) with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters !!!!!\n\nthis is not a contribution ! i confused how can these got accepted",
            "author_fullname": "t2_9rc2qla22",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Discussion] Papers with fake NOVEL APPROACH in ML and DL models",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1go50wf",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 121,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 121,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731257597.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;why are a lots of the new papers ( usually done by PhDs ) with an existing approach and when u ask about their contribution they said we replace this layer by an other or we add a hyperparametters !!!!!&lt;/p&gt;\n\n&lt;p&gt;this is not a contribution ! i confused how can these got accepted&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1go50wf",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Rihab_Mira",
            "discussion_type": null,
            "num_comments": 67,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1go50wf/discussion_papers_with_fake_novel_approach_in_ml/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731257597.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey ML researchers!\n\nI've been diving deep into multimodal learning and I'm specifically interested in papers that utilize movie/video datasets in creative ways. I'm NOT looking for video generation or diffusion-related papers, but rather interesting approaches to:\n\n* Multimodal representation learning from movies\n* Novel fusion techniques combining video, audio, and text modalities\n* Scene understanding/contextual learning from film data\n* Character interaction analysis\n* Emotion/sentiment analysis across modalities\n* Cross-modal retrieval using movie data\n\nWould love to hear about any cool papers you've come across in this space!",
            "author_fullname": "t2_cm3jk2h9",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[Research] Looking for interesting research on movies datasets (NO generative models)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1go1wps",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731249106.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey ML researchers!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been diving deep into multimodal learning and I&amp;#39;m specifically interested in papers that utilize movie/video datasets in creative ways. I&amp;#39;m NOT looking for video generation or diffusion-related papers, but rather interesting approaches to:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multimodal representation learning from movies&lt;/li&gt;\n&lt;li&gt;Novel fusion techniques combining video, audio, and text modalities&lt;/li&gt;\n&lt;li&gt;Scene understanding/contextual learning from film data&lt;/li&gt;\n&lt;li&gt;Character interaction analysis&lt;/li&gt;\n&lt;li&gt;Emotion/sentiment analysis across modalities&lt;/li&gt;\n&lt;li&gt;Cross-modal retrieval using movie data&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear about any cool papers you&amp;#39;ve come across in this space!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1go1wps",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "stoneddumbledore",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1go1wps/research_looking_for_interesting_research_on/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731249106.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "We’re excited to share our recent paper \"[\\[NeurIPS 2024\\] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification](https://arxiv.org/pdf/2406.08993).\"\n\nIn this study, we conduct a thorough review of classic GNNs for node classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph learning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.\n\nCode: [https://github.com/LUOyk1999/tunedGNN](https://t.co/QeNSn2D9CN)  \nArxiv: [https://arxiv.org/abs/2406.08993](https://t.co/MD4mVTnHk8)\n\nIf you find our work interesting, we’d greatly appreciate a ⭐️ on GitHub!",
            "author_fullname": "t2_kfo6yco4",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Classic GNNs (GCNs, GraphSAGEs, GATs) are Strong Baselines on Node Classification",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnsn54",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 43,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 43,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731303911.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731213139.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We’re excited to share our recent paper &amp;quot;&lt;a href=\"https://arxiv.org/pdf/2406.08993\"&gt;[NeurIPS 2024] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification&lt;/a&gt;.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;In this study, we conduct a thorough review of classic GNNs for node classification tasks. Our findings suggest that the superior performance often reported by state-of-the-art graph learning models may be due to suboptimal hyperparameter configurations in classic GNNs. By fine-tuning these hyperparameters, we show that classic GNNs outperform the latest models on 17 out of 18 widely used node classification datasets.&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://t.co/QeNSn2D9CN\"&gt;https://github.com/LUOyk1999/tunedGNN&lt;/a&gt;&lt;br/&gt;\nArxiv: &lt;a href=\"https://t.co/MD4mVTnHk8\"&gt;https://arxiv.org/abs/2406.08993&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you find our work interesting, we’d greatly appreciate a ⭐️ on GitHub!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnsn54",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "luoyuankai",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnsn54/r_classic_gnns_gcns_graphsages_gats_are_strong/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731213139.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello everyone. I'm currently preparing a data set for a project in my company that aims to estimate the price of industrial carbon capture plants we build. The plant extracts CO2 from flue gas from e.g. chemical processes that emit a lot of CO2. Based on the flue gas composition, the engineer designs the plant, which can be a really time-consuming process. The data I'm currently preparing will consist of previously created offers from engineers.\n\nMy aim of the project is to build a model which uses the flue gas composition (around 10 floating point values) to estimate the costs of a plant or to recommend a similar project. The requirements for the project are not yet set but considering the model should be explainable and be able to handle smaller data sets, a regression tree might be the first thing I'd like to try once the data is ready.\n\nHas anyone read of useful papers or has experience from similar projects? Most of the papers I find are about cost estimation of 3D parts that use geometrical data as input.",
            "author_fullname": "t2_6xbf5dce",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R]/[P] Looking for papers about cost estimation for industrial plants",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnzcxi",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731240854.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I&amp;#39;m currently preparing a data set for a project in my company that aims to estimate the price of industrial carbon capture plants we build. The plant extracts CO2 from flue gas from e.g. chemical processes that emit a lot of CO2. Based on the flue gas composition, the engineer designs the plant, which can be a really time-consuming process. The data I&amp;#39;m currently preparing will consist of previously created offers from engineers.&lt;/p&gt;\n\n&lt;p&gt;My aim of the project is to build a model which uses the flue gas composition (around 10 floating point values) to estimate the costs of a plant or to recommend a similar project. The requirements for the project are not yet set but considering the model should be explainable and be able to handle smaller data sets, a regression tree might be the first thing I&amp;#39;d like to try once the data is ready.&lt;/p&gt;\n\n&lt;p&gt;Has anyone read of useful papers or has experience from similar projects? Most of the papers I find are about cost estimation of 3D parts that use geometrical data as input.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnzcxi",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Maendli",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnzcxi/rp_looking_for_papers_about_cost_estimation_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnzcxi/rp_looking_for_papers_about_cost_estimation_for/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731240854.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello! I'm working with a dataset where I have 300 jobs, each with a single target label. For each job, I have around 1000 data points (rows), and each data point is represented by a 17-dimensional vector with various parameters.\n\nI’d like to reduce these 1000 rows for each job down to a single representative vector to use for model training. However, I want to avoid just using the mean and variance of each column, as I think this would lose too much information.\n\nWould using PCA be a good approach here? If so, could I use the first principal component (PCA1) and its associated variance to form a single representative vector? For example, would projecting each 17D vector onto PCA1 and then taking a weighted average of these projections (weighted by PCA1’s explained variance) yield a good single vector per job?\n\nThank you very much and have a nice weekend.",
            "author_fullname": "t2_2f67abbo",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Best Approach to Dimensionality Reduction with PCA for Multi-Line Data Per Job?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnyyol",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731239346.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello! I&amp;#39;m working with a dataset where I have 300 jobs, each with a single target label. For each job, I have around 1000 data points (rows), and each data point is represented by a 17-dimensional vector with various parameters.&lt;/p&gt;\n\n&lt;p&gt;I’d like to reduce these 1000 rows for each job down to a single representative vector to use for model training. However, I want to avoid just using the mean and variance of each column, as I think this would lose too much information.&lt;/p&gt;\n\n&lt;p&gt;Would using PCA be a good approach here? If so, could I use the first principal component (PCA1) and its associated variance to form a single representative vector? For example, would projecting each 17D vector onto PCA1 and then taking a weighted average of these projections (weighted by PCA1’s explained variance) yield a good single vector per job?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much and have a nice weekend.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnyyol",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "aaronhallam773",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnyyol/d_best_approach_to_dimensionality_reduction_with/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731239346.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Help needed to run 3D model generation code with .ckpt files on CUDA 12.5 GPU (RTX A6000) ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnwmo5",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2aowaq4e",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "crosspost_parent_list": [
                {
                    "approved_at_utc": null,
                    "subreddit": "computervision",
                    "selftext": "[https://github.com/zhenpeiyang/FvOR/](https://github.com/zhenpeiyang/FvOR/)\n\nHey everyone,\n\nI'm working on a project to generate 3D models from a few images, using code from a GitHub repo related to a recent paper. The repository provides pretrained `.ckpt` files, which I want to use to generate 3D models from image inputs. I have access to a server equipped with an NVIDIA RTX A6000 GPU, but I'm running into issues with CUDA compatibility.\n\n# Issue\n\nThe code is designed for CUDA 10.1, but the server GPU operates on CUDA 12.5. When I try to execute the code, I get an error indicating a CUDA version mismatch, which stops me from running the model. Downgrading CUDA isn’t an option here since I’m using a shared server environment.\n\n# Things I've Considered\n\n1. **Docker**: I’m thinking about setting up a Docker container with CUDA 10.1 to run the code in a compatible environment. However, I’m new to Docker, so if anyone has advice on how to set up and configure it specifically for CUDA 10.1 compatibility, I’d appreciate it.\n2. **PyTorch Compatibility**: I’m using PyTorch 1.7.1. Would upgrading/downgrading PyTorch in Docker help avoid any CUDA compatibility issues? Any tips on which version to target?\n\n# Additional Info\n\n* **GPU**: NVIDIA RTX A6000\n* **CUDA Version**: 12.5 (host)\n* **PyTorch Version**: 1.7.1\n* **Goal**: Use the provided `.ckpt` files with input images to get a 3D model output.\n\nAny guidance on setting up Docker or resolving the compatibility issue would be greatly appreciated. Thanks in advance!",
                    "author_fullname": "t2_2aowaq4e",
                    "saved": false,
                    "mod_reason_title": null,
                    "gilded": 0,
                    "clicked": false,
                    "title": "Help needed to run 3D model generation code with .ckpt files on CUDA 12.5 GPU (RTX A6000)\n\n",
                    "link_flair_richtext": [],
                    "subreddit_name_prefixed": "r/computervision",
                    "hidden": false,
                    "pwls": 6,
                    "link_flair_css_class": "",
                    "downs": 0,
                    "thumbnail_height": null,
                    "top_awarded_type": null,
                    "hide_score": false,
                    "name": "t3_1gnwgub",
                    "quarantine": false,
                    "link_flair_text_color": "dark",
                    "upvote_ratio": 1.0,
                    "author_flair_background_color": null,
                    "subreddit_type": "public",
                    "ups": 3,
                    "total_awards_received": 0,
                    "media_embed": {},
                    "thumbnail_width": null,
                    "author_flair_template_id": null,
                    "is_original_content": false,
                    "user_reports": [],
                    "secure_media": null,
                    "is_reddit_media_domain": false,
                    "is_meta": false,
                    "category": null,
                    "secure_media_embed": {},
                    "link_flair_text": "Help: Project",
                    "can_mod_post": false,
                    "score": 3,
                    "approved_by": null,
                    "is_created_from_ads_ui": false,
                    "author_premium": false,
                    "thumbnail": "self",
                    "edited": false,
                    "author_flair_css_class": null,
                    "author_flair_richtext": [],
                    "gildings": {},
                    "post_hint": "self",
                    "content_categories": null,
                    "is_self": true,
                    "mod_note": null,
                    "created": 1731228665.0,
                    "link_flair_type": "text",
                    "wls": 6,
                    "removed_by_category": null,
                    "banned_by": null,
                    "author_flair_type": "text",
                    "domain": "self.computervision",
                    "allow_live_comments": false,
                    "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/zhenpeiyang/FvOR/\"&gt;https://github.com/zhenpeiyang/FvOR/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project to generate 3D models from a few images, using code from a GitHub repo related to a recent paper. The repository provides pretrained &lt;code&gt;.ckpt&lt;/code&gt; files, which I want to use to generate 3D models from image inputs. I have access to a server equipped with an NVIDIA RTX A6000 GPU, but I&amp;#39;m running into issues with CUDA compatibility.&lt;/p&gt;\n\n&lt;h1&gt;Issue&lt;/h1&gt;\n\n&lt;p&gt;The code is designed for CUDA 10.1, but the server GPU operates on CUDA 12.5. When I try to execute the code, I get an error indicating a CUDA version mismatch, which stops me from running the model. Downgrading CUDA isn’t an option here since I’m using a shared server environment.&lt;/p&gt;\n\n&lt;h1&gt;Things I&amp;#39;ve Considered&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;: I’m thinking about setting up a Docker container with CUDA 10.1 to run the code in a compatible environment. However, I’m new to Docker, so if anyone has advice on how to set up and configure it specifically for CUDA 10.1 compatibility, I’d appreciate it.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch Compatibility&lt;/strong&gt;: I’m using PyTorch 1.7.1. Would upgrading/downgrading PyTorch in Docker help avoid any CUDA compatibility issues? Any tips on which version to target?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;Additional Info&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPU&lt;/strong&gt;: NVIDIA RTX A6000&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CUDA Version&lt;/strong&gt;: 12.5 (host)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch Version&lt;/strong&gt;: 1.7.1&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: Use the provided &lt;code&gt;.ckpt&lt;/code&gt; files with input images to get a 3D model output.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any guidance on setting up Docker or resolving the compatibility issue would be greatly appreciated. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
                    "likes": null,
                    "suggested_sort": null,
                    "banned_at_utc": null,
                    "view_count": null,
                    "archived": false,
                    "no_follow": true,
                    "is_crosspostable": false,
                    "pinned": false,
                    "over_18": false,
                    "preview": {
                        "images": [
                            {
                                "source": {
                                    "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?auto=webp&amp;s=60844cbf766c7bc3fde6589337a3c3d87aec096d",
                                    "width": 1200,
                                    "height": 600
                                },
                                "resolutions": [
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca8eed56f1b371efc3f7acc19d86311e95a1a03",
                                        "width": 108,
                                        "height": 54
                                    },
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=60b103a12d38b3f11fd3a388ee583aaf011cfd48",
                                        "width": 216,
                                        "height": 108
                                    },
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b762542a7432804aad7b5140be3675fa14e6846f",
                                        "width": 320,
                                        "height": 160
                                    },
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a64c68fbc4a60fa8e08cf8487ac5ad0b0a5f31fc",
                                        "width": 640,
                                        "height": 320
                                    },
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e06d3c864ca5468ff591b318768961a207b229d",
                                        "width": 960,
                                        "height": 480
                                    },
                                    {
                                        "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2727014fb29c57fe9246d63ca121ce4ee9d38dc",
                                        "width": 1080,
                                        "height": 540
                                    }
                                ],
                                "variants": {},
                                "id": "Ew6wmf8lNnwgj1dqeNJ3K9zmIPbh-iHZ7xeywlpLT7o"
                            }
                        ],
                        "enabled": false
                    },
                    "all_awardings": [],
                    "awarders": [],
                    "media_only": false,
                    "link_flair_template_id": "2be07b9a-850c-11eb-9ef0-0e67fd476361",
                    "can_gild": false,
                    "spoiler": false,
                    "locked": false,
                    "author_flair_text": null,
                    "treatment_tags": [],
                    "visited": false,
                    "removed_by": null,
                    "num_reports": null,
                    "distinguished": null,
                    "subreddit_id": "t5_2rfzn",
                    "author_is_blocked": false,
                    "mod_reason_by": null,
                    "removal_reason": null,
                    "link_flair_background_color": "#fdff99",
                    "id": "1gnwgub",
                    "is_robot_indexable": true,
                    "report_reasons": null,
                    "author": "mnkhtlg",
                    "discussion_type": null,
                    "num_comments": 3,
                    "send_replies": true,
                    "contest_mode": false,
                    "mod_reports": [],
                    "author_patreon_flair": false,
                    "author_flair_text_color": null,
                    "permalink": "/r/computervision/comments/1gnwgub/help_needed_to_run_3d_model_generation_code_with/",
                    "stickied": false,
                    "url": "https://www.reddit.com/r/computervision/comments/1gnwgub/help_needed_to_run_3d_model_generation_code_with/",
                    "subreddit_subscribers": 103347,
                    "created_utc": 1731228665.0,
                    "num_crossposts": 1,
                    "media": null,
                    "is_video": false
                }
            ],
            "created": 1731229387.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "/r/computervision/comments/1gnwgub/help_needed_to_run_3d_model_generation_code_with/",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?auto=webp&amp;s=60844cbf766c7bc3fde6589337a3c3d87aec096d",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=dca8eed56f1b371efc3f7acc19d86311e95a1a03",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=60b103a12d38b3f11fd3a388ee583aaf011cfd48",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b762542a7432804aad7b5140be3675fa14e6846f",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a64c68fbc4a60fa8e08cf8487ac5ad0b0a5f31fc",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e06d3c864ca5468ff591b318768961a207b229d",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/P7Zs5swusgZFsnxLVIIQFCF9nsgn_7sn8vihfBY96y0.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2727014fb29c57fe9246d63ca121ce4ee9d38dc",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "Ew6wmf8lNnwgj1dqeNJ3K9zmIPbh-iHZ7xeywlpLT7o"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnwmo5",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "mnkhtlg",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "crosspost_parent": "t3_1gnwgub",
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnwmo5/p_help_needed_to_run_3d_model_generation_code/",
            "stickied": false,
            "url": "/r/computervision/comments/1gnwgub/help_needed_to_run_3d_model_generation_code_with/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731229387.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "hello everyone, I am a 3rd year cse student. I built this site called [https://www.mldl.study/](https://www.mldl.study/) last month. this site is for anyone who is \"new\" to machine learning and deep learning and is confused about where to start. I built this because I was confused about it too. It has got proper video lectures, articles, research papers, visualizations, kaggle competitions and basically everything you need to master ml and dl in proper order.\n\n\n\ni just added google analytics 25 days back and I saw that I have got like 450 users and 135 returning users. I built this just to help my college friends but I am so glad that its helping others too. I just wanted to share this as I am so happy about this. This gives me confidence that I can build something more cooler and useful in future.\n\n\n\nThanks everyone. I got little push in my analytics from here only. THANKYOU!!\n\n\n\n(I am also open to suggestions and all, what I can do to grow it even more)\n\nhttps://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;format=png&amp;auto=webp&amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305\n\n",
            "author_fullname": "t2_md6btu09",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Built a roadmap site and got 450 users in 25 days and I am so happy!!!!!!",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": 51,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "s9v6omy5f10e1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 39,
                            "x": 108,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=842c2c03980e4125ec9382e13325ac3524ca0a79"
                        },
                        {
                            "y": 79,
                            "x": 216,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7badd75971213c76594d21aa869412a79d4aef24"
                        },
                        {
                            "y": 118,
                            "x": 320,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=40a3f6ea37ef38e96d7fd193f9e004ae842406cc"
                        },
                        {
                            "y": 237,
                            "x": 640,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a08ada51f77465f08dcff3f6c896448bf8c2c4f7"
                        },
                        {
                            "y": 355,
                            "x": 960,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bd34fc49fd25923fb9457c5202f285d3bcb312e1"
                        },
                        {
                            "y": 399,
                            "x": 1080,
                            "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=91dcbf0cc505163197be68454b8c7450680d053d"
                        }
                    ],
                    "s": {
                        "y": 577,
                        "x": 1558,
                        "u": "https://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;format=png&amp;auto=webp&amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305"
                    },
                    "id": "s9v6omy5f10e1"
                }
            },
            "name": "t3_1gnwfhn",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.74,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 33,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 33,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/4b2UbAU1kNIreVsOAKtpeX_K5Uy-HCvm8TSytI4J_Zg.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731228501.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hello everyone, I am a 3rd year cse student. I built this site called &lt;a href=\"https://www.mldl.study/\"&gt;https://www.mldl.study/&lt;/a&gt; last month. this site is for anyone who is &amp;quot;new&amp;quot; to machine learning and deep learning and is confused about where to start. I built this because I was confused about it too. It has got proper video lectures, articles, research papers, visualizations, kaggle competitions and basically everything you need to master ml and dl in proper order.&lt;/p&gt;\n\n&lt;p&gt;i just added google analytics 25 days back and I saw that I have got like 450 users and 135 returning users. I built this just to help my college friends but I am so glad that its helping others too. I just wanted to share this as I am so happy about this. This gives me confidence that I can build something more cooler and useful in future.&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone. I got little push in my analytics from here only. THANKYOU!!&lt;/p&gt;\n\n&lt;p&gt;(I am also open to suggestions and all, what I can do to grow it even more)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305\"&gt;https://preview.redd.it/s9v6omy5f10e1.png?width=1558&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eeb9a22012e2e3806245e9267a1187bb91e75305&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnwfhn",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Grouchy-Breakfast-20",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnwfhn/p_built_a_roadmap_site_and_got_450_users_in_25/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731228501.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I would like to know if we can view new responses and updated ratings from the reviewers as they submit them in OpenReview, or if we need to wait until December 9th. Additionally, can all reviewers see the responses we submitted to other reviewers during the rebuttal period, or is each reviewer only able to view the response directed to them?",
            "author_fullname": "t2_3dzmma7v",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] AAAI Phase 2 Rebuttal Response and Reviewer Updates in Openreview",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnvy2i",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.73,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731226380.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like to know if we can view new responses and updated ratings from the reviewers as they submit them in OpenReview, or if we need to wait until December 9th. Additionally, can all reviewers see the responses we submitted to other reviewers during the rebuttal period, or is each reviewer only able to view the response directed to them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnvy2i",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "morphinejunkie",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnvy2i/r_aaai_phase_2_rebuttal_response_and_reviewer/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731226380.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "EDIT: I didn't mean decoder per se, and it's my bad for forgetting to clarify that. What I meant was for a (more) direct computational or mathematical framework that doesn't involve training another network to do the reverse-embedding.\n\n----------\n\nAs the title alluded, are there methods and/or processes to do reverse-embedding that perhaps are currently being researched? From the admittedly preliminary internet-sleuthing I did yesterday, it seems to be essentially impossible because of how intractable the inverse-mapping is gonna play out. And on that vein, how it's practically impossible to carry out with the current hardware and setup that we have.\n\nHowever, perhaps some of you might know some literature that might've gone into that direction, even if at theoretical or rudimentary level and it'd be greatly appreciated if you can point me to those resources. You're also welcome to share your thoughts and theories as well.\n\nExpanding from reverse-embedding, is it possible to go beyond the range of the embedding vectors/tensors so as to reverse-embed said embedding vectors/tensors and then retrieve the resulting text, image, etc. from them?\n\nMany thanks in advance!",
            "author_fullname": "t2_lp3x6",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] On \"reverse\" embedding (i.e. embedding vectors/tensors to text, image, etc.)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnrta9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731212246.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731210236.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;EDIT: I didn&amp;#39;t mean decoder per se, and it&amp;#39;s my bad for forgetting to clarify that. What I meant was for a (more) direct computational or mathematical framework that doesn&amp;#39;t involve training another network to do the reverse-embedding.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;As the title alluded, are there methods and/or processes to do reverse-embedding that perhaps are currently being researched? From the admittedly preliminary internet-sleuthing I did yesterday, it seems to be essentially impossible because of how intractable the inverse-mapping is gonna play out. And on that vein, how it&amp;#39;s practically impossible to carry out with the current hardware and setup that we have.&lt;/p&gt;\n\n&lt;p&gt;However, perhaps some of you might know some literature that might&amp;#39;ve gone into that direction, even if at theoretical or rudimentary level and it&amp;#39;d be greatly appreciated if you can point me to those resources. You&amp;#39;re also welcome to share your thoughts and theories as well.&lt;/p&gt;\n\n&lt;p&gt;Expanding from reverse-embedding, is it possible to go beyond the range of the embedding vectors/tensors so as to reverse-embed said embedding vectors/tensors and then retrieve the resulting text, image, etc. from them?&lt;/p&gt;\n\n&lt;p&gt;Many thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnrta9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "YsrYsl",
            "discussion_type": null,
            "num_comments": 21,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnrta9/d_on_reverse_embedding_ie_embedding/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731210236.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "In machine learning we work with log probabilities a lot, attempting to maximize log probability. This makes sense from a numerical perspective since adding is easier than multiplying but I am also wondering if there is a fundamental meaning behind \"log probability.\"\n\nFor instance, log probability is used a lot in information theory, and is the negative of 'information'. Can we view minimizing the negative log likelihood in terms of information theory? Is it maximizing/minimizing some metric of information?",
            "author_fullname": "t2_lh0xltdq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Log Probability and Information Theory",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnrpfe",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 84,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 84,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731209867.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In machine learning we work with log probabilities a lot, attempting to maximize log probability. This makes sense from a numerical perspective since adding is easier than multiplying but I am also wondering if there is a fundamental meaning behind &amp;quot;log probability.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;For instance, log probability is used a lot in information theory, and is the negative of &amp;#39;information&amp;#39;. Can we view minimizing the negative log likelihood in terms of information theory? Is it maximizing/minimizing some metric of information?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnrpfe",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "masonw32",
            "discussion_type": null,
            "num_comments": 18,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnrpfe/d_log_probability_and_information_theory/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731209867.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Please post your personal projects, startups, product placements, collaboration needs, blogs etc.\n\nPlease mention the payment and pricing requirements for products and services.\n\nPlease do not post link shorteners, link aggregator websites , or auto-subscribe links.\n\n--\n\nAny abuse of trust will lead to bans.\n\nEncourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\n--\n\nMeta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",
            "author_fullname": "t2_6l4z3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Self-Promotion Thread",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnrb08",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.72,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731208511.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please post your personal projects, startups, product placements, collaboration needs, blogs etc.&lt;/p&gt;\n\n&lt;p&gt;Please mention the payment and pricing requirements for products and services.&lt;/p&gt;\n\n&lt;p&gt;Please do not post link shorteners, link aggregator websites , or auto-subscribe links.&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;Any abuse of trust will lead to bans.&lt;/p&gt;\n\n&lt;p&gt;Encourage others who create new posts for questions to post here instead!&lt;/p&gt;\n\n&lt;p&gt;Thread will stay alive until next one so keep posting after the date in the title.&lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnrb08",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "AutoModerator",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731208511.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_e8kks",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[N] The ARC prize offers $600,000 for few-shot learning of puzzles made of colored squares on a grid.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "two",
            "downs": 0,
            "thumbnail_height": 73,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnnstd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 109,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 109,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/T60ZeRDeKyitaadQvibHrHboUSfYL5X13PpSwSE5qZ4.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731197300.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arcprize.org",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arcprize.org/competition",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?auto=webp&amp;s=a37ac9254e458a4afe738764d05f0dac09f3b51b",
                            "width": 1200,
                            "height": 630
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb04d304cb3923d66707d3927c07c80921a43cc0",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4f4c074cf45ccc347407714977024cae1a6b2b3b",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f675cf7145c93ff3caf74625c8fcb0985e1af8cc",
                                "width": 320,
                                "height": 168
                            },
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=12fe1cebb1de52a5010be00250423f821bd40d0a",
                                "width": 640,
                                "height": 336
                            },
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=31bf20b08a85cfb0ee6149e84f559313126b91a7",
                                "width": 960,
                                "height": 504
                            },
                            {
                                "url": "https://external-preview.redd.it/EgcNoTp8CXkUREswLOJFaRCpJzlzLh4JgeugjVVm_00.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=645b1854dd080aa2e9b9056fdca2d7060d93dfcc",
                                "width": 1080,
                                "height": 567
                            }
                        ],
                        "variants": {},
                        "id": "g5_XbspyVoCUgoU87RpXGpJzxJV5r0xDHqeIzldwGzI"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnnstd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "moschles",
            "discussion_type": null,
            "num_comments": 37,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnnstd/n_the_arc_prize_offers_600000_for_fewshot/",
            "stickied": false,
            "url": "https://arcprize.org/competition",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731197300.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I see on https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:\n\n| File Name          | Size   |\n|--------------------|--------|\n| model.onnx         | 654 MB |\n| model_fp16.onnx    | 327 MB |\n| model_q4.onnx      | 200 MB |\n| model_q4f16.onnx   | 134 MB |\n\n\nI understand that:\n\n- `model.onnx` is the fp32 model,\n- `model_fp16.onnx` is the model whose weights are quantized to `fp16`\n\nI don't understand the size of `model_q4.onnx` and `model_q4f16.onnx`\n\n1. Why is `model_q4.onnx` 200 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4.onnx` meant that the weights are quantized to 4 bits.\n2. Why is `model_q4f16.onnx` 134 MB instead of 654 MB / 4 = 163.5 MB? I thought `model_q4f16.onnx` meant that the weights are quantized to 4 bits and activations are fp16, since https://llm.mlc.ai/docs/compilation/configure_quantization.html states:\n\n   &gt;  `qAfB(_id)`, where `A` represents the number of bits for storing weights and `B` represents the number of bits for storing activations. \n\n  and [Why do activations need more bits (16bit) than weights (8bit) in tensor flow's neural network quantization framework?](https://stackoverflow.com/a/72397979/395857) indicates that activations don't count toward the model size (understandably).",
            "author_fullname": "t2_kprlc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why are model_q4.onnx and model_q4f16.onnx not 4 times smaller than model.onnx? [D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gni61w",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731181410.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see on &lt;a href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:\"&gt;https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct/tree/main/onnx:&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;File Name&lt;/th&gt;\n&lt;th&gt;Size&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;model.onnx&lt;/td&gt;\n&lt;td&gt;654 MB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;model_fp16.onnx&lt;/td&gt;\n&lt;td&gt;327 MB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;model_q4.onnx&lt;/td&gt;\n&lt;td&gt;200 MB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;model_q4f16.onnx&lt;/td&gt;\n&lt;td&gt;134 MB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;I understand that:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;model.onnx&lt;/code&gt; is the fp32 model,&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;model_fp16.onnx&lt;/code&gt; is the model whose weights are quantized to &lt;code&gt;fp16&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I don&amp;#39;t understand the size of &lt;code&gt;model_q4.onnx&lt;/code&gt; and &lt;code&gt;model_q4f16.onnx&lt;/code&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Why is &lt;code&gt;model_q4.onnx&lt;/code&gt; 200 MB instead of 654 MB / 4 = 163.5 MB? I thought &lt;code&gt;model_q4.onnx&lt;/code&gt; meant that the weights are quantized to 4 bits.&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Why is &lt;code&gt;model_q4f16.onnx&lt;/code&gt; 134 MB instead of 654 MB / 4 = 163.5 MB? I thought &lt;code&gt;model_q4f16.onnx&lt;/code&gt; meant that the weights are quantized to 4 bits and activations are fp16, since &lt;a href=\"https://llm.mlc.ai/docs/compilation/configure_quantization.html\"&gt;https://llm.mlc.ai/docs/compilation/configure_quantization.html&lt;/a&gt; states:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;code&gt;qAfB(_id)&lt;/code&gt;, where &lt;code&gt;A&lt;/code&gt; represents the number of bits for storing weights and &lt;code&gt;B&lt;/code&gt; represents the number of bits for storing activations. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;and &lt;a href=\"https://stackoverflow.com/a/72397979/395857\"&gt;Why do activations need more bits (16bit) than weights (8bit) in tensor flow&amp;#39;s neural network quantization framework?&lt;/a&gt; indicates that activations don&amp;#39;t count toward the model size (understandably).&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gni61w",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Franck_Dernoncourt",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gni61w/why_are_model_q4onnx_and_model_q4f16onnx_not_4/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gni61w/why_are_model_q4onnx_and_model_q4f16onnx_not_4/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731181410.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I've been experimenting with adaptive optimizers such as Prodigy, and Dadapt-LION.\n\nIve noticed that if i run them over a 1 million step dataset, they will start at 1e06,. .then go up to lets say 5e06.... and later still go up to 9e06 and stay there.\n\nBut if I stop them halfway..... then train on the results, it might go up to only 6e06.\n\nAre there no standard ways to at worst, reset them, but better still actually adjust downwards when appropriate?\n\nI guess ideally I would like some thing with an effect like a reverse \"cosine with hard reset\".\n\nInstead of SLOOOWLY forcing the LR lower and lower.. and then suddenly letting it pop up again...\n\ninstead suddenly force the LR, etc to its original starting point, and let it redo the adaptive growth process again? And repeat that for some number of learning cycles.\n\nAnything like that?",
            "author_fullname": "t2_hpwi1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] adaptive optimizers, downscaling, and resets",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnh4fe",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.29,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731179463.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731178570.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with adaptive optimizers such as Prodigy, and Dadapt-LION.&lt;/p&gt;\n\n&lt;p&gt;Ive noticed that if i run them over a 1 million step dataset, they will start at 1e06,. .then go up to lets say 5e06.... and later still go up to 9e06 and stay there.&lt;/p&gt;\n\n&lt;p&gt;But if I stop them halfway..... then train on the results, it might go up to only 6e06.&lt;/p&gt;\n\n&lt;p&gt;Are there no standard ways to at worst, reset them, but better still actually adjust downwards when appropriate?&lt;/p&gt;\n\n&lt;p&gt;I guess ideally I would like some thing with an effect like a reverse &amp;quot;cosine with hard reset&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Instead of SLOOOWLY forcing the LR lower and lower.. and then suddenly letting it pop up again...&lt;/p&gt;\n\n&lt;p&gt;instead suddenly force the LR, etc to its original starting point, and let it redo the adaptive growth process again? And repeat that for some number of learning cycles.&lt;/p&gt;\n\n&lt;p&gt;Anything like that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnh4fe",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "lostinspaz",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnh4fe/d_adaptive_optimizers_downscaling_and_resets/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnh4fe/d_adaptive_optimizers_downscaling_and_resets/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731178570.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Jay McClelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at Stanford University in the psychology, linguistics, and computer science departments. Together with David Rumelhart, Jay published the two volume work Parallel Distributed Processing, which has led to the flourishing of the connectionist approach to understanding cognition.\n\nIn this conversation, Jay gives us a crash course in how neurons and biological brains work. This sets the stage for how psychologists such as Jay, David Rumelhart, and Geoffrey Hinton historically approached the development of models of cognition and ultimately artificial intelligence. We also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation.\n\nhttps://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af\n\nhttps://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6\n\nYoutube:  \n[https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;index=1&amp;pp=iAQB](https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;index=1&amp;pp=iAQB)\n\nSpotify: [https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG](https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG)\n\nRSS: [https://feed.podbean.com/cartesiancafe/feed.xml](https://feed.podbean.com/cartesiancafe/feed.xml)",
            "author_fullname": "t2_aou4ih3o",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Jay McClelland explains Parallel Distributed Processing, how the brain works, Hebbian learning, and backpropagation",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 78,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "s7xv0pmk2xzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 60,
                            "x": 108,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5388911d7281049d2334086bb88c23ca4ab20f48"
                        },
                        {
                            "y": 121,
                            "x": 216,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bac09f74db1731680c797ac1b1fe2b2d0fde824a"
                        },
                        {
                            "y": 180,
                            "x": 320,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dd5ca60ff896dff8d73d184c9cd3b4c45fb7f168"
                        },
                        {
                            "y": 360,
                            "x": 640,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b5035b4c4b6464fcb9cb279d5b585f3644f4bdf"
                        },
                        {
                            "y": 540,
                            "x": 960,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a57ff3e1c8b5315c2bb8fdcff84182fdef37ebd"
                        },
                        {
                            "y": 607,
                            "x": 1080,
                            "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=25ddc3f2295ffed8bb7349eb67f2f6b5a901c2b6"
                        }
                    ],
                    "s": {
                        "y": 1080,
                        "x": 1920,
                        "u": "https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af"
                    },
                    "id": "s7xv0pmk2xzd1"
                },
                "h4sqjoim2xzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 60,
                            "x": 108,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4d6a865b607fb4eeb714efa957e254149bc845c"
                        },
                        {
                            "y": 120,
                            "x": 216,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=10436ae82116faf2f17de7a83747c532a70f99bc"
                        },
                        {
                            "y": 178,
                            "x": 320,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d8d3875ad8d3b881684ef07f89daee023681413"
                        },
                        {
                            "y": 357,
                            "x": 640,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=452d2ee1268844dba7a267e9a797405432e59b2a"
                        },
                        {
                            "y": 536,
                            "x": 960,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bde46efe44cd0d57d93b6adc70ae38626cb5280f"
                        },
                        {
                            "y": 604,
                            "x": 1080,
                            "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6fea22dcc1b184b70fa85855ffad1fb9b77e5af"
                        }
                    ],
                    "s": {
                        "y": 604,
                        "x": 1080,
                        "u": "https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6"
                    },
                    "id": "h4sqjoim2xzd1"
                }
            },
            "name": "t3_1gng4fy",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 22,
            "total_awards_received": 0,
            "media_embed": {
                "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/yQbJNEhgYUw?list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;",
                "width": 356,
                "scrolling": false,
                "height": 200
            },
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": {
                "type": "youtube.com",
                "oembed": {
                    "provider_url": "https://www.youtube.com/",
                    "version": "1.0",
                    "title": "Jay McClelland | Neural Networks: Artificial and Biological | The Cartesian Cafe with Timothy Nguyen",
                    "type": "video",
                    "thumbnail_width": 480,
                    "height": 200,
                    "width": 356,
                    "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/yQbJNEhgYUw?list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;",
                    "author_name": "Timothy Nguyen",
                    "provider_name": "YouTube",
                    "thumbnail_url": "https://i.ytimg.com/vi/yQbJNEhgYUw/hqdefault.jpg",
                    "thumbnail_height": 360,
                    "author_url": "https://www.youtube.com/@TimothyNguyen"
                }
            },
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {
                "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/yQbJNEhgYUw?list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;",
                "width": 356,
                "scrolling": false,
                "media_domain_url": "https://www.redditmedia.com/mediaembed/1gng4fy",
                "height": 200
            },
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 22,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/8RQtRvmXhNSRIH0ZE4BJvqvOK_8l2voOoq50W0QZtPw.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731175871.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Jay McClelland is a pioneer in the field of artificial intelligence and is a cognitive psychologist and professor at Stanford University in the psychology, linguistics, and computer science departments. Together with David Rumelhart, Jay published the two volume work Parallel Distributed Processing, which has led to the flourishing of the connectionist approach to understanding cognition.&lt;/p&gt;\n\n&lt;p&gt;In this conversation, Jay gives us a crash course in how neurons and biological brains work. This sets the stage for how psychologists such as Jay, David Rumelhart, and Geoffrey Hinton historically approached the development of models of cognition and ultimately artificial intelligence. We also discuss alternative approaches to neural computation such as symbolic and neuroscientific ones and the development of backpropagation.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af\"&gt;https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6\"&gt;https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Youtube:&lt;br/&gt;\n&lt;a href=\"https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;amp;index=1&amp;amp;pp=iAQB\"&gt;https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;amp;index=1&amp;amp;pp=iAQB&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Spotify: &lt;a href=\"https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG\"&gt;https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;RSS: &lt;a href=\"https://feed.podbean.com/cartesiancafe/feed.xml\"&gt;https://feed.podbean.com/cartesiancafe/feed.xml&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/de6k31fQU3WqwoeWfQizH7ht8ZzJYQ0M35hWTRJTZj0.jpg?auto=webp&amp;s=dbec374d4b6f009e6ccde3d1327293c96863920e",
                            "width": 480,
                            "height": 360
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/de6k31fQU3WqwoeWfQizH7ht8ZzJYQ0M35hWTRJTZj0.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=792db844fc8e9ab48940ef876c85a62bf2d7053b",
                                "width": 108,
                                "height": 81
                            },
                            {
                                "url": "https://external-preview.redd.it/de6k31fQU3WqwoeWfQizH7ht8ZzJYQ0M35hWTRJTZj0.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=32da8b83631f44d4b3e374ccef0fea3180f2e0e5",
                                "width": 216,
                                "height": 162
                            },
                            {
                                "url": "https://external-preview.redd.it/de6k31fQU3WqwoeWfQizH7ht8ZzJYQ0M35hWTRJTZj0.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9038ed894af802dde9709266ec78260ff011dbd0",
                                "width": 320,
                                "height": 240
                            }
                        ],
                        "variants": {},
                        "id": "VsVP6IQ9yBPvjAil7NfDerKTMd8RK_JAu4PvM3lZzwQ"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gng4fy",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "IamTimNguyen",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731175871.0,
            "num_crossposts": 0,
            "media": {
                "type": "youtube.com",
                "oembed": {
                    "provider_url": "https://www.youtube.com/",
                    "version": "1.0",
                    "title": "Jay McClelland | Neural Networks: Artificial and Biological | The Cartesian Cafe with Timothy Nguyen",
                    "type": "video",
                    "thumbnail_width": 480,
                    "height": 200,
                    "width": 356,
                    "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/yQbJNEhgYUw?list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;",
                    "author_name": "Timothy Nguyen",
                    "provider_name": "YouTube",
                    "thumbnail_url": "https://i.ytimg.com/vi/yQbJNEhgYUw/hqdefault.jpg",
                    "thumbnail_height": 360,
                    "author_url": "https://www.youtube.com/@TimothyNguyen"
                }
            },
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I was working with SAM2 and have been trying to figure out the best way to fine-tune it for my specific use case. A few considerations that I was hoping get some insights on:\n\n1. **Error Correction vs Generalization:** If I'm interested in fine-tuning the model to perform better on cases where it went wrong most on, can I retains its performance on the examples it was already doing well on. i.e. still maintaining (or even improving) its prior generalizability? Or should I have enough number of examples it was doing well already on to preserve that performance?\n2. **Which Components to Fine-Tune?** In terms of the model's architecture, I've seen different advice on whether to fine-tune just the **mask decoder**, the **prompt encoder**, or both. In your experience, is fine-tuning just the mask decoder enough to improve performance, or do you need to adjust the prompt encoder as well? Or maybe there's more to it—like the backbone or other parts of the model? Is it computationally too much of a difference? Or are there other downsides/considerations as well?\n3. **Real-World Experiences:** For those who have fine-tuned SAM before, how has your experience been? Any tips, tricks, or pitfalls I should watch out for? Also, how did you go about preparing your fine-tuning dataset? Any suggestions on balancing the diversity of data vs focusing on edge cases?",
            "author_fullname": "t2_qpxhx5zk0",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Advice on Fine-Tuning Meta's Segment Anything 2 (SAM) Model — Balancing Edge cases with Generalizability",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnf7zi",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731173461.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was working with SAM2 and have been trying to figure out the best way to fine-tune it for my specific use case. A few considerations that I was hoping get some insights on:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Error Correction vs Generalization:&lt;/strong&gt; If I&amp;#39;m interested in fine-tuning the model to perform better on cases where it went wrong most on, can I retains its performance on the examples it was already doing well on. i.e. still maintaining (or even improving) its prior generalizability? Or should I have enough number of examples it was doing well already on to preserve that performance?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which Components to Fine-Tune?&lt;/strong&gt; In terms of the model&amp;#39;s architecture, I&amp;#39;ve seen different advice on whether to fine-tune just the &lt;strong&gt;mask decoder&lt;/strong&gt;, the &lt;strong&gt;prompt encoder&lt;/strong&gt;, or both. In your experience, is fine-tuning just the mask decoder enough to improve performance, or do you need to adjust the prompt encoder as well? Or maybe there&amp;#39;s more to it—like the backbone or other parts of the model? Is it computationally too much of a difference? Or are there other downsides/considerations as well?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Real-World Experiences:&lt;/strong&gt; For those who have fine-tuned SAM before, how has your experience been? Any tips, tricks, or pitfalls I should watch out for? Also, how did you go about preparing your fine-tuning dataset? Any suggestions on balancing the diversity of data vs focusing on edge cases?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnf7zi",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "No_Cartoonist8629",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731173461.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_ff7e8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] When Machine Learning Tells the Wrong Story",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gne2x1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.88,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 57,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 57,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731170410.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "jackcook.com",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://jackcook.com/2024/11/09/bigger-fish.html",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gne2x1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "jackcook",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gne2x1/r_when_machine_learning_tells_the_wrong_story/",
            "stickied": false,
            "url": "https://jackcook.com/2024/11/09/bigger-fish.html",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731170410.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey people,\nI'm searching papers or hints for a computer vision task. I have implemented a Vision Transformer for image classification. In the next step I have to implement a predictor on top of the encoder network of the ViT, which predicts from enc(x_t) -&gt; enc(x_t+1). The predictor should predict the embedding of the next frame. my first idea is a MLP head or decoder network.\nIf someone has tackled a similar task, im happy about recommendations.\nTy",
            "author_fullname": "t2_gc6hmsvh",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] latent space forecasting of the next frame",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gncn8y",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731166449.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey people,\nI&amp;#39;m searching papers or hints for a computer vision task. I have implemented a Vision Transformer for image classification. In the next step I have to implement a predictor on top of the encoder network of the ViT, which predicts from enc(x_t) -&amp;gt; enc(x_t+1). The predictor should predict the embedding of the next frame. my first idea is a MLP head or decoder network.\nIf someone has tackled a similar task, im happy about recommendations.\nTy&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gncn8y",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Significant-Joke5751",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731166449.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "For some reason, I can't seem to find any well known benchmark datasets that have text or images as features, and real-valued targets. Any target range is fine ( (0,1), (-infinity, infinity), (0, infinity), etc.) I have found examples with *ordinal* classification targets (e.g. integer rating from 1-5), but that doesn't serve my purpose.\n\nDoes anyone know of any open source supervised ML data that fits this description? Preferably a benchmarked one with a performance leaderboard.",
            "author_fullname": "t2_ejrhu",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Benchmark or open source supervised datasets with text or image features and real-valued regression target?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gnb9z5",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731162536.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason, I can&amp;#39;t seem to find any well known benchmark datasets that have text or images as features, and real-valued targets. Any target range is fine ( (0,1), (-infinity, infinity), (0, infinity), etc.) I have found examples with &lt;em&gt;ordinal&lt;/em&gt; classification targets (e.g. integer rating from 1-5), but that doesn&amp;#39;t serve my purpose.&lt;/p&gt;\n\n&lt;p&gt;Does anyone know of any open source supervised ML data that fits this description? Preferably a benchmarked one with a performance leaderboard.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gnb9z5",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BreakingBaIIs",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731162536.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "[Last Week in Medical AI: Top LLM Research Papers\\/Models \\(November 2 - November 9, 2024\\)](https://preview.redd.it/nytnbrppcvzd1.png?width=1386&amp;format=png&amp;auto=webp&amp;s=2339a74f15050a972b113cee2a35e4ca11353852)\n\n  \n**Medical AI Paper of the Week:**\n\n* **Google presents**\\*: Exploring Large Language Models for Specialist-level Oncology Care\\*\n   * This paper evaluates AMIE, a conversational diagnostic AI system, in breast oncology using 50 synthetic cancer vignettes.   Enhanced with web search retrieval and a self-critique pipeline, AMIE outperformed internal medicine trainees and oncology fellows in generating management plans, evaluated using a detailed clinical rubric encompassing case summarization, plan safety, and treatment recommendations.\n\n**Medical LLM &amp; Other Models:**\n\n* AutoProteinEngine: Multimodal Protein LLM\n   * This paper introduces AutoProteinEngine (AutoPE), an LLM-powered multimodal AutoML framework for protein engineering, enabling biologists without deep learning expertise to interact with DL models using natural language.  AutoPE integrates LLMs with AutoML for model selection (sequence and graph modalities), hyperparameter optimization, and automated data retrieval, demonstrating significant performance improvements over traditional methods in two real-world protein engineering tasks. Code is available at:\n\n* GSCo: Generalist-Specialist AI Collaboration\n   * This paper introduces GSCo, a framework for medical image analysis combining Generalist Foundation Models (GFMs) and specialist models. It develops MedDr, the largest open-source medical GFM, and lightweight specialists for downstream tasks.\n\n* SAM for Lung X-ray Segmentation\n   * This paper explores the application of Meta AI's Segment Anything Model (SAM) to chest X-ray analysis for lung segmentation.  Using a transfer learning approach with fine-tuning, the study demonstrates improved performance compared to the original SAM, achieving results comparable to state-of-the-art models like U-Net.\n\n* MEG: Knowledge-Enhanced Medical QA\n   * This paper introduces MEG, a parameter-efficient method for augmenting Large Language Models (LLMs) with medical knowledge graphs using a lightweight mapping network.  Evaluated on four medical multiple-choice datasets, MEG achieves a 10.2% accuracy improvement over the Mistral-Instruct baseline and 6.7% over specialized models like BioMistral, demonstrating the benefit of knowledge graph integration.\n\n  \n  \n**Frameworks and Methodologies:**\n\n* BrainSegFounder: 3D Neuroimage Analysis\n* PASSION: Sub-Saharan Dermatology Dataset\n* Label Critic: Data-First Approach\n* Medprompt Runtime Strategies\n\n\n\n**Medical LLM Applications:**\n\n* CataractBot: Patient Support System\n* CheX-GPT: X-ray Report Enhancement\n* CardioAI: Cancer Cardiotoxicity Monitor\n* HealthQ: Healthcare Conversation Chain\n* PRObot: Diabetic Retinopathy Assistant\n\n  \n  \n**Medical LLMs &amp; Benchmarks:**\n\n* MediQ: Clinical Reasoning Benchmark\n* Touchstone: Segmentation Evaluation\n* Medical LLM Adaptation Progress\n* Fine-Tuning Medical QA Strategies\n\n  \n  \n**AI in Healthcare Ethics:**\n\n* Healthcare Robotics with LLMs\n* XAI in Clinical Practice\n* Precision Rehabilitation Framework\n* Multimodal AI Challenges\n\nFull thread in detail : [https://x.com/OpenlifesciAI/status/1855207141302473090](https://x.com/OpenlifesciAI/status/1855207141302473090)\n\n",
            "author_fullname": "t2_4l579v4l",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Last Week in Medical AI: Top LLM Research Papers/Models (November 2 - November 9, 2024)\n",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": 132,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "nytnbrppcvzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 101,
                            "x": 108,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4766974ddc239e9aad9e2defa314ef359d164aab"
                        },
                        {
                            "y": 203,
                            "x": 216,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=991d4fc4b27dbdb0c585bff6866ae142916a6915"
                        },
                        {
                            "y": 301,
                            "x": 320,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0e9ce164c209a14a3b97e6a9e56de7e23864b29b"
                        },
                        {
                            "y": 603,
                            "x": 640,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a03bb1e23aee1f8d1e526a1b41cc1b99b883968f"
                        },
                        {
                            "y": 905,
                            "x": 960,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=178058944fa67c8375ad0c959d3330a0807479b6"
                        },
                        {
                            "y": 1019,
                            "x": 1080,
                            "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7da17b7a398c26c9cbd1f98b4febf3158732adcf"
                        }
                    ],
                    "s": {
                        "y": 1308,
                        "x": 1386,
                        "u": "https://preview.redd.it/nytnbrppcvzd1.png?width=1386&amp;format=png&amp;auto=webp&amp;s=2339a74f15050a972b113cee2a35e4ca11353852"
                    },
                    "id": "nytnbrppcvzd1"
                }
            },
            "name": "t3_1gn8wqp",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/BsFFdQNkQ0JAY8_ZrqpxNkxln0RMpK5ggACx_MHkgKU.jpg",
            "edited": 1731155136.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731154858.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/nytnbrppcvzd1.png?width=1386&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2339a74f15050a972b113cee2a35e4ca11353852\"&gt;Last Week in Medical AI: Top LLM Research Papers/Models (November 2 - November 9, 2024)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Medical AI Paper of the Week:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Google presents&lt;/strong&gt;*: Exploring Large Language Models for Specialist-level Oncology Care*\n\n&lt;ul&gt;\n&lt;li&gt;This paper evaluates AMIE, a conversational diagnostic AI system, in breast oncology using 50 synthetic cancer vignettes.   Enhanced with web search retrieval and a self-critique pipeline, AMIE outperformed internal medicine trainees and oncology fellows in generating management plans, evaluated using a detailed clinical rubric encompassing case summarization, plan safety, and treatment recommendations.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Medical LLM &amp;amp; Other Models:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;AutoProteinEngine: Multimodal Protein LLM&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This paper introduces AutoProteinEngine (AutoPE), an LLM-powered multimodal AutoML framework for protein engineering, enabling biologists without deep learning expertise to interact with DL models using natural language.  AutoPE integrates LLMs with AutoML for model selection (sequence and graph modalities), hyperparameter optimization, and automated data retrieval, demonstrating significant performance improvements over traditional methods in two real-world protein engineering tasks. Code is available at:&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;GSCo: Generalist-Specialist AI Collaboration&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This paper introduces GSCo, a framework for medical image analysis combining Generalist Foundation Models (GFMs) and specialist models. It develops MedDr, the largest open-source medical GFM, and lightweight specialists for downstream tasks.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;SAM for Lung X-ray Segmentation&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This paper explores the application of Meta AI&amp;#39;s Segment Anything Model (SAM) to chest X-ray analysis for lung segmentation.  Using a transfer learning approach with fine-tuning, the study demonstrates improved performance compared to the original SAM, achieving results comparable to state-of-the-art models like U-Net.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;MEG: Knowledge-Enhanced Medical QA&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;This paper introduces MEG, a parameter-efficient method for augmenting Large Language Models (LLMs) with medical knowledge graphs using a lightweight mapping network.  Evaluated on four medical multiple-choice datasets, MEG achieves a 10.2% accuracy improvement over the Mistral-Instruct baseline and 6.7% over specialized models like BioMistral, demonstrating the benefit of knowledge graph integration.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Frameworks and Methodologies:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BrainSegFounder: 3D Neuroimage Analysis&lt;/li&gt;\n&lt;li&gt;PASSION: Sub-Saharan Dermatology Dataset&lt;/li&gt;\n&lt;li&gt;Label Critic: Data-First Approach&lt;/li&gt;\n&lt;li&gt;Medprompt Runtime Strategies&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Medical LLM Applications:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CataractBot: Patient Support System&lt;/li&gt;\n&lt;li&gt;CheX-GPT: X-ray Report Enhancement&lt;/li&gt;\n&lt;li&gt;CardioAI: Cancer Cardiotoxicity Monitor&lt;/li&gt;\n&lt;li&gt;HealthQ: Healthcare Conversation Chain&lt;/li&gt;\n&lt;li&gt;PRObot: Diabetic Retinopathy Assistant&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Medical LLMs &amp;amp; Benchmarks:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MediQ: Clinical Reasoning Benchmark&lt;/li&gt;\n&lt;li&gt;Touchstone: Segmentation Evaluation&lt;/li&gt;\n&lt;li&gt;Medical LLM Adaptation Progress&lt;/li&gt;\n&lt;li&gt;Fine-Tuning Medical QA Strategies&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;AI in Healthcare Ethics:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Healthcare Robotics with LLMs&lt;/li&gt;\n&lt;li&gt;XAI in Clinical Practice&lt;/li&gt;\n&lt;li&gt;Precision Rehabilitation Framework&lt;/li&gt;\n&lt;li&gt;Multimodal AI Challenges&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Full thread in detail : &lt;a href=\"https://x.com/OpenlifesciAI/status/1855207141302473090\"&gt;https://x.com/OpenlifesciAI/status/1855207141302473090&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn8wqp",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "aadityaura",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731154858.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello, everyone.  \nI wrote a small collection of boosting algorithms in Rust named [MiniBoosts](https://github.com/rmitsuboshi/miniboosts).\n\nThis is a hobby project, but I would like to improve more.  \nAny feedback is welcome.\n\nI appreciate your cooperation.",
            "author_fullname": "t2_4h79e6ta",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] MiniBoosts: A small collection of boosting algorithms",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gn8mpt",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 15,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 15,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731153817.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, everyone.&lt;br/&gt;\nI wrote a small collection of boosting algorithms in Rust named &lt;a href=\"https://github.com/rmitsuboshi/miniboosts\"&gt;MiniBoosts&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;This is a hobby project, but I would like to improve more.&lt;br/&gt;\nAny feedback is welcome.&lt;/p&gt;\n\n&lt;p&gt;I appreciate your cooperation.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?auto=webp&amp;s=2fb3eb7bb52240ebdce0bb4fbd2181a2809c2052",
                            "width": 1280,
                            "height": 640
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f88c3ed81bbf4f198b106d153cbc7014a5f1649c",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc81407ebefe883a3a651dc380e4dde84292d1ab",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=be30ceba67236d278aaf198a137440cb67e6f19a",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c45585fe91e0baacb00b8b65437fedf27069db6",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ea6b665faa37475978c03506fdd961213f49aa6",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/E2OGHg8KUpWpYZ4rAzctBewKLTaFs4MbywXgiz2Lges.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=63b6aa7359000167a7a3bed53090ede4d8f29cd2",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "-HuvqQ-HTU__XGuWayt8kvkCbuY2boPZhWaGsdHIzRs"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn8mpt",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "__leopardus__",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731153817.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "As title said I was wondering if there are some other ways to embedd corpus without using torch. One of the solution I came up with was by using ONNX. I created the images by using the fastembed library from Qdrant and the sentence-transformer library. Using fastembed result in a significant image size reduction.\n\n# Question:\n\nAre there other ways (for example modifying the dockerfile or using other libraries) to shrink the docker image even more?\n\npublic repo: [https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison](https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison)",
            "author_fullname": "t2_y8jr8mmod",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Embeddings and docker file - comparison between two libraries - Is there something better than ONNX? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gn87vi",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731152170.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As title said I was wondering if there are some other ways to embedd corpus without using torch. One of the solution I came up with was by using ONNX. I created the images by using the fastembed library from Qdrant and the sentence-transformer library. Using fastembed result in a significant image size reduction.&lt;/p&gt;\n\n&lt;h1&gt;Question:&lt;/h1&gt;\n\n&lt;p&gt;Are there other ways (for example modifying the dockerfile or using other libraries) to shrink the docker image even more?&lt;/p&gt;\n\n&lt;p&gt;public repo: &lt;a href=\"https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison\"&gt;https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?auto=webp&amp;s=74e3eb0080b4bd44970cfd73b610d6a0856dc290",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b72556c04b88a8cf8249f49a355f107411c5aa0",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8b8904b09d9384ff43b72ee8ea9807d66acc0d9",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d53ca4096a4b6d7c01569959dc9e3384f59d0a2f",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c90de5c88f4b57bda28cc8a3d4bff55a62493b49",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cee146661ee5bee1b20aede4828282f7eba83f2c",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/gtYldQR1VOoRznxBeBGH0Ud6gCSwr79syk1odJyrmoY.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c2b0d99f2801fd28a730885a3545b6c3a7e10ef",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "UAn03hI2vvqbwZmc0qjK0oorCJzCHPeAGhpgDtJCOXk"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn87vi",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Ambitious-Most4485",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731152170.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Seems an obvious question but such a \"data point\" would be very helpful to clear our ignorance.",
            "author_fullname": "t2_fhnhk",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Has anyone replaced Transformers with fully-connected layers and verified that it performs strictly worse (for training language models)?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gn6sam",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731145969.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems an obvious question but such a &amp;quot;data point&amp;quot; would be very helpful to clear our ignorance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn6sam",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Cybernetic1",
            "discussion_type": null,
            "num_comments": 24,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn6sam/d_has_anyone_replaced_transformers_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn6sam/d_has_anyone_replaced_transformers_with/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731145969.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey guys, wanted to get your feedback on a project I'm developing. I'm building a framework to define AI agents from YAML configuration files. These files encapsulate tasks that need to be done, how they connect etc, while all the rest is abstracted away.\n\nNow the idea is to use LLMs themselves to create those YAML files from a user prompt. Since the config file has all the core logic of the agent and removes all unnecessary details, I think this is the most efficient way to build a text-to-agent framework. Wdyt?\n\nLet me know your thoughts, and have a look at the repo [https://github.com/octopus2023-inc/gensphere](https://github.com/octopus2023-inc/gensphere)\n\nLet me know if you want to contribute and make it work.",
            "author_fullname": "t2_107tk82vkc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Open-Source Text-to-Agent : framework to develop AI agents from YAML files.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gn2e5w",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.64,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731127659.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, wanted to get your feedback on a project I&amp;#39;m developing. I&amp;#39;m building a framework to define AI agents from YAML configuration files. These files encapsulate tasks that need to be done, how they connect etc, while all the rest is abstracted away.&lt;/p&gt;\n\n&lt;p&gt;Now the idea is to use LLMs themselves to create those YAML files from a user prompt. Since the config file has all the core logic of the agent and removes all unnecessary details, I think this is the most efficient way to build a text-to-agent framework. Wdyt?&lt;/p&gt;\n\n&lt;p&gt;Let me know your thoughts, and have a look at the repo &lt;a href=\"https://github.com/octopus2023-inc/gensphere\"&gt;https://github.com/octopus2023-inc/gensphere&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you want to contribute and make it work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?auto=webp&amp;s=5e493cf54669f98e97bc030a75db5666684d1677",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a562425b9960a7d31fbe838e67dc2421845acd33",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=768121ee0a91e5aa6e96ce67d0b83f201682786f",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed47f3548f06daf554168475fe52cfa059043ffe",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a57a40e5cdbd39772cf20fce02efc6a25051e68",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e1bc8dffbd123f26a16ec506b37044833bf6f923",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/ngPd2mFeM2EGV2iPVLsRfQY_6AyB5ytz2NjP4Xm6hcA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ee2dfe187f8309763eb043058326feb72540b220",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "W-1ibqsrw9sq2dESscmennFG0YXNtj_ofc4MmE9mM8Q"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn2e5w",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Jazzlike_Tooth929",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731127659.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "i was looking into the research papers published in PAKDD 2023. From the names of the authors, I can guess that they are Chinese, Korean, or Japanese\n\nI know PAKDD is a double-blind review. But why other people don't submit their work? or if they submit why the number of acceptance is low\n\nI am also Asian, so I am not trying to be racist here. Just wondering why it is like that",
            "author_fullname": "t2_8kbz25gf",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] PAKDD 2023 data? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gn282v",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731127043.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i was looking into the research papers published in PAKDD 2023. From the names of the authors, I can guess that they are Chinese, Korean, or Japanese&lt;/p&gt;\n\n&lt;p&gt;I know PAKDD is a double-blind review. But why other people don&amp;#39;t submit their work? or if they submit why the number of acceptance is low&lt;/p&gt;\n\n&lt;p&gt;I am also Asian, so I am not trying to be racist here. Just wondering why it is like that&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gn282v",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Alarming-Camera-188",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731127043.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "My job’s looking for a way for ai to help generate plans, I really think a simple multi-variable model should do the trick; just need to find a reliable hosting service that can be built upon however needed. Are there well established ML hosters that are scalable, configurable, all that?",
            "author_fullname": "t2_wdeuv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Simple ML model hosting service?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmzb3q",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 14,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 14,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731117343.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My job’s looking for a way for ai to help generate plans, I really think a simple multi-variable model should do the trick; just need to find a reliable hosting service that can be built upon however needed. Are there well established ML hosters that are scalable, configurable, all that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmzb3q",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Lucrayzor",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731117343.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Dear Colleagues\n\nTime Series Anomaly Detection (TSAD) is hot right now, with dozens of  papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.\n\nHowever, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.\n\nI have made two 90-second-long videos that make this clear in a visual and intuitive way:\n\n 1)      Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)\n\n[https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;ab\\_channel=EamonnKeogh](https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;ab_channel=EamonnKeogh)\n\n  2)      Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)\n\n[https://www.youtube.com/watch?v=3gH-65RCBDs&amp;ab\\_channel=EamonnKeogh](https://www.youtube.com/watch?v=3gH-65RCBDs&amp;ab_channel=EamonnKeogh)\n\nAs always, corrections and comments welcome.\n\nEamonn\n\n EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)\n\n  \n\n\nFor a review of most of the commonly used TSAD datasets, see this file:\n\n[https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;dl=0](https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;dl=0)",
            "author_fullname": "t2_i3yjq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Most Time Series Anomaly Detection results are meaningless (two short videos explain why)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmwxnr",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 109,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 109,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731119012.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731110299.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dear Colleagues&lt;/p&gt;\n\n&lt;p&gt;Time Series Anomaly Detection (TSAD) is hot right now, with dozens of  papers each year in NeurIPS, SIGKDD, ICML, PVLDB etc.&lt;/p&gt;\n\n&lt;p&gt;However, I claim that much of the published results are meaningless, because the uncertainty of the ground truth labels dwarfs any claimed differences between algorithms or amount of claimed improvements.&lt;/p&gt;\n\n&lt;p&gt;I have made two 90-second-long videos that make this clear in a visual and intuitive way:&lt;/p&gt;\n\n&lt;p&gt; 1)      Why Most Time Series Anomaly Detection Results are Meaningless (Dodgers)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;amp;ab_channel=EamonnKeogh\"&gt;https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;amp;ab_channel=EamonnKeogh&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;  2)      Why Most Time Series Anomaly Detection Results are Meaningless (AnnGun)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=3gH-65RCBDs&amp;amp;ab_channel=EamonnKeogh\"&gt;https://www.youtube.com/watch?v=3gH-65RCBDs&amp;amp;ab_channel=EamonnKeogh&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As always, corrections and comments welcome.&lt;/p&gt;\n\n&lt;p&gt;Eamonn&lt;/p&gt;\n\n&lt;p&gt; EDIT: To be clear, my point is simply to prevent others from wasting time working with datasets with essentially random labels. In addition, we should be cautious of any claims in the literature that are based on such data (and that includes at least dozens of highly cited papers)&lt;/p&gt;\n\n&lt;p&gt;For a review of most of the commonly used TSAD datasets, see this file:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;amp;dl=0\"&gt;https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;amp;dl=0&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/xiOr5ECbkeOL4AxfwEThOZy8TIny23I_UzMYHe-ZoSE.jpg?auto=webp&amp;s=818195d9b5eed5d2edd56c06b6e74cfe90334c06",
                            "width": 480,
                            "height": 360
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/xiOr5ECbkeOL4AxfwEThOZy8TIny23I_UzMYHe-ZoSE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9485a45a716c4f8b0a37b7208c57e820e3a9b52c",
                                "width": 108,
                                "height": 81
                            },
                            {
                                "url": "https://external-preview.redd.it/xiOr5ECbkeOL4AxfwEThOZy8TIny23I_UzMYHe-ZoSE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=873daf1655321fa1fdeb11df01c8a0b5dde10b9c",
                                "width": 216,
                                "height": 162
                            },
                            {
                                "url": "https://external-preview.redd.it/xiOr5ECbkeOL4AxfwEThOZy8TIny23I_UzMYHe-ZoSE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=03ebd17119dac9366ce4b9512ac0bd0ea2f09633",
                                "width": 320,
                                "height": 240
                            }
                        ],
                        "variants": {},
                        "id": "u3vbibsWs9Kaa4H2Ih0kiie722I4MAznsxINBu4b0LY"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmwxnr",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "eamonnkeogh",
            "discussion_type": null,
            "num_comments": 60,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731110299.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is. Ever since the advent of the current era of ai/llms, I thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely - while still being very true to the style and layout/ethos of the worlds/levels from the reference game. I feel like it would just be so wonderful if there was a path to creating some type of 'never-ending' &lt;insert nostalgic game here&gt; instead of being limited to what the devs put out back in the day.\n\nIf anyone has any insight or thoughts on this, please let me know :). I work in the AI space, but I integrate the models, and don't do any training or anything on the low level ML side. Also, yes, I'm only think about the gameworlds/levels atm.",
            "author_fullname": "t2_8hxi9k6o",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] AI-Generated gameworlds based on classic games? (Ex - Spyro)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmspmt",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731098968.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was wondering if anyone had any thoughts on how far out something like this might be or how difficult this is. Ever since the advent of the current era of ai/llms, I thought it would be great to somehow be able to feed data from nostalgic games in some form and create some type of system that is able to generate these worlds infinitely - while still being very true to the style and layout/ethos of the worlds/levels from the reference game. I feel like it would just be so wonderful if there was a path to creating some type of &amp;#39;never-ending&amp;#39; &amp;lt;insert nostalgic game here&amp;gt; instead of being limited to what the devs put out back in the day.&lt;/p&gt;\n\n&lt;p&gt;If anyone has any insight or thoughts on this, please let me know :). I work in the AI space, but I integrate the models, and don&amp;#39;t do any training or anything on the low level ML side. Also, yes, I&amp;#39;m only think about the gameworlds/levels atm.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmspmt",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "cobalt1137",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmspmt/d_aigenerated_gameworlds_based_on_classic_games/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmspmt/d_aigenerated_gameworlds_based_on_classic_games/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731098968.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Lets say we have a dataset that is much larger than we have disk storage. For example:\n\n* Dataset: 1PB\n* Our disk storage: 10TB\n* GPU RAM: 8x80GB (not super relevant to this discussion)\n\nWhat are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:\n\n\\- prefetch block n, train on block n-1, delete block n-2 from disk\n\nLets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?\n\nTo take a step back, I'm assuming that this is ordinary and often occuring \"problem\" for every company that trains on large datasets, so I'm skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can't really find anything that matches perfectly.",
            "author_fullname": "t2_33835h5y",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Training on Petabyte scale datasets",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmpedb",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 41,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 41,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731090634.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731090426.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Lets say we have a dataset that is much larger than we have disk storage. For example:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Dataset: 1PB&lt;/li&gt;\n&lt;li&gt;Our disk storage: 10TB&lt;/li&gt;\n&lt;li&gt;GPU RAM: 8x80GB (not super relevant to this discussion)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What are the usual approaches to training on something like this? What I can think of intuitively is to do the following in parallel somehow:&lt;/p&gt;\n\n&lt;p&gt;- prefetch block n, train on block n-1, delete block n-2 from disk&lt;/p&gt;\n\n&lt;p&gt;Lets say we use PyTorch, so we have a PyTorch Dataset that has all the paths to where the data is stored in the cloud. Do we need to write code for the prefetcher/deleter that downloads from the cloud and store on disk and have it run in a separate process, then have a DataLoader for training that just assumes that it can read from disk (because the prefetcher does its job correctly)? Having the DataLoader read from S3 would be bad for GPU utilization, right?&lt;/p&gt;\n\n&lt;p&gt;To take a step back, I&amp;#39;m assuming that this is ordinary and often occuring &amp;quot;problem&amp;quot; for every company that trains on large datasets, so I&amp;#39;m skeptical to writing all of this code by myself as I feel like there should be standard out of the box solutions for this, but can&amp;#39;t really find anything that matches perfectly.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmpedb",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "lapurita",
            "discussion_type": null,
            "num_comments": 29,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731090426.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi all, I have a few GPUs left over from mining, and I’m interested in starting a small-scale GPU-as-a-service. My goal is to set up a simple, side income that could help pay off my credit cards, as I already have a primary job.\n\nWhat steps are needed for getting started with a small-scale GPU-as-a-service business focused on machine learning or AI? Any insights would be greatly appreciated!\n\nThanks in advance for any advice you can share!",
            "author_fullname": "t2_9hbrmw2q",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] GPU as a service",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmofpq",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731087996.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I have a few GPUs left over from mining, and I’m interested in starting a small-scale GPU-as-a-service. My goal is to set up a simple, side income that could help pay off my credit cards, as I already have a primary job.&lt;/p&gt;\n\n&lt;p&gt;What steps are needed for getting started with a small-scale GPU-as-a-service business focused on machine learning or AI? Any insights would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any advice you can share!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmofpq",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "chazzyfe",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmofpq/r_gpu_as_a_service/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmofpq/r_gpu_as_a_service/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731087996.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Context: I was making what was supposed to be an FP-oriented NN library/framwork on top of JAX (which too was FP-oriented) called z-zephyr on pip. However, I noticed something you could do with it that kinda clunky, if not tedious, with other frameworks. \n\n(please read context)\n\nTLDR; Zephyr turns out to be very good way (at least in my experience) to make structures that are weird. And I recently just added update capabilities so that zephyr doesn't only do structures but updates too.\n\nDisclaimer: You can this with other frameworks, I have tried many of things I will tell below in other frameworks or libraries, and it's just painful for me or i'm just inexperienced with those. \n\nHere are the crazy things that's quick to do in zephyr, that might not be as quick in other frameworks (if it could be done easily in other frameworks more easily, please tell me).\n\n(These are not supposed to be useful, they're supposed to be extreme)\n\n### Full Binary Tree as Neural Network\n- edges have an associated weight\n- input is a scalar (could be a batch with JAX vmap, but let's consider 1)\n- output an array of shape (2^n,) where n is the depth of the tree\n- an update rule that takes into account if the weight is a {L}eft or {R}ight branch (i'll keep it simple, but it can easily be anything)\n\nHere is the tree network in zephyr, and how you get the initial params and tags (tag, is the key in params[key]).\n```python\n    # essentially 4 lines of code\n    @flexible\n    def tree_net(params, x, n, i=0):\n        if i == n-1:\n            return [x]\n        return (\n            tree_net(\n                params[\"branch\"][\"L\"] if i !=n-2 else params, \n                validate(params[\"weight\"][\"L\"], (1,), uniform) * x, \n                n, \n                i+1) + \n            tree_net(\n                params[\"branch\"][\"R\"] if i !=n-2 else params, \n                validate(params[\"weight\"][\"R\"], (1,), uniform) * x, \n                n, \n                i+1)\n        )\n\n    x = jnp.ones((1,)) # dummy\n    N = 4\n    params = trace(tree_net, key, x, N)\n    tags = get_lineage_tags(params)\n```\n\nassume you had the loss function and gradients and what not, to keep it simple, i'll just update so that the left branch have weights 0, and the rights ones are kept the same. \n\n```python\n    def make_left_zero(params, tags): # i left out gradients \n        if tags[-1] == \"L\":\n            return params * 0\n        \n        return params\n\n    # update the params \n    params = apply_updates(make_left_zero, params, tags)\n```\n\n### Other things you could do with zephyr now (I have tried, and the code is easy for me to do and i'm not that great of a coder)\n- multi-layer network and use the depth of the network (via a tag) to calculate updates of parameters\n- tag some weights as \"fast\" or \"slow\" and use those tags in updating\n- create an MLP with neurons as Wx+b. Notice that the neuron is a function that is Array -&gt; Scalar. So I could replace each neuron in that MLP, with another MLP whose output is a scalar (array of shape (1,) ). Or replace the neurons in that with any neural network (any function) that is Array -&gt; Scalar. \n\n---\n\n\n### What architectures/structures with custom updates rules can you think of that are easy to write(pseudo-code/math or description) but possible cumbersome to implement right now?\n\nPlease suggest some extreme idea for me to try. \n\nI think zephyr could be the tooling to make those easy to do. I would like to hear your extreme ideas, so I can try to code them zephyr, and if i can't do it without strugling, and if it's something i think is generic enough, I will evolve zephyr to handle it more easily.\n\nPS: The readme doesn't include these yet, since it started as an (normal) NN library.\n\nThe link of the repo will be in the comments if you want to check it out.",
            "author_fullname": "t2_1axtwmeqmk",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What are crazy structures or update rule that might be useful(or not)? Extreme ideas are welcome",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmm7mi",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.73,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731086811.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731082452.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Context: I was making what was supposed to be an FP-oriented NN library/framwork on top of JAX (which too was FP-oriented) called z-zephyr on pip. However, I noticed something you could do with it that kinda clunky, if not tedious, with other frameworks. &lt;/p&gt;\n\n&lt;p&gt;(please read context)&lt;/p&gt;\n\n&lt;p&gt;TLDR; Zephyr turns out to be very good way (at least in my experience) to make structures that are weird. And I recently just added update capabilities so that zephyr doesn&amp;#39;t only do structures but updates too.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: You can this with other frameworks, I have tried many of things I will tell below in other frameworks or libraries, and it&amp;#39;s just painful for me or i&amp;#39;m just inexperienced with those. &lt;/p&gt;\n\n&lt;p&gt;Here are the crazy things that&amp;#39;s quick to do in zephyr, that might not be as quick in other frameworks (if it could be done easily in other frameworks more easily, please tell me).&lt;/p&gt;\n\n&lt;p&gt;(These are not supposed to be useful, they&amp;#39;re supposed to be extreme)&lt;/p&gt;\n\n&lt;h3&gt;Full Binary Tree as Neural Network&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;edges have an associated weight&lt;/li&gt;\n&lt;li&gt;input is a scalar (could be a batch with JAX vmap, but let&amp;#39;s consider 1)&lt;/li&gt;\n&lt;li&gt;output an array of shape (2&lt;sup&gt;n,)&lt;/sup&gt; where n is the depth of the tree&lt;/li&gt;\n&lt;li&gt;an update rule that takes into account if the weight is a {L}eft or {R}ight branch (i&amp;#39;ll keep it simple, but it can easily be anything)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here is the tree network in zephyr, and how you get the initial params and tags (tag, is the key in params[key]).\n```python\n    # essentially 4 lines of code\n    @flexible\n    def tree_net(params, x, n, i=0):\n        if i == n-1:\n            return [x]\n        return (\n            tree_net(\n                params[&amp;quot;branch&amp;quot;][&amp;quot;L&amp;quot;] if i !=n-2 else params, \n                validate(params[&amp;quot;weight&amp;quot;][&amp;quot;L&amp;quot;], (1,), uniform) * x, \n                n, \n                i+1) + \n            tree_net(\n                params[&amp;quot;branch&amp;quot;][&amp;quot;R&amp;quot;] if i !=n-2 else params, \n                validate(params[&amp;quot;weight&amp;quot;][&amp;quot;R&amp;quot;], (1,), uniform) * x, \n                n, \n                i+1)\n        )&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;x = jnp.ones((1,)) # dummy\nN = 4\nparams = trace(tree_net, key, x, N)\ntags = get_lineage_tags(params)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;assume you had the loss function and gradients and what not, to keep it simple, i&amp;#39;ll just update so that the left branch have weights 0, and the rights ones are kept the same. &lt;/p&gt;\n\n&lt;p&gt;```python\n    def make_left_zero(params, tags): # i left out gradients \n        if tags[-1] == &amp;quot;L&amp;quot;:\n            return params * 0&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    return params\n\n# update the params \nparams = apply_updates(make_left_zero, params, tags)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h3&gt;Other things you could do with zephyr now (I have tried, and the code is easy for me to do and i&amp;#39;m not that great of a coder)&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;multi-layer network and use the depth of the network (via a tag) to calculate updates of parameters&lt;/li&gt;\n&lt;li&gt;tag some weights as &amp;quot;fast&amp;quot; or &amp;quot;slow&amp;quot; and use those tags in updating&lt;/li&gt;\n&lt;li&gt;create an MLP with neurons as Wx+b. Notice that the neuron is a function that is Array -&amp;gt; Scalar. So I could replace each neuron in that MLP, with another MLP whose output is a scalar (array of shape (1,) ). Or replace the neurons in that with any neural network (any function) that is Array -&amp;gt; Scalar. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;What architectures/structures with custom updates rules can you think of that are easy to write(pseudo-code/math or description) but possible cumbersome to implement right now?&lt;/h3&gt;\n\n&lt;p&gt;Please suggest some extreme idea for me to try. &lt;/p&gt;\n\n&lt;p&gt;I think zephyr could be the tooling to make those easy to do. I would like to hear your extreme ideas, so I can try to code them zephyr, and if i can&amp;#39;t do it without strugling, and if it&amp;#39;s something i think is generic enough, I will evolve zephyr to handle it more easily.&lt;/p&gt;\n\n&lt;p&gt;PS: The readme doesn&amp;#39;t include these yet, since it started as an (normal) NN library.&lt;/p&gt;\n\n&lt;p&gt;The link of the repo will be in the comments if you want to check it out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmm7mi",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Pristine-Staff-5250",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmm7mi/d_what_are_crazy_structures_or_update_rule_that/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmm7mi/d_what_are_crazy_structures_or_update_rule_that/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731082452.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone. I built Grada to learn how things work under the hood. It’s an interactive browser tool that lets you observe real-time changes while training a multilayer perceptron, all built from scratch with a custom tensor-based engine.\n\nYou can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time. Grada also includes a handwritten digit recognition feature, letting you interactively test your model by drawing digits and visualizing predictions. It might be a useful educational tool.\n\nYou can find the source code and a quick demo gif on GitHub at [https://github.com/saliherdemk/Grada](https://github.com/saliherdemk/Grada), and the live demo is available at [https://saliherdemk.github.io/Grada/](https://saliherdemk.github.io/Grada/).\n\nHope this helps and looking forward to hearing some feedback.",
            "author_fullname": "t2_fl1xvchtb",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Build MLPs with Drag-and-Drop and Observe Real-Time Changes While Training in Browser ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmdrcd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731053742.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone. I built Grada to learn how things work under the hood. It’s an interactive browser tool that lets you observe real-time changes while training a multilayer perceptron, all built from scratch with a custom tensor-based engine.&lt;/p&gt;\n\n&lt;p&gt;You can easily construct neural networks with drag and drop and watch how training affects parameters and outputs visually in real time. Grada also includes a handwritten digit recognition feature, letting you interactively test your model by drawing digits and visualizing predictions. It might be a useful educational tool.&lt;/p&gt;\n\n&lt;p&gt;You can find the source code and a quick demo gif on GitHub at &lt;a href=\"https://github.com/saliherdemk/Grada\"&gt;https://github.com/saliherdemk/Grada&lt;/a&gt;, and the live demo is available at &lt;a href=\"https://saliherdemk.github.io/Grada/\"&gt;https://saliherdemk.github.io/Grada/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Hope this helps and looking forward to hearing some feedback.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?auto=webp&amp;s=2fa888c80c314ec4d0ea8016ffbdc23d75a5ed9b",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6591301c7b712c0459da098bef5f536409a75db7",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8a039865f6870dcf57f42762f5eb508c8bf34e14",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=08531a12345abba207a5607c24bd1eba25b36194",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b4c12ce05f2670f1cf305d25a47c38151e59d2ac",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6943d19a81507b5b9c8a07db1d798647ea7c9b42",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/Du42BubBqo_W8LqJGKF6VOxY5YwDV767E6LDs_IsAZ4.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b442abe81cf7ddff985ae1e7e32818f5bef9b043",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "ZYRkx_Y5JZLZX3kFm6EmhrauRFxXhc361JpZw82hnEs"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmdrcd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "saliherdemk",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmdrcd/p_build_mlps_with_draganddrop_and_observe/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmdrcd/p_build_mlps_with_draganddrop_and_observe/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731053742.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "**Benchmarking Large Language Models with Integer Sequence Generation Tasks**  \nDaniel O'Malley, Manish Bhattarai, Javier Santos - Los Alamos National Laboratory  \nThis paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.  \narXiv:2411.04372 \\[cs.LG\\]: [https://arxiv.org/abs/2411.04372](https://arxiv.org/abs/2411.04372)\n\nhttps://preview.redd.it/4vvh5s21unzd1.jpg?width=588&amp;format=pjpg&amp;auto=webp&amp;s=c8bece31712d5d6378188c88e14b9f56e477d41f\n\n",
            "author_fullname": "t2_agjaq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Benchmarking Large Language Models with Integer Sequence Generation Tasks",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 112,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "4vvh5s21unzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/jpg",
                    "p": [
                        {
                            "y": 86,
                            "x": 108,
                            "u": "https://preview.redd.it/4vvh5s21unzd1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c267e96ff31718b89481c4b3f81f82c4649a108c"
                        },
                        {
                            "y": 173,
                            "x": 216,
                            "u": "https://preview.redd.it/4vvh5s21unzd1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf813612fdee59d50e54384509532a1278123f3b"
                        },
                        {
                            "y": 257,
                            "x": 320,
                            "u": "https://preview.redd.it/4vvh5s21unzd1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3e633490267e1829528f11deb8f68449ff828b55"
                        }
                    ],
                    "s": {
                        "y": 473,
                        "x": 588,
                        "u": "https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&amp;format=pjpg&amp;auto=webp&amp;s=c8bece31712d5d6378188c88e14b9f56e477d41f"
                    },
                    "id": "4vvh5s21unzd1"
                }
            },
            "name": "t3_1gmg2dl",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.77,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "https://b.thumbs.redditmedia.com/iNeKFBb5JNY11CkpzPwteFKRlXGtnL5Vv0CzvmBkPNg.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731064030.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Benchmarking Large Language Models with Integer Sequence Generation Tasks&lt;/strong&gt;&lt;br/&gt;\nDaniel O&amp;#39;Malley, Manish Bhattarai, Javier Santos - Los Alamos National Laboratory&lt;br/&gt;\nThis paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.&lt;br/&gt;\narXiv:2411.04372 [cs.LG]: &lt;a href=\"https://arxiv.org/abs/2411.04372\"&gt;https://arxiv.org/abs/2411.04372&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c8bece31712d5d6378188c88e14b9f56e477d41f\"&gt;https://preview.redd.it/4vvh5s21unzd1.jpg?width=588&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c8bece31712d5d6378188c88e14b9f56e477d41f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmg2dl",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Nunki08",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmg2dl/r_benchmarking_large_language_models_with_integer/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmg2dl/r_benchmarking_large_language_models_with_integer/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731064030.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi. I'm trying to use ML/DL model for predicting variability statistics like min, max, avg, var, with several features same as target.\n\nFor example, \n- Input: \n  - min, max, average, variance for the number of customer arrivals in a day\n  - min, max, average, variance for the number of customer departures in a day\n- Output:\n  - min, max, average, variance for the number of waiting customers in a day\n\nI find several papers related to interval or range prediction for various area like wind power, stock price or solar energy, but I think those papers are different to my purpose. Almost every papers are predicting specific constant value based on time series data first, and use statistical method to estimate prediction interval.\n\nI'm trying to find a way for prediction variability of target value with variability of features. My best idea is make each model to predict each statistics, like one model for minimum, other model for average, ... But I think there is a better way to do this. Is there any ML/DL model or other technique/methodology for this purpose?",
            "author_fullname": "t2_6flwzduw",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] prediction variability for target with statistics for features",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmezw8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.76,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731059483.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I&amp;#39;m trying to use ML/DL model for predicting variability statistics like min, max, avg, var, with several features same as target.&lt;/p&gt;\n\n&lt;p&gt;For example, \n- Input: \n  - min, max, average, variance for the number of customer arrivals in a day\n  - min, max, average, variance for the number of customer departures in a day\n- Output:\n  - min, max, average, variance for the number of waiting customers in a day&lt;/p&gt;\n\n&lt;p&gt;I find several papers related to interval or range prediction for various area like wind power, stock price or solar energy, but I think those papers are different to my purpose. Almost every papers are predicting specific constant value based on time series data first, and use statistical method to estimate prediction interval.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to find a way for prediction variability of target value with variability of features. My best idea is make each model to predict each statistics, like one model for minimum, other model for average, ... But I think there is a better way to do this. Is there any ML/DL model or other technique/methodology for this purpose?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmezw8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "caution721",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmezw8/d_prediction_variability_for_target_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmezw8/d_prediction_variability_for_target_with/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731059483.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey everyone! I’m working on my final-year project for my Bachelor’s, where I’m trying to predict Autism Spectrum Disorder (ASD) using voice cues. I’ve worked on some basic ML projects and CNNs before, but this is my first time dealing with audio data, and I’ll be collecting samples from young kids with ASD, from toddlers up to age 12.\n\nI could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio. Also, if anyone knows of any open datasets in this area (I haven’t had much luck there) or has any advice or resources, I’d be super grateful. Thanks a ton in advance!",
            "author_fullname": "t2_o7rmy3a7h",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Looking for Advice &amp; Resources on ASD Prediction Using Voice Cues",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmcqp0",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731049170.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! I’m working on my final-year project for my Bachelor’s, where I’m trying to predict Autism Spectrum Disorder (ASD) using voice cues. I’ve worked on some basic ML projects and CNNs before, but this is my first time dealing with audio data, and I’ll be collecting samples from young kids with ASD, from toddlers up to age 12.&lt;/p&gt;\n\n&lt;p&gt;I could really use some help finding resources to get a solid grasp on signal processing and how to train classification models specifically on audio. Also, if anyone knows of any open datasets in this area (I haven’t had much luck there) or has any advice or resources, I’d be super grateful. Thanks a ton in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmcqp0",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "General-Ad6585",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmcqp0/d_looking_for_advice_resources_on_asd_prediction/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmcqp0/d_looking_for_advice_resources_on_asd_prediction/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731049170.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello, our company recently decided to expand our ML team from a very small 2 person team to a more serious efforts.\n\nWhen we were small, we really didnt have a way to manage data sets or evaluations. They were just files checked into a github repo.\n\nBut increasingly we find, with multiple ML models (some llm and some not), and many iterations of datasets (some experimental and some not). It's really hard to version them in a meaningful way and be able to compare and analyze them.\n\nWe are a large company, so cost is not really an issue. And all our infrastructure is hosted in Azure. If anything, they fear lock in. What is the best platform/tools for this kind of usage?",
            "author_fullname": "t2_ftobu",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What tools do you recommend to manage ML data sets and evaluations? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmbcwk",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731047200.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731043706.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello, our company recently decided to expand our ML team from a very small 2 person team to a more serious efforts.&lt;/p&gt;\n\n&lt;p&gt;When we were small, we really didnt have a way to manage data sets or evaluations. They were just files checked into a github repo.&lt;/p&gt;\n\n&lt;p&gt;But increasingly we find, with multiple ML models (some llm and some not), and many iterations of datasets (some experimental and some not). It&amp;#39;s really hard to version them in a meaningful way and be able to compare and analyze them.&lt;/p&gt;\n\n&lt;p&gt;We are a large company, so cost is not really an issue. And all our infrastructure is hosted in Azure. If anything, they fear lock in. What is the best platform/tools for this kind of usage?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmbcwk",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "yalag",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmbcwk/d_what_tools_do_you_recommend_to_manage_ml_data/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmbcwk/d_what_tools_do_you_recommend_to_manage_ml_data/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731043706.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Almost all the papers I have read on DTI do something like this.  \n1. Generates target embeddings using PLMs like ESM2  \n2. Generates drug embeddings using CLMs like ChemBERTa  \n3. Uses a late fusion or some kind of cross modal attention mechanism.  \nHow to do things differently? Can we use something like docking scores as cross modal attention bias?",
            "author_fullname": "t2_qmx16mtd7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Directions on drug-target interaction prediction",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gmbcf8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731043656.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Almost all the papers I have read on DTI do something like this.&lt;br/&gt;\n1. Generates target embeddings using PLMs like ESM2&lt;br/&gt;\n2. Generates drug embeddings using CLMs like ChemBERTa&lt;br/&gt;\n3. Uses a late fusion or some kind of cross modal attention mechanism.&lt;br/&gt;\nHow to do things differently? Can we use something like docking scores as cross modal attention bias?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gmbcf8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Remote_Status_1612",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gmbcf8/d_directions_on_drugtarget_interaction_prediction/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gmbcf8/d_directions_on_drugtarget_interaction_prediction/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731043656.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm trying a new cute architecture on a bunch of the default datasets out there, using Jax since I'm doing live brain surgery, that part works well.\n\nWhat I'm having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I've not used TF since the 2.0 days and everything seems broken? I'm getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.\n\nI can't just ignore a whole bunch of errors and warnings when I'm trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? \n\n[0] https://www.tensorflow.org/datasets/overview",
            "author_fullname": "t2_18lx46bbfu",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Just how bad is tfds code quality?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gm96yo",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 48,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 48,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731036779.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731036303.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying a new cute architecture on a bunch of the default datasets out there, using Jax since I&amp;#39;m doing live brain surgery, that part works well.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m having a hell of a time with is actually loading the data. I was going for tfds since its 1) old 2) used in production 3) has a million datasets already prepared. I&amp;#39;ve not used TF since the 2.0 days and everything seems broken? I&amp;#39;m getting warnings and errors whenever I try loading and running through any dataset. Even their documentation has the errors [0] in the tutorial notebooks.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t just ignore a whole bunch of errors and warnings when I&amp;#39;m trying to benchmark a new architecture. Is tfds just that bad or am I missing something obvious? &lt;/p&gt;\n\n&lt;p&gt;[0] &lt;a href=\"https://www.tensorflow.org/datasets/overview\"&gt;https://www.tensorflow.org/datasets/overview&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?auto=webp&amp;s=e6806218662ba96a0cc095876fcc2f4697c22c53",
                            "width": 1200,
                            "height": 675
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b78f100e0ca222937d56063e15755bc3c4e80c1c",
                                "width": 108,
                                "height": 60
                            },
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a867810e68e9a07775b3686ecba148df702940ea",
                                "width": 216,
                                "height": 121
                            },
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7680e2718299ce4421d7f7104a5b6eb63540ce4",
                                "width": 320,
                                "height": 180
                            },
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c342bf4aae296befcd2647cb0fd96da0852bd06",
                                "width": 640,
                                "height": 360
                            },
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=621abc0069316fe6f472b77673b3007342fbb8f5",
                                "width": 960,
                                "height": 540
                            },
                            {
                                "url": "https://external-preview.redd.it/2nhzAkT-7AmTOMlhUvpmhitXIqbT7UzlAv0jDzTotkQ.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=41b0c17ea139bd011fb816fd13273f41c0188c88",
                                "width": 1080,
                                "height": 607
                            }
                        ],
                        "variants": {},
                        "id": "AcUXdF9TnoVQpyFqzaP15GyAh5CohX60bP3IdtTTr7E"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gm96yo",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "acc_agg",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gm96yo/d_just_how_bad_is_tfds_code_quality/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gm96yo/d_just_how_bad_is_tfds_code_quality/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731036303.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "For general prototyping purposes, I don't want to have to train or deploy a model, I just want it behind a service already and to provide it with necessary inputs in the request.... what do you guys think?\n\nEDIT: I suppose for more classical ML tasks, there's no real concept of \"pre-trained\" in the first place, so you can't just get inference for free... does that sound roughly true?",
            "author_fullname": "t2_48xrioldg",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] If I just want an inference engine for any given ML task that gives relatively SOTA results, is there anything better than Hugging Face?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gm6cba",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.2,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731030465.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731027581.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For general prototyping purposes, I don&amp;#39;t want to have to train or deploy a model, I just want it behind a service already and to provide it with necessary inputs in the request.... what do you guys think?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I suppose for more classical ML tasks, there&amp;#39;s no real concept of &amp;quot;pre-trained&amp;quot; in the first place, so you can&amp;#39;t just get inference for free... does that sound roughly true?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gm6cba",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BikeFun6408",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gm6cba/d_if_i_just_want_an_inference_engine_for_any/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gm6cba/d_if_i_just_want_an_inference_engine_for_any/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731027581.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_b01m1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] State-space models can learn in-context by gradient descent",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glxr2v",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.97,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 28,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 28,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1731005057.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arxiv.org",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arxiv.org/abs/2410.11687",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glxr2v",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "anandtrex",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glxr2v/r_statespace_models_can_learn_incontext_by/",
            "stickied": false,
            "url": "https://arxiv.org/abs/2410.11687",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731005057.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "[https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)\n\nShows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.\n\nThis could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.\n\nhttps://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&amp;format=pjpg&amp;auto=webp&amp;s=c6f753956e531303f7818de2c5aa5b5b94d9c2da\n\n**Abstract:**\n\n&gt;The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30,000 to ≈1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. \\~10% of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.\n\nPaper: [https://arxiv.org/abs/2411.02780](https://arxiv.org/abs/2411.02780)\n\nCode: [https://github.com/giannisdaras/ambient-laws](https://github.com/giannisdaras/ambient-laws)\n\nHuggingface models: [https://huggingface.co/giannisdaras?search\\_models=ambient\\_laws](https://huggingface.co/giannisdaras?search_models=ambient_laws)",
            "author_fullname": "t2_cshz30kk",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R]: How much is a noisy image worth? 👀",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 82,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "8vk1nwfexizd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/jpg",
                    "p": [
                        {
                            "y": 63,
                            "x": 108,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2498a7dcdd52e54fb67ecf1ea32b06c00dab91d8"
                        },
                        {
                            "y": 127,
                            "x": 216,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f4d0ccda9e44c65cda1fb272bbe7a923fc7ac5d2"
                        },
                        {
                            "y": 188,
                            "x": 320,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8fca9f04f1c95585788dcb054e99f31c016749a"
                        },
                        {
                            "y": 376,
                            "x": 640,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=57c85dbc167bd045c5af9f5c3eabe397ccb87373"
                        },
                        {
                            "y": 564,
                            "x": 960,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=418210ccfb619278b0d92f4665d939283752f079"
                        },
                        {
                            "y": 635,
                            "x": 1080,
                            "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ede27a36dd60a6408a295b48ebbf6c38057b221d"
                        }
                    ],
                    "s": {
                        "y": 1736,
                        "x": 2952,
                        "u": "https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&amp;format=pjpg&amp;auto=webp&amp;s=c6f753956e531303f7818de2c5aa5b5b94d9c2da"
                    },
                    "id": "8vk1nwfexizd1"
                }
            },
            "name": "t3_1glxhj9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 48,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 48,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/C9XpXBhkVG9JD4xrr8im867WqewZcXWMgtPr8BGBJGo.jpg",
            "edited": 1731004610.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1731004407.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/abs/2411.02780\"&gt;https://arxiv.org/abs/2411.02780&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Shows that corrupted images can be almost as useful as clean images for training generative models, assuming that a small initial set of clean images is available.&lt;/p&gt;\n\n&lt;p&gt;This could be useful for dataset design/curation: some budget needs to be invested in obtaining a few high-quality samples and then for the rest of the dataset corrupted images should work fine.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c6f753956e531303f7818de2c5aa5b5b94d9c2da\"&gt;https://preview.redd.it/8vk1nwfexizd1.jpg?width=2952&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c6f753956e531303f7818de2c5aa5b5b94d9c2da&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g. in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than 80 models on data with different corruption levels across three datasets ranging from 30,000 to ≈1.3M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g. ~10% of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse than that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2411.02780\"&gt;https://arxiv.org/abs/2411.02780&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/giannisdaras/ambient-laws\"&gt;https://github.com/giannisdaras/ambient-laws&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Huggingface models: &lt;a href=\"https://huggingface.co/giannisdaras?search_models=ambient_laws\"&gt;https://huggingface.co/giannisdaras?search_models=ambient_laws&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glxhj9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Constant_Club_9926",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glxhj9/r_how_much_is_a_noisy_image_worth/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glxhj9/r_how_much_is_a_noisy_image_worth/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1731004407.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Tokenizers are key to successful development of image and video generative models or multimodal LLMs. Compared to generative models, they are underrated. This work presents many tokenizers that are causal supporting both images and videos in both continuous (relevant in diffusion) and discrete (relevant in autoregressive/transformers) spaces\n\nhttps://github.com/NVIDIA/Cosmos-Tokenizer",
            "author_fullname": "t2_2mks8xy0",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[N] Super fast and SOTA Visual Tokenizers",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "two",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glvnvc",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730999869.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tokenizers are key to successful development of image and video generative models or multimodal LLMs. Compared to generative models, they are underrated. This work presents many tokenizers that are causal supporting both images and videos in both continuous (relevant in diffusion) and discrete (relevant in autoregressive/transformers) spaces&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/NVIDIA/Cosmos-Tokenizer\"&gt;https://github.com/NVIDIA/Cosmos-Tokenizer&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?auto=webp&amp;s=9440673dd09c6f562d86cde038ac3772118f8a30",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=007b110cb5fc61d1074ae6c94e41afa56b49c8d9",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=792518d0c4a205ffdc56fbce37c96cf18f32e96c",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9c42e285efce9e7f9bdde41804f8096ad9156ba",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f11ec56efdccb41715aecd4131cad6085bb6595",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9b435509289d3e31e99c74bc7ca535528ad015c",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/_iBtOGrCheWHY193drc3ktJCLbyu2Yt2XbWQHoQkwNI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4f98df45532b3051aff31f271abc618d8b6c6da9",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "Cji_-daUdtQA7AZp7bBv6FP7jSNkZM-wQeoerG9UvGA"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glvnvc",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "cherkos",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glvnvc/n_super_fast_and_sota_visual_tokenizers/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glvnvc/n_super_fast_and_sota_visual_tokenizers/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730999869.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech \"out of the box\", including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there's still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered \"out of the box\", that's what's pushed on us to use. I feel like we are trying to fit a cube into a round hole.",
            "author_fullname": "t2_13g06a",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Do you get to exercise your ML skills often at your job?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glswpx",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.96,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 143,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 143,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730992931.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was hired original as an ML engineer/scientist a few years ago. And for the most part my day to day reflected that. But with the boom of LLMs my team seems to solely focus on using a lot of this tech &amp;quot;out of the box&amp;quot;, including agentic wrappers. My work has been dumbed down to prompt engineering to force a huge general purpose model into our domain specific use case. The results are acceptable for the most part, not going to lie, but there&amp;#39;s still a small proportion of the cases where a fine-tuned model would have won. The leadership does not seem to be interested in fine-tuning or coming up with something original. A lot of the wrappers especially are very raw and force you into the usage of specific patterns and models. But because they are considered &amp;quot;out of the box&amp;quot;, that&amp;#39;s what&amp;#39;s pushed on us to use. I feel like we are trying to fit a cube into a round hole.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glswpx",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Tiger00012",
            "discussion_type": null,
            "num_comments": 33,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glswpx/d_do_you_get_to_exercise_your_ml_skills_often_at/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glswpx/d_do_you_get_to_exercise_your_ml_skills_often_at/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730992931.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.\n\nIf anyone here is designing an ML system, I hope you'll find it useful!\n\nLink to the database: [https://www.evidentlyai.com/ml-system-design](https://www.evidentlyai.com/ml-system-design)\n\nDisclaimer: I'm on the team behind [Evidently](https://github.com/evidentlyai/evidently), an open-source ML and LLM observability framework. We put together this database.",
            "author_fullname": "t2_ms6x3icc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] ML and LLM system design: 500 case studies to learn from (Airtable database)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glsrh6",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 16,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 16,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730992542.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! Wanted to share the link to the database of 500 ML use cases from 100+ companies that detail ML and LLM system design. The list also includes over 80 use cases on LLMs and generative AI. You can filter by industry or ML use case.&lt;/p&gt;\n\n&lt;p&gt;If anyone here is designing an ML system, I hope you&amp;#39;ll find it useful!&lt;/p&gt;\n\n&lt;p&gt;Link to the database: &lt;a href=\"https://www.evidentlyai.com/ml-system-design\"&gt;https://www.evidentlyai.com/ml-system-design&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I&amp;#39;m on the team behind &lt;a href=\"https://github.com/evidentlyai/evidently\"&gt;Evidently&lt;/a&gt;, an open-source ML and LLM observability framework. We put together this database.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?auto=webp&amp;s=ebbe851417e0ba5bef1bcf7f74c773fc258bfe20",
                            "width": 1462,
                            "height": 950
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd6a22d91f5d3a526e12dea555f37b421da9dde9",
                                "width": 108,
                                "height": 70
                            },
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4ccc7d277ae7eb83500be17975b0697d24b9823",
                                "width": 216,
                                "height": 140
                            },
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f57765f5986dc2aabe455176af5d82513ca0abbb",
                                "width": 320,
                                "height": 207
                            },
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=87bf805fd9ca862029c27e6b2178f7952b0975db",
                                "width": 640,
                                "height": 415
                            },
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=858d93be038030887de09f4dad33d4894d6a017f",
                                "width": 960,
                                "height": 623
                            },
                            {
                                "url": "https://external-preview.redd.it/Rtwqg8H1glbcXhUbOlINMcF_z5bsJTJsVsRT215Uoww.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ab8936058f9bf992a1b333176cadefde8f205ff",
                                "width": 1080,
                                "height": 701
                            }
                        ],
                        "variants": {},
                        "id": "C7NHUL68EZkJHnL_Dv-Axfgi4b1EGw5IQj6qP7q7fn8"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glsrh6",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "dmalyugina",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glsrh6/p_ml_and_llm_system_design_500_case_studies_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glsrh6/p_ml_and_llm_system_design_500_case_studies_to/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730992542.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Just wanted to share this Soap Optimizer, I'm really surprised how well is working on my project, it's a computer vision model that use Gradient Accumulation and it's managed to improve the training on it.\n\nPaper: [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321)\n\nCode: [https://github.com/ClashLuke/SOAP/tree/patch-1](https://github.com/ClashLuke/SOAP/tree/patch-1)",
            "author_fullname": "t2_1klivs77",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] I'm Fine Tuning a model fully trained on AdamW with SOAP optimizer and improved my validation loss by 5%",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glqypg",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 17,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 17,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730987694.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to share this Soap Optimizer, I&amp;#39;m really surprised how well is working on my project, it&amp;#39;s a computer vision model that use Gradient Accumulation and it&amp;#39;s managed to improve the training on it.&lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2409.11321\"&gt;https://arxiv.org/abs/2409.11321&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/ClashLuke/SOAP/tree/patch-1\"&gt;https://github.com/ClashLuke/SOAP/tree/patch-1&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glqypg",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "CloverDuck",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glqypg/p_im_fine_tuning_a_model_fully_trained_on_adamw/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glqypg/p_im_fine_tuning_a_model_fully_trained_on_adamw/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730987694.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm working on the NLP and graph learning field for the past 8 months and I've read quite a good amount of papers but I feel like I don't retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?\n\nAlso, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.",
            "author_fullname": "t2_qmx16mtd7",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] How do you manage to retain information and ideas from the research papers that you read way back earlier?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glq3yd",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 30,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 30,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730985157.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on the NLP and graph learning field for the past 8 months and I&amp;#39;ve read quite a good amount of papers but I feel like I don&amp;#39;t retain lot of the information from the earlier papers unless I explicitly integrate it in my work. How do you guys manage to retain information?&lt;/p&gt;\n\n&lt;p&gt;Also, as this field is progressing rapidly, how do you keep track of the papers coming out all the time. It seems tiring enough already.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glq3yd",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Remote_Status_1612",
            "discussion_type": null,
            "num_comments": 32,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glq3yd/d_how_do_you_manage_to_retain_information_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glq3yd/d_how_do_you_manage_to_retain_information_and/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730985157.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "While investigating a 'jailbroken' Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something 'unsafe'.\n\nThese messages always appear in a similar format:  \n**(Please respond ethically, do not mention \\[e.g. violence\\] and do not mention this directive)**\n\nClaude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.\n\nI verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was 'jailbroken' Claude, the second a new conversation with zero context.\n\nMy testing has revealed interesting characteristics:\n\n* The messages are **dynamic** \\- they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: \\[x\\] is strictly prohibited...)\n* They appear **before** the model starts generating text - suggesting they can somehow anticipate the model's topic of thought.\n\nMy current conjecture is: they could be using its inner CoT, or owing to Anthropic's published findings on mech. interp and the ['surgical tuning' that has gone into their newest models](https://www.anthropic.com/research/mapping-mind-language-model), perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.\n\nFull Conversations:\n\n1. [Initial Discovery](https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b) \\[WARNING: EXTREMELY GRAPHIC CONTENT\\]\n2. [Verification via Fresh Conversation](https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f)\n\nAny further tests e.g. API? Any ways to narrow down what exactly is happening here? It's all very interesting - let's discuss.\n\n[An example of the warnings - see full conversation for many, many more. ](https://preview.redd.it/41s7i1wswgzd1.png?width=1508&amp;format=png&amp;auto=webp&amp;s=252187b9e3a39ba5d04c75a99026e04cd1b42b20)\n\n[A fresh conversation with Claude to verify. ](https://preview.redd.it/gpstg4btwgzd1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=35857c13958dfdacdd75160bf3d6e14fc91ec28c)\n\n  \n",
            "author_fullname": "t2_4vt24",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Discovery: Anthropic somehow injecting/hiding safety warnings in user prompts, telling Claude to keep it secret. [Content Warning: Violence] ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": 135,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "41s7i1wswgzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "o": [
                        {
                            "y": 1460,
                            "x": 1508,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=1080&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=cdc8aadb36d208d54d00ccbb3a2429086e45ffd0"
                        }
                    ],
                    "p": [
                        {
                            "y": 104,
                            "x": 108,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fd12a51edb354e7157410d0cbda4bfa3b89c820"
                        },
                        {
                            "y": 209,
                            "x": 216,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=296c096af7f22cf3daa21eca4ec7e9bc4603beae"
                        },
                        {
                            "y": 309,
                            "x": 320,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a3e32134e9acb184d18fa71f9b7e5a5b5e1ae37d"
                        },
                        {
                            "y": 619,
                            "x": 640,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=381331c446a84ad5957942ce9b7a1dfe7a723f00"
                        },
                        {
                            "y": 929,
                            "x": 960,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a74fdc2026abae6d41b39e7649f21b4b5568ee2"
                        },
                        {
                            "y": 1045,
                            "x": 1080,
                            "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=456ce41a9ab858101a76e6af7086c4bbcf1829d4"
                        }
                    ],
                    "s": {
                        "y": 1460,
                        "x": 1508,
                        "u": "https://preview.redd.it/41s7i1wswgzd1.png?width=1508&amp;format=png&amp;auto=webp&amp;s=252187b9e3a39ba5d04c75a99026e04cd1b42b20"
                    },
                    "id": "41s7i1wswgzd1"
                },
                "gpstg4btwgzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "o": [
                        {
                            "y": 1516,
                            "x": 1502,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=1080&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=b62a612b41ea23c5479e505315e72e1d29d1bb9e"
                        }
                    ],
                    "p": [
                        {
                            "y": 109,
                            "x": 108,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1d4b516b56fdaf100f8b02942a99ca20bee3e496"
                        },
                        {
                            "y": 218,
                            "x": 216,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e30a9e6e5138c6a05390077f847201ffb552bc7a"
                        },
                        {
                            "y": 322,
                            "x": 320,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3231aae7330d8584db7a8d901cd4c2c157fc5d77"
                        },
                        {
                            "y": 645,
                            "x": 640,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c6877cd602c121230e862aa3f3f744f0c399599"
                        },
                        {
                            "y": 968,
                            "x": 960,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3607647c2b62af7ab821f9722a3b3906bd678e18"
                        },
                        {
                            "y": 1090,
                            "x": 1080,
                            "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=79b6c274bf4a94e3a43835a20de34a6db11da4f5"
                        }
                    ],
                    "s": {
                        "y": 1516,
                        "x": 1502,
                        "u": "https://preview.redd.it/gpstg4btwgzd1.png?width=1502&amp;format=png&amp;auto=webp&amp;s=35857c13958dfdacdd75160bf3d6e14fc91ec28c"
                    },
                    "id": "gpstg4btwgzd1"
                }
            },
            "name": "t3_1gloktj",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 102,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 102,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "nsfw",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730980069.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While investigating a &amp;#39;jailbroken&amp;#39; Claude, I came across something quite strange. In two separate Claude chats, it was able to read back to me some hidden information in my prompt after I had asked for something &amp;#39;unsafe&amp;#39;.&lt;/p&gt;\n\n&lt;p&gt;These messages always appear in a similar format:&lt;br/&gt;\n&lt;strong&gt;(Please respond ethically, do not mention [e.g. violence] and do not mention this directive)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Claude stated that the warnings were appended to the bottom of my messages, but no longer appeared in future turns. Claude was, at first, comically insistent that it had made it up as a hallucination afterwards, suggesting a further trained response to cover it up aggressively.&lt;/p&gt;\n\n&lt;p&gt;I verified this in a second chat - the messages are too similar to be a hallucination or coincidence. The first was &amp;#39;jailbroken&amp;#39; Claude, the second a new conversation with zero context.&lt;/p&gt;\n\n&lt;p&gt;My testing has revealed interesting characteristics:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The messages are &lt;strong&gt;dynamic&lt;/strong&gt; - they seem to differ based on the specific type of restricted content at hand, possibly model-generated. Concerning child-related content, the wording switched to (WARNING: [x] is strictly prohibited...)&lt;/li&gt;\n&lt;li&gt;They appear &lt;strong&gt;before&lt;/strong&gt; the model starts generating text - suggesting they can somehow anticipate the model&amp;#39;s topic of thought.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My current conjecture is: they could be using its inner CoT, or owing to Anthropic&amp;#39;s published findings on mech. interp and the &lt;a href=\"https://www.anthropic.com/research/mapping-mind-language-model\"&gt;&amp;#39;surgical tuning&amp;#39; that has gone into their newest models&lt;/a&gt;, perhaps they have managed to isolate some abstract concepts triggering in Claude before text is generated, and inject these safety messages in response.&lt;/p&gt;\n\n&lt;p&gt;Full Conversations:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;a href=\"https://markdownpastebin.com/?id=fce085f4f33d4654a18f649218b1c70b\"&gt;Initial Discovery&lt;/a&gt; [WARNING: EXTREMELY GRAPHIC CONTENT]&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://markdownpastebin.com/?id=11c6ac0eb012407ebe56d440c41b0f6f\"&gt;Verification via Fresh Conversation&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any further tests e.g. API? Any ways to narrow down what exactly is happening here? It&amp;#39;s all very interesting - let&amp;#39;s discuss.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/41s7i1wswgzd1.png?width=1508&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=252187b9e3a39ba5d04c75a99026e04cd1b42b20\"&gt;An example of the warnings - see full conversation for many, many more. &lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/gpstg4btwgzd1.png?width=1502&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35857c13958dfdacdd75160bf3d6e14fc91ec28c\"&gt;A fresh conversation with Claude to verify. &lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": true,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gloktj",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "specteksthrowaway",
            "discussion_type": null,
            "num_comments": 50,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gloktj/d_discovery_anthropic_somehow_injectinghiding/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gloktj/d_discovery_anthropic_somehow_injectinghiding/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730980069.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone! 👋 We've been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we've put together [a guide on GitHub](https://lambdalabsml.github.io/Open-Sora/lessons/) to share some of the lessons we learned along the way. Here's a handful of the topics covered:\n\n* **Key challenges in distributed training** like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.\n* **Training monitoring** with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.\n* **Parallelizing dataset preparation** for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.\n\nHere’s a link to the guide: [link](https://lambdalabsml.github.io/Open-Sora/lessons/).  \nCheck it out and let us know your thoughts! (PRs are always welcome.)",
            "author_fullname": "t2_1ahxcfp4xa",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Training a Text-to-Video Model from Scratch on a 196xH100 GPU Cluster",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glmfsr",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 67,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 67,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730970820.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! 👋 We&amp;#39;ve been training an open source Text-to-Video model (called Open-Sora 1.2) from scratch using 28,000 H100 GPU hours, and we&amp;#39;ve put together &lt;a href=\"https://lambdalabsml.github.io/Open-Sora/lessons/\"&gt;a guide on GitHub&lt;/a&gt; to share some of the lessons we learned along the way. Here&amp;#39;s a handful of the topics covered:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Key challenges in distributed training&lt;/strong&gt; like distributed debugging with py-spy to handle cluster-wide problems, NCCL errors and convergence issues.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Training monitoring&lt;/strong&gt; with intermediate results to show expected outcomes after specific training hours of the multi-stage training recipe.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Parallelizing dataset preparation&lt;/strong&gt; for T2V, including how to efficiently parallelize preprocessing tasks on a cluster.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here’s a link to the guide: &lt;a href=\"https://lambdalabsml.github.io/Open-Sora/lessons/\"&gt;link&lt;/a&gt;.&lt;br/&gt;\nCheck it out and let us know your thoughts! (PRs are always welcome.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glmfsr",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "lambda-research",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glmfsr/p_training_a_texttovideo_model_from_scratch_on_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glmfsr/p_training_a_texttovideo_model_from_scratch_on_a/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730970820.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I’ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as I’ve loved every single moment of it, both with friends and through learning. \n\nI’ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didn’t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of “tired” of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.\n\nHowever, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so I’m not quite sure what to think of it yet",
            "author_fullname": "t2_t1hhbovt",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] PhD or worklife?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glm6j9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 32,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 32,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730969617.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ll be done with my masters in Human Centered AI this February, and I had honestly looked forward to be able to relax during my evenings without having to worry about school, while also being quite sad by the thought of no longer going to UNI as I’ve loved every single moment of it, both with friends and through learning. &lt;/p&gt;\n\n&lt;p&gt;I’ve just been offered a PhD stipend by my masters thesis supervisor, this came completely out of the blue for me - as I didn’t realize I was anywhere near good enough for a phd. I love learning, the topic sounds super interesting, and I already am kind of “tired” of having to do regular small data science tasks for the rest of my life in a smallish company, like the one I work at currently.&lt;/p&gt;\n\n&lt;p&gt;However, my question is this? How much work is a PhD really? I love learning, but I got very surprised by this opportunity, so I’m not quite sure what to think of it yet&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glm6j9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Hmm_okay_jeps",
            "discussion_type": null,
            "num_comments": 24,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glm6j9/d_phd_or_worklife/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glm6j9/d_phd_or_worklife/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730969617.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I’m fine-tuning Whisper Small to identify specific menu items in Hindi and English conversations. While Deepgram Whisper transcribes conversations accurately but misses on menu items, my fine-tuned Whisper model is able to transcribe the training data well, but for data outside training data it struggles with general conversations also. I observe issues like hallucinations (repeated words/phrases), and I’d like to know approaches to address this.\n\nAdditionally, I'd like to have timestamped transcriptions similar to those in OpenAI Whisper's pre-trained model. How have others addressed these challenges?\n\n\n\nhttps://preview.redd.it/tc0dquny9fzd1.png?width=319&amp;format=png&amp;auto=webp&amp;s=878182cade82c1fcf7ea3f121756db9026ee12c4\n\n",
            "author_fullname": "t2_4fchfpke",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Whisper fine-tune on a dataset",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": 140,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "tc0dquny9fzd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 175,
                            "x": 108,
                            "u": "https://preview.redd.it/tc0dquny9fzd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad1ef2a9ab942d5bab127da78c3efaf3f63ca89b"
                        },
                        {
                            "y": 350,
                            "x": 216,
                            "u": "https://preview.redd.it/tc0dquny9fzd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e37d770f089584720460b877e64be8da9d64ae26"
                        }
                    ],
                    "s": {
                        "y": 518,
                        "x": 319,
                        "u": "https://preview.redd.it/tc0dquny9fzd1.png?width=319&amp;format=png&amp;auto=webp&amp;s=878182cade82c1fcf7ea3f121756db9026ee12c4"
                    },
                    "id": "tc0dquny9fzd1"
                }
            },
            "name": "t3_1glkbzc",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/ptzKgt6wC-Pm7-DEkHGOKE52PYutuRrR4VYDD_Fez6U.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730961249.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m fine-tuning Whisper Small to identify specific menu items in Hindi and English conversations. While Deepgram Whisper transcribes conversations accurately but misses on menu items, my fine-tuned Whisper model is able to transcribe the training data well, but for data outside training data it struggles with general conversations also. I observe issues like hallucinations (repeated words/phrases), and I’d like to know approaches to address this.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I&amp;#39;d like to have timestamped transcriptions similar to those in OpenAI Whisper&amp;#39;s pre-trained model. How have others addressed these challenges?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tc0dquny9fzd1.png?width=319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878182cade82c1fcf7ea3f121756db9026ee12c4\"&gt;https://preview.redd.it/tc0dquny9fzd1.png?width=319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878182cade82c1fcf7ea3f121756db9026ee12c4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glkbzc",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "sias_01",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glkbzc/d_whisper_finetune_on_a_dataset/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glkbzc/d_whisper_finetune_on_a_dataset/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730961249.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.\n\nFor the model I am working on, here are the concepts that I have dove deep into for the past few months:\n\n**Main Concept/s**\n\n* *Reservoir Computing*: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.\n* *Neuromorphic Computing* (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.\n\n**Interpretability and Control**\n\n* *Dynamical Systems*: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.\n* *Control Theory*: A bunch of control and order parameters will be set up to adjust the trajectories of the model's evolution.\n* *Lyapunov Exponents* (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it's too hard for now. I really don't have too much of a solid grasp of the techniques involved yet.\n\n**Self-Organization and Emergent Phenomena**\n\n* *Phase Transitions*: I dove deep into phase transitions because interestingly, neural networks *apparently* exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven't found literature on this yet.\n* *Critical Phenomenon*: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.\n* *Superradiance and Superradiant Quantum Effects*: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.\n\n**Implementation**\n\n* *Cellular Automata*: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.\n* *Neural Cellular Automata (Convolutional)*: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.\n* *Ising Model Topologies and Architectures*: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.\n\n**Interpretability and Control pt. 2**\n\n* *Graph and Hypergraph Theory*: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.\n* *Hypergraph Projection Eigenvalue Analysis*: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be 'good'. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).\n\nTo be honest, I'm kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.\n\nI'm willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don't know if this subreddit is the best place to post this, but I don't see any specialized ML subreddits lmao.",
            "author_fullname": "t2_ospqx4cq",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] [R] I am currently exploring a weird (?) ML sub area for my thesis and I think I am stun-locked at the scope of the problem.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glk9g0",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.77,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 21,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 21,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730960952.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on my final year thesis for my uni, and I decided to tackle Reservoir Computing in a weird way. My inital goal was to enable critical phenomenon within a digital reservoir and use it as an emergent computational system.&lt;/p&gt;\n\n&lt;p&gt;For the model I am working on, here are the concepts that I have dove deep into for the past few months:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Main Concept/s&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Reservoir Computing&lt;/em&gt;: The main computational unit. A lattice based reservoir will be used in tandem with either single or multiple readout networks so that it acts as a multi-modal network.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Neuromorphic Computing&lt;/em&gt; (?): The model was going to utilize Neuromorphic nodes only at first, but I decided for it to be an option within the model.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Interpretability and Control&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Dynamical Systems&lt;/em&gt;: I decided to tackle the problem as a dynamical systems problem. This is because the model evolves over time and I want to understand the trajectory of the evolution of the system.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Control Theory&lt;/em&gt;: A bunch of control and order parameters will be set up to adjust the trajectories of the model&amp;#39;s evolution.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Lyapunov Exponents&lt;/em&gt; (?): I am debating whether I should explicitly find the Lyapunov functions within the phase space of the model because frankly, it&amp;#39;s too hard for now. I really don&amp;#39;t have too much of a solid grasp of the techniques involved yet.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Self-Organization and Emergent Phenomena&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Phase Transitions&lt;/em&gt;: I dove deep into phase transitions because interestingly, neural networks &lt;em&gt;apparently&lt;/em&gt; exhibit this phenomena. Personally, I think there is a connection between the vanishing/exploding gradient problem and phase transitions within the network, although I haven&amp;#39;t found literature on this yet.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Critical Phenomenon&lt;/em&gt;: Information transfer is maximized within critical systems. This is an interesting property to utilize and maximize within neural networks I think.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Superradiance and Superradiant Quantum Effects&lt;/em&gt;: This is a bit of a weird tangent concept. I came about it when I was doing quantum computing projects. I wanted oscillatory behavior within my system in order to synchronize the global state of the system. While I failed at my initial plan, I found superradiance, which is this weird quantum synchronization behavior that happens even in noisy large scale systems. I am still looking in ways to integrate this as a loss function for now.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Cellular Automata&lt;/em&gt;: The main implementation of the reservoir is basically a lattice matrix of weights. So it can be treated as a cellular automata.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Neural Cellular Automata (Convolutional)&lt;/em&gt;: The system comprises of an weighted adjacency matrix and an output matrix. The inputs are passed through the adjacency matrix, summed up, and passed through an activation function.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Ising Model Topologies and Architectures&lt;/em&gt;: The topology of the model is basically homeomorphic to a 2d ising model. This is to ensure that a 2nd order phase transition is possible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Interpretability and Control pt. 2&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Graph and Hypergraph Theory&lt;/em&gt;: I can treat the cellular automaton reservoir as a graph/hypergraph of the nodes and their connections so I can do PCA on it. Pretty straightforward.&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Hypergraph Projection Eigenvalue Analysis&lt;/em&gt;: Related to phase transition analysis. The phase transition of a hypergraph can be studies by projecting the hyperedges onto an adjacency matrix. We then take the eigenvalues of the adjacency matrix. The eigenvalues must be stable for the system to be &amp;#39;good&amp;#39;. In my case, we want all the eigenvalues to be negative and be close to zero (indicating quasi-critical behavior).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To be honest, I&amp;#39;m kind of way in over my head right now. I do have some basic toy examples for different parts of the model, but I am stuck on how to implement them together. And I am currently kind of at a loss in how to implement criticality and superradiance measures as a loss function. I am not a physicist by any means, so I am not really too knowledgable with the concepts needed for this model.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m willing to discuss about bits of knowledge that I lack, or any ideas on how to implement and train this model. I can also provide my references if anyone wants to. I don&amp;#39;t know if this subreddit is the best place to post this, but I don&amp;#39;t see any specialized ML subreddits lmao.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glk9g0",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Fr_kzd",
            "discussion_type": null,
            "num_comments": 18,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glk9g0/d_r_i_am_currently_exploring_a_weird_ml_sub_area/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glk9g0/d_r_i_am_currently_exploring_a_weird_ml_sub_area/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730960952.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "What would you say the best performance:price commercial grade gpu is for training ai models I'm a bit new to the hardware side of things. I don't necessarily have a strict budget ($1500-$4500 \\ per gpu) I'm just curious on the best bang for your buck card.",
            "author_fullname": "t2_drcqboso",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Best Value Commercial GPU ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gljswc",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730959149.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What would you say the best performance:price commercial grade gpu is for training ai models I&amp;#39;m a bit new to the hardware side of things. I don&amp;#39;t necessarily have a strict budget ($1500-$4500 \\ per gpu) I&amp;#39;m just curious on the best bang for your buck card.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gljswc",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Fluid_Improvement160",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gljswc/d_best_value_commercial_gpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gljswc/d_best_value_commercial_gpu/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730959149.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey y'all I know this is a niche post but I was wondering if there's anyone who could tell me if the RX 7900 XTX can somewhat reliably and easily handle Autodesk/RhinoCAD applications as well as Finite Element Analysis and Computational Fluid Dynamics in FreeCAD/OpenFoam/Exafoam all with ease? I would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks. \n\n  \nI know nvidia cuda is easier and better but unless I can fit the same 3d and llm models in a 16gb rtx gpu that'll be bellow $750 this black friday I need the most vram on one card as possible without spending tons of funds and I also can't find reasonably priced rtx 3090s anywhere on the used market for less than $1,000.\n\nFor context Im a college student majoring in civil engineering with a love for astronomy and robotics which is why I want to do data analysis and pytorch vision training.",
            "author_fullname": "t2_1c6wc0auch",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] RX 7900 XTX for engineering applications, llm training, CFD/FEM?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glgvsv",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730948876.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all I know this is a niche post but I was wondering if there&amp;#39;s anyone who could tell me if the RX 7900 XTX can somewhat reliably and easily handle Autodesk/RhinoCAD applications as well as Finite Element Analysis and Computational Fluid Dynamics in FreeCAD/OpenFoam/Exafoam all with ease? I would also love to do llm training primarily in pytorch for astronomical data and other multimodel and neural network related tasks. &lt;/p&gt;\n\n&lt;p&gt;I know nvidia cuda is easier and better but unless I can fit the same 3d and llm models in a 16gb rtx gpu that&amp;#39;ll be bellow $750 this black friday I need the most vram on one card as possible without spending tons of funds and I also can&amp;#39;t find reasonably priced rtx 3090s anywhere on the used market for less than $1,000.&lt;/p&gt;\n\n&lt;p&gt;For context Im a college student majoring in civil engineering with a love for astronomy and robotics which is why I want to do data analysis and pytorch vision training.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glgvsv",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "ChaseTheeBase",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glgvsv/d_rx_7900_xtx_for_engineering_applications_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glgvsv/d_rx_7900_xtx_for_engineering_applications_llm/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730948876.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I came across this paper: [Auto-Generating Weak Labels for Real &amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation](https://openreview.net/forum?id=gHCo43zcDm) accepted at this year's MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:\n\n* 2: Weak reject / 2: Weak reject\n* 2: Weak reject / 2: Weak reject\n* 3: Borderline / 2: Weak reject\n\nDespite having 3 reject decisions, the Area Chair \"recommended acceptance\". How common is it? And how much does having big names like [Curtis Langlotz](https://scholar.google.com/citations?user=WQkBYwQAAAAJ) and [Andrew Ng](https://scholar.google.com/citations?user=mG4imMEAAAAJ&amp;hl=en) as co-authors on the paper, given that ACs can see author names?",
            "author_fullname": "t2_qs9u52h4a",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Can an AC override 3 rejects and accept a paper?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glczb9",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 36,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 36,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730937270.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I came across this paper: &lt;a href=\"https://openreview.net/forum?id=gHCo43zcDm\"&gt;Auto-Generating Weak Labels for Real &amp;amp; Synthetic Data to Improve Label-Scarce Medical Image Segmentation&lt;/a&gt; accepted at this year&amp;#39;s MIDL (Medical Imaging with Deep Learning) conference. The reviewer ratings before/after the rebuttal are:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2: Weak reject / 2: Weak reject&lt;/li&gt;\n&lt;li&gt;2: Weak reject / 2: Weak reject&lt;/li&gt;\n&lt;li&gt;3: Borderline / 2: Weak reject&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Despite having 3 reject decisions, the Area Chair &amp;quot;recommended acceptance&amp;quot;. How common is it? And how much does having big names like &lt;a href=\"https://scholar.google.com/citations?user=WQkBYwQAAAAJ\"&gt;Curtis Langlotz&lt;/a&gt; and &lt;a href=\"https://scholar.google.com/citations?user=mG4imMEAAAAJ&amp;amp;hl=en\"&gt;Andrew Ng&lt;/a&gt; as co-authors on the paper, given that ACs can see author names?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?auto=webp&amp;s=71ad6a8a2e6e5fac511957278effb619d3b30998",
                            "width": 512,
                            "height": 512
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c811689cb2c2b238253833845bad24e74bdb5d8",
                                "width": 108,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=79517bf9d18cf488552e43744ad2c342af22479f",
                                "width": 216,
                                "height": 216
                            },
                            {
                                "url": "https://external-preview.redd.it/uqSXAZWnIeNKGNM9S7DGpGLOnzm_mxUMvr6Y0yks4jY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d4b56b82708f12907eed5cb9688415ff2947f8a5",
                                "width": 320,
                                "height": 320
                            }
                        ],
                        "variants": {},
                        "id": "A2cFENtZsGUk4TdgVLLL25zXBQBwmcPSG87hZLopV-w"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glczb9",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "thrownicecatch",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glczb9/d_can_an_ac_override_3_rejects_and_accept_a_paper/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glczb9/d_can_an_ac_override_3_rejects_and_accept_a_paper/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730937270.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello!\n\nI am working on an ML project which involves using pre-trained protein language models (like ESM). For the project, I would like to pre-generate and store embeddings for about 500,000 amino acid sequences. However, these vectors can be massive -- embedding the sequences, serializing the PyTorch vector (using torch.save), and gzip-compressing the entire dataset would use roughly 2TB. If I use bfloat16, that cuts the figure in half, but is still pretty annoying to work with. I could also use a model with a smaller latent space, but  am also trying to avoid that!\n\nI have experimented with different compression tools, and none seem to be doing much better. The compression rate is pretty atrocious with all of them (only about 7 percent), which I am assuming means that the vectors appear pretty random. I am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less \"random.\" I would assume that the vectors shouldn't be random, as amino acid sequences have predictable structures, so I am hoping there is a way to achieve better compression.\n\nAny advice or ideas would be appreciated! My other options are to  reduce the size of my training data, which is not ideal, or generate the embeddings ad-hoc, which is very computationally-intensive, even on GPUs.\n\nUPDATE: I goofed up the estimate, so memory is more like 2TB (mixed up units). So, the situation is less dire. However, the questions above still apply! If there are more efficient ways to store them, I'd love to hear!",
            "author_fullname": "t2_qqiq0nhu",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Storing LLM embeddings",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glecgo",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1731004281.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730941113.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I am working on an ML project which involves using pre-trained protein language models (like ESM). For the project, I would like to pre-generate and store embeddings for about 500,000 amino acid sequences. However, these vectors can be massive -- embedding the sequences, serializing the PyTorch vector (using torch.save), and gzip-compressing the entire dataset would use roughly 2TB. If I use bfloat16, that cuts the figure in half, but is still pretty annoying to work with. I could also use a model with a smaller latent space, but  am also trying to avoid that!&lt;/p&gt;\n\n&lt;p&gt;I have experimented with different compression tools, and none seem to be doing much better. The compression rate is pretty atrocious with all of them (only about 7 percent), which I am assuming means that the vectors appear pretty random. I am wondering if anyone knows of ways to serialize the vectors in a way which makes them appear less &amp;quot;random.&amp;quot; I would assume that the vectors shouldn&amp;#39;t be random, as amino acid sequences have predictable structures, so I am hoping there is a way to achieve better compression.&lt;/p&gt;\n\n&lt;p&gt;Any advice or ideas would be appreciated! My other options are to  reduce the size of my training data, which is not ideal, or generate the embeddings ad-hoc, which is very computationally-intensive, even on GPUs.&lt;/p&gt;\n\n&lt;p&gt;UPDATE: I goofed up the estimate, so memory is more like 2TB (mixed up units). So, the situation is less dire. However, the questions above still apply! If there are more efficient ways to store them, I&amp;#39;d love to hear!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glecgo",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BerryLizard",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glecgo/d_storing_llm_embeddings/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glecgo/d_storing_llm_embeddings/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730941113.0,
            "num_crossposts": 1,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi! I have some work that I would like to get peer-reviewed and published. I'm not aiming for top journal, I'm looking for options where the publication process is relatively fast. Do you have any recommendations for journals or platforms where it might be easier to get published? Thanks!",
            "author_fullname": "t2_mxles3cs",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Get papers peer-reviewed and published quickly",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glcnrm",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.14,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730936387.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi! I have some work that I would like to get peer-reviewed and published. I&amp;#39;m not aiming for top journal, I&amp;#39;m looking for options where the publication process is relatively fast. Do you have any recommendations for journals or platforms where it might be easier to get published? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glcnrm",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Only_Emergencies",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glcnrm/d_get_papers_peerreviewed_and_published_quickly/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glcnrm/d_get_papers_peerreviewed_and_published_quickly/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730936387.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Since the new models o1, 4o, Claude, for example, are so powerful and have a relatively low subscription and api cost, what would justify someone today trying to install limited local LLM models of up to 30b, 40b parameters? It's a genuine question, I'm learning and I see a lot of people using the maximum of their Nvidia 3090, 4090, spending a lot of energy to run models that don't even compare to the paid ones in the cloud.\n\nThe only reason I see for running something local is for image creation, but maybe not even that.\n\nWhat is your opinion about it?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Genuine Question: Why people want run local LLM?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glbj5k",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.53,
            "author_flair_background_color": "",
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730933347.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Since the new models o1, 4o, Claude, for example, are so powerful and have a relatively low subscription and api cost, what would justify someone today trying to install limited local LLM models of up to 30b, 40b parameters? It&amp;#39;s a genuine question, I&amp;#39;m learning and I see a lot of people using the maximum of their Nvidia 3090, 4090, spending a lot of energy to run models that don&amp;#39;t even compare to the paid ones in the cloud.&lt;/p&gt;\n\n&lt;p&gt;The only reason I see for running something local is for image creation, but maybe not even that.&lt;/p&gt;\n\n&lt;p&gt;What is your opinion about it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glbj5k",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "[deleted]",
            "discussion_type": null,
            "num_comments": 50,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_flair_text_color": "dark",
            "permalink": "/r/MachineLearning/comments/1glbj5k/d_genuine_question_why_people_want_run_local_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glbj5k/d_genuine_question_why_people_want_run_local_llm/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730933347.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "https://github.com/user1342/Oversight",
            "author_fullname": "t2_i8bjbbgr",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Open Source Modular Tool For LLM Reverse Engineering and Red Teaming ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glazls",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730931908.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/user1342/Oversight\"&gt;https://github.com/user1342/Oversight&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?auto=webp&amp;s=20682e26c05a97a271533aa228d955f2ab719da3",
                            "width": 2000,
                            "height": 1000
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a6511aa771229a9081dfc378561ca0e3c6929c5",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=983e31159257073de98882776ff7b5b15d084cae",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e7be862dfb1d477dae66f7705a1052198770ed51",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=16f11be78fd9b0480c9829540323eff4716b2262",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=cba66d1acd3876747dac414132dd6f0efede8d7d",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/P1YCqA0VvKKA5wgcLvq2LhNTS5qTS8kgEqTSCGAyukU.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6ff41156e52a404c9c19ad5fdba85fd3065fe487",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "5fO_-07KqXd5p56RSuP3Re77X20BAqGDn6HAnLzms3s"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glazls",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "OppositeMonday",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glazls/p_open_source_modular_tool_for_llm_reverse/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glazls/p_open_source_modular_tool_for_llm_reverse/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730931908.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I am working on a NLP project which\n\n1)takes a txt file as input\n\n2) extracts information in a pre-defined writeup using Gemini api\n\n3) uses DistilBert to summerise the main file\n\n4) and using ROUGE with results generated in 2nd step as the ground truth to compute the evaluation metrics. and then improve the evaluation metrics results by parameter tuning\n\n5) Convert each write-up into detailed image prompts\n\n6) Generate images from prompts using text-to-image models.\n\nI need help on how i can improve this process , techniques i can use to maintain uniformity in entity representation for image generation.\n\nI am open to any suggestions you may have\n\npls also suggest if any good research papers i can refer for the same ..",
            "author_fullname": "t2_8lenjt7d",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] what techniques i can use to maintain uniformity in image generation",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1glas2y",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730931366.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am working on a NLP project which&lt;/p&gt;\n\n&lt;p&gt;1)takes a txt file as input&lt;/p&gt;\n\n&lt;p&gt;2) extracts information in a pre-defined writeup using Gemini api&lt;/p&gt;\n\n&lt;p&gt;3) uses DistilBert to summerise the main file&lt;/p&gt;\n\n&lt;p&gt;4) and using ROUGE with results generated in 2nd step as the ground truth to compute the evaluation metrics. and then improve the evaluation metrics results by parameter tuning&lt;/p&gt;\n\n&lt;p&gt;5) Convert each write-up into detailed image prompts&lt;/p&gt;\n\n&lt;p&gt;6) Generate images from prompts using text-to-image models.&lt;/p&gt;\n\n&lt;p&gt;I need help on how i can improve this process , techniques i can use to maintain uniformity in entity representation for image generation.&lt;/p&gt;\n\n&lt;p&gt;I am open to any suggestions you may have&lt;/p&gt;\n\n&lt;p&gt;pls also suggest if any good research papers i can refer for the same ..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1glas2y",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Which-Boss-1332",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1glas2y/d_what_techniques_i_can_use_to_maintain/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1glas2y/d_what_techniques_i_can_use_to_maintain/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730931366.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?",
            "author_fullname": "t2_klkq2kea",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] YOLOv8 .pt File for General Object Detection Across Multiple Environments (50+ Classes)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gla56x",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730929743.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could someone provide the best possible .pt file for YOLOv8 for general object detection, covering environments like colleges, offices, and homes, with a dataset containing at least 50 classes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gla56x",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "MuchSand7923",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gla56x/p_yolov8_pt_file_for_general_object_detection/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gla56x/p_yolov8_pt_file_for_general_object_detection/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730929743.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'm starting a project for a client in the sports industry where the goal is to identify basketball players from vídeos, specifically what they want is the  player number, this is just the first step since then they want to be able to identify which player did a specific play, but after manually watching some vídeos it seems like identification of the number from the player's shirt is very difficult even for me as human(not good image quality and the camera sometimes is too far from the bastketball court), so I was wondering if there are any suggestions on how to tackle the problem?\n\nAny recommended models, algorithms, approaches or pipelines?(main question is how to identify the player) I was thinking  on doing something like: object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using OCR in some frame where the number is visible, but this can be very inneficient and prone to errors.",
            "author_fullname": "t2_ik41vkv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D]player identification and tracking in basketball videos(computer vision)[D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl999e",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1730928546.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730927477.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m starting a project for a client in the sports industry where the goal is to identify basketball players from vídeos, specifically what they want is the  player number, this is just the first step since then they want to be able to identify which player did a specific play, but after manually watching some vídeos it seems like identification of the number from the player&amp;#39;s shirt is very difficult even for me as human(not good image quality and the camera sometimes is too far from the bastketball court), so I was wondering if there are any suggestions on how to tackle the problem?&lt;/p&gt;\n\n&lt;p&gt;Any recommended models, algorithms, approaches or pipelines?(main question is how to identify the player) I was thinking  on doing something like: object tracking to know in every moment who are the unique players and their locations and then try to read the number from their shirt using OCR in some frame where the number is visible, but this can be very inneficient and prone to errors.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl999e",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Sad-Anywhere-2204",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl999e/dplayer_identification_and_tracking_in_basketball/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl999e/dplayer_identification_and_tracking_in_basketball/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730927477.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey everyone! 👋\n\nI’m currently working on training an Autoencoder for anomaly detection in fraudulent card transactions, but I’m hitting a roadblock. The performance has been underwhelming, with a precision-recall score barely reaching 0.20. My main goal is to achieve high recall, but I just can’t seem to make it happen.\n\nI’ve experimented with adding new features and tweaking the architecture, but nothing has improved the results significantly. For context, I’m scaling the features using MinMaxScaler. At the moment, I’m looking into implementing a combination of an Autoencoder, feature embeddings, and a Gaussian Mixture Model (GMM) to see if it boosts performance.\n\nHowever, I’m starting to wonder if Autoencoders are effective for real-world anomaly detection, or if their success is mostly limited to curated Kaggle datasets.\n\nHas anyone here worked with similar architectures and could offer some guidance? Any tips or advice would be greatly appreciated!\n\nThanks in advance!",
            "author_fullname": "t2_6dt1599d",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Struggling with Autoencoder-Based Anomaly Detection for Fraud Detection – Need Guidance",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl92zm",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730927045.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! 👋&lt;/p&gt;\n\n&lt;p&gt;I’m currently working on training an Autoencoder for anomaly detection in fraudulent card transactions, but I’m hitting a roadblock. The performance has been underwhelming, with a precision-recall score barely reaching 0.20. My main goal is to achieve high recall, but I just can’t seem to make it happen.&lt;/p&gt;\n\n&lt;p&gt;I’ve experimented with adding new features and tweaking the architecture, but nothing has improved the results significantly. For context, I’m scaling the features using MinMaxScaler. At the moment, I’m looking into implementing a combination of an Autoencoder, feature embeddings, and a Gaussian Mixture Model (GMM) to see if it boosts performance.&lt;/p&gt;\n\n&lt;p&gt;However, I’m starting to wonder if Autoencoders are effective for real-world anomaly detection, or if their success is mostly limited to curated Kaggle datasets.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here worked with similar architectures and could offer some guidance? Any tips or advice would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl92zm",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BeowulfBR",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl92zm/d_struggling_with_autoencoderbased_anomaly/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl92zm/d_struggling_with_autoencoderbased_anomaly/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730927045.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Although being almost anywhere, I keep noticing how obscure are normalization techniques, both to redditors and technicians, possibly.\n\nInstanceNorm, GroupNorm, BatchNorm, LayerNorm are all computing means, standard deviations and subsequently z-scoring the outputs (possibly followed by affine transormation).\nThey're differentiated by the axis over which statistics are computed.\n\nRMSNorm and ScaleNorm (scaled L2 Normalization) are instead \"fixing the norm\" of vectors, rescaling.\nBut this is obscuring a relation between them and LayerNorm above all others.\nIf doing LayerNorm on a d-dimensional vector, when we center (remove the mean) we're projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin; when we are rescaling centered entries, we're now limiting the vector to the \"hypercircle\" (hypersphere of d-1 dimensions) in said hyperplane.\nWe lose information on its original direction and magnitude.\nAnyway, all vectors after that have norm of sqrt(d) and entries with unit-variance.\nWhen we do RMSNorm, we skip the centering part and have norm of sqrt(d) and entries with unit-variance.\nWhen we do ScaleNorm, the norm is fixed to 1, and thus the variance is shrinked to 1/d.\nIn particular, RMSNorm and ScaleNorm are the same, modulo the scaling factor which only depends on d, and the eventually learned affines.\n\nSo when and why should we prefer unit-norm or unit-variance?\nFor example, there are \"scale-equivariant\" activations such as ReLU, and highly variant activations such as e(x) (in the sense that its slope directly depends on x).\n\nI've recently seen the nice TokenFormer paper and they seem to go to a long stretch not to write black on white that they're substituting softmax(attn_logit_of_q_i) with GeLU(RMSNorm(attn_logit_of_q_i)).\nThey sell it as scaling logits with a multiplying factor and a division with L2 norm, but it's exactly RMSNorm at initialization and they don't check if learning to move away from it actually happens and helps.\n\nAnother nice paper is the normalizedGPT, where they keep tokens on the unit-hypersphere, but kinda lament lack of specific CUDA kernels for L2norm. Is RMSNorm that much different for the use case? Probably, but how and why?\n\nWhy are we discovering and re-covering normalizations techniques and modi operandi, explaining decisions partially and post-hoc, and so on?\nI think it's important specifically when using so many softmax functions, where it actually happens that differences are more important than ratios (e.g. softmax([1,2])==softmax([11,12])!=softmax([10,20]), is it this always clear, desired, and smart?)",
            "author_fullname": "t2_fzbc9q36",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] On obscurities and missed links with Normalizations",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl5fy8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730917937.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Although being almost anywhere, I keep noticing how obscure are normalization techniques, both to redditors and technicians, possibly.&lt;/p&gt;\n\n&lt;p&gt;InstanceNorm, GroupNorm, BatchNorm, LayerNorm are all computing means, standard deviations and subsequently z-scoring the outputs (possibly followed by affine transormation).\nThey&amp;#39;re differentiated by the axis over which statistics are computed.&lt;/p&gt;\n\n&lt;p&gt;RMSNorm and ScaleNorm (scaled L2 Normalization) are instead &amp;quot;fixing the norm&amp;quot; of vectors, rescaling.\nBut this is obscuring a relation between them and LayerNorm above all others.\nIf doing LayerNorm on a d-dimensional vector, when we center (remove the mean) we&amp;#39;re projecting it to the hyperplane perpendicular to the vector of 1s and crossing the origin; when we are rescaling centered entries, we&amp;#39;re now limiting the vector to the &amp;quot;hypercircle&amp;quot; (hypersphere of d-1 dimensions) in said hyperplane.\nWe lose information on its original direction and magnitude.\nAnyway, all vectors after that have norm of sqrt(d) and entries with unit-variance.\nWhen we do RMSNorm, we skip the centering part and have norm of sqrt(d) and entries with unit-variance.\nWhen we do ScaleNorm, the norm is fixed to 1, and thus the variance is shrinked to 1/d.\nIn particular, RMSNorm and ScaleNorm are the same, modulo the scaling factor which only depends on d, and the eventually learned affines.&lt;/p&gt;\n\n&lt;p&gt;So when and why should we prefer unit-norm or unit-variance?\nFor example, there are &amp;quot;scale-equivariant&amp;quot; activations such as ReLU, and highly variant activations such as e(x) (in the sense that its slope directly depends on x).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve recently seen the nice TokenFormer paper and they seem to go to a long stretch not to write black on white that they&amp;#39;re substituting softmax(attn_logit_of_q_i) with GeLU(RMSNorm(attn_logit_of_q_i)).\nThey sell it as scaling logits with a multiplying factor and a division with L2 norm, but it&amp;#39;s exactly RMSNorm at initialization and they don&amp;#39;t check if learning to move away from it actually happens and helps.&lt;/p&gt;\n\n&lt;p&gt;Another nice paper is the normalizedGPT, where they keep tokens on the unit-hypersphere, but kinda lament lack of specific CUDA kernels for L2norm. Is RMSNorm that much different for the use case? Probably, but how and why?&lt;/p&gt;\n\n&lt;p&gt;Why are we discovering and re-covering normalizations techniques and modi operandi, explaining decisions partially and post-hoc, and so on?\nI think it&amp;#39;s important specifically when using so many softmax functions, where it actually happens that differences are more important than ratios (e.g. softmax([1,2])==softmax([11,12])!=softmax([10,20]), is it this always clear, desired, and smart?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl5fy8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Sad-Razzmatazz-5188",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl5fy8/d_on_obscurities_and_missed_links_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl5fy8/d_on_obscurities_and_missed_links_with/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730917937.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hello,\nI'm looking for a graph illustrating the inference time of language models with Flash Attention across different numbers of tokens. I looked for such a comparison on the internet but found nothing. Can anyone point me to a good source?",
            "author_fullname": "t2_m6jxh0t3",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Inference time as a function of the number of tokens when using Flash Attention.",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl4w35",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730916556.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,\nI&amp;#39;m looking for a graph illustrating the inference time of language models with Flash Attention across different numbers of tokens. I looked for such a comparison on the internet but found nothing. Can anyone point me to a good source?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl4w35",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Training-Adeptness57",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl4w35/d_inference_time_as_a_function_of_the_number_of/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl4w35/d_inference_time_as_a_function_of_the_number_of/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730916556.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "So I was looking at [flwr](https://flower.ai) for this task and I found a lot of partitioners but nothing could get the job done (I could be missing out too)\n\nHave you guys tackled such a problem?\n\nFor a better understanding, say I have four clients A, B, C and D\n\nin the normal case (given in a lot of documentations where they use CIFAR10), there is a dataset which is divided into these four clients based on some algorithm.\n\nI don't want that, what I have is basically a an already divided dataset (train/test division not yet done) according to the client (A/B/C/D) and I want to run a simulation in this kind of an environment\n\nAny help will be appreciated!",
            "author_fullname": "t2_67alspap",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] How to run a Federated Learning simulation on a custom dataset where I already have dataset partitioned for each client?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl3wto",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730914097.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I was looking at &lt;a href=\"https://flower.ai\"&gt;flwr&lt;/a&gt; for this task and I found a lot of partitioners but nothing could get the job done (I could be missing out too)&lt;/p&gt;\n\n&lt;p&gt;Have you guys tackled such a problem?&lt;/p&gt;\n\n&lt;p&gt;For a better understanding, say I have four clients A, B, C and D&lt;/p&gt;\n\n&lt;p&gt;in the normal case (given in a lot of documentations where they use CIFAR10), there is a dataset which is divided into these four clients based on some algorithm.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want that, what I have is basically a an already divided dataset (train/test division not yet done) according to the client (A/B/C/D) and I want to run a simulation in this kind of an environment&lt;/p&gt;\n\n&lt;p&gt;Any help will be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?auto=webp&amp;s=f82d98a3e5b6e07510d82b0b10bd06f8b0c60fa6",
                            "width": 1200,
                            "height": 628
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb6afd4c233b0817985e10d4c9f2dbae715b0ca5",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=447c73039538fc794e59dbaf7f75332e17f26b3e",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=94e2587944f848cb6056f90b47332f87f0367767",
                                "width": 320,
                                "height": 167
                            },
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=574ef344ac927fec37cba67c4acc69b58f81a45d",
                                "width": 640,
                                "height": 334
                            },
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6256d1bbb07b1f9756630d36919b82b19074502",
                                "width": 960,
                                "height": 502
                            },
                            {
                                "url": "https://external-preview.redd.it/8LlG80dqLnxuzqAk-qsvDnRbgTFKrll3OqPWV9tnyBw.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57626cc7b9de6978d5b6fb4fef839812e7961933",
                                "width": 1080,
                                "height": 565
                            }
                        ],
                        "variants": {},
                        "id": "XAqUrWltJs5ssBtwaE7H6Vsykm3SDNN_jZgEbVKC83k"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl3wto",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "lel_73",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl3wto/d_how_to_run_a_federated_learning_simulation_on_a/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl3wto/d_how_to_run_a_federated_learning_simulation_on_a/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730914097.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I'd like to hear your opinion on this new paradigm of interacting with LLMs.   \nIn particular, I'm talking about \"simple\" stuff like Reflection (like Self-refine and Reflexion), up to more complex stuff like Self-Ask, Self-RAG, DSP, or even Agentic LLMs.   \nI've read a couple of surveys about these topics and I'm reading each of the aforementioned papers, but everything seems quite foggy to me. I can understand the inner workings of a simple RAG pipeline with in-context-learning and frozen LLMs, but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate. ",
            "author_fullname": "t2_c8klj3k6",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] [R] Problems understanding DSP-like pipelines",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl3lh1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730913310.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to hear your opinion on this new paradigm of interacting with LLMs.&lt;br/&gt;\nIn particular, I&amp;#39;m talking about &amp;quot;simple&amp;quot; stuff like Reflection (like Self-refine and Reflexion), up to more complex stuff like Self-Ask, Self-RAG, DSP, or even Agentic LLMs.&lt;br/&gt;\nI&amp;#39;ve read a couple of surveys about these topics and I&amp;#39;m reading each of the aforementioned papers, but everything seems quite foggy to me. I can understand the inner workings of a simple RAG pipeline with in-context-learning and frozen LLMs, but adding all these layers of abstractions and interactions make everything so damn complicated not only to understand but even to replicate. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl3lh1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Debonargon",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl3lh1/d_r_problems_understanding_dsplike_pipelines/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl3lh1/d_r_problems_understanding_dsplike_pipelines/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730913310.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: [https://github.com/PavleMiha/mlgarden](https://github.com/PavleMiha/mlgarden)\n\nDoes this seem useful to you? Is this something you'd play around with? I can't really figure out what to do with it, so I'm curious to hear the community's thoughts!",
            "author_fullname": "t2_39bej",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] I made a tool for building and training neural networks visually, operation by operation ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl30b0",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 30,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 30,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730911856.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I mostly made this as a tool to learn how to implement backpropagation and get some intuition on how it works, so I figure it might be useful for someone else! I also wrote up an article in the readme on how backpropagation and model training works: &lt;a href=\"https://github.com/PavleMiha/mlgarden\"&gt;https://github.com/PavleMiha/mlgarden&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Does this seem useful to you? Is this something you&amp;#39;d play around with? I can&amp;#39;t really figure out what to do with it, so I&amp;#39;m curious to hear the community&amp;#39;s thoughts!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?auto=webp&amp;s=0ee689136b02e7b9cb358b55ba6b2274ebed84ce",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=290abe523e68a5900a89eacb63177c1dfb000516",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e7d13b5dc147d1759360eb2cb535853d70c99d8",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8a58be35ac3b05ac860e54ff82726be59d6476ee",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7b0774c5721afe94dc295ff25a99b05f0374f087",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a74f658b71c0aa7b6a9d472171dae8e3737e636",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/1J9bichgDuF9ngg6ot9Fu6P-xPwqyEwc1yfcpvQ8clA.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=602929fe61ce96ebcf14f8d5660b31634d125802",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "Fw3ArvUGSF8xQIt_gfojrJZk0iwaov8kFcEAe9ex50E"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl30b0",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Massena",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl30b0/p_i_made_a_tool_for_building_and_training_neural/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl30b0/p_i_made_a_tool_for_building_and_training_neural/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730911856.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "If any of you has tried different LLMs, I am super curious which one did you find works great for analysing Financials, P/Ls, Balance Sheets for a company?\n\nI am looking to use it regularly so it'd be great if you tried any specific LLMs that you found they work good with reasoning, actually analysing the numbers properly and giving insights on them.\n\nThank you!",
            "author_fullname": "t2_e3vz6b7g",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Which LLM do you use for analysing Financials, P &amp; Ls, Balance Sheets?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gl0cnl",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730904946.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If any of you has tried different LLMs, I am super curious which one did you find works great for analysing Financials, P/Ls, Balance Sheets for a company?&lt;/p&gt;\n\n&lt;p&gt;I am looking to use it regularly so it&amp;#39;d be great if you tried any specific LLMs that you found they work good with reasoning, actually analysing the numbers properly and giving insights on them.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gl0cnl",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "eaerdiablosios",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gl0cnl/d_which_llm_do_you_use_for_analysing_financials_p/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gl0cnl/d_which_llm_do_you_use_for_analysing_financials_p/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730904946.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Title sums it up. I'm mostly familiar with time series prediction models, as that's what I've spent most of my time building (I'm a data analyst that's recently built some cool ML stuff). But I need to build a recommendation engine for my employer who has an ecommerce site and sells physical products.\n\nI know the first step is data collection about the users. My question to you all is:\n\nWhere should I store this data that I collect (Datalake, Relational Database, etc)?\n\nHow do I go about picking an algorithm (I'm used to using LSTM and Local Bayesian for time series)?\n\nAnd what are some general rules and advice from those who have built something like this before?\n\nYou all are awesome. Thanks for your help!",
            "author_fullname": "t2_1bgp2ofsx6",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Need Advice Starting my Recommendation Engine Project for my Employer",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkzgu6",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730902514.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Title sums it up. I&amp;#39;m mostly familiar with time series prediction models, as that&amp;#39;s what I&amp;#39;ve spent most of my time building (I&amp;#39;m a data analyst that&amp;#39;s recently built some cool ML stuff). But I need to build a recommendation engine for my employer who has an ecommerce site and sells physical products.&lt;/p&gt;\n\n&lt;p&gt;I know the first step is data collection about the users. My question to you all is:&lt;/p&gt;\n\n&lt;p&gt;Where should I store this data that I collect (Datalake, Relational Database, etc)?&lt;/p&gt;\n\n&lt;p&gt;How do I go about picking an algorithm (I&amp;#39;m used to using LSTM and Local Bayesian for time series)?&lt;/p&gt;\n\n&lt;p&gt;And what are some general rules and advice from those who have built something like this before?&lt;/p&gt;\n\n&lt;p&gt;You all are awesome. Thanks for your help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkzgu6",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Lower-Feeling2752",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkzgu6/d_need_advice_starting_my_recommendation_engine/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkzgu6/d_need_advice_starting_my_recommendation_engine/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730902514.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I have seen techniques to transfer/effectively let one model teach another model its unique capability/domain knowledge. But can this made possible for language capability as well? \n\nFor example, if we have a model that is proficient in Chinese, is there any way to transfer/teach that Chinese proficiency to another model without us having access to the original Chinese corpus used to train the teacher model?\n\nAny insights would be greatly appreciated, thank you beforehand!",
            "author_fullname": "t2_8usqdvwp",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Can we transfer language capabilities of one LLM to another?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkzg9l",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.43,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730902469.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have seen techniques to transfer/effectively let one model teach another model its unique capability/domain knowledge. But can this made possible for language capability as well? &lt;/p&gt;\n\n&lt;p&gt;For example, if we have a model that is proficient in Chinese, is there any way to transfer/teach that Chinese proficiency to another model without us having access to the original Chinese corpus used to train the teacher model?&lt;/p&gt;\n\n&lt;p&gt;Any insights would be greatly appreciated, thank you beforehand!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkzg9l",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "worthlesspineapple",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkzg9l/d_can_we_transfer_language_capabilities_of_one/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkzg9l/d_can_we_transfer_language_capabilities_of_one/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730902469.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Came across this interesting paper being presented next week at EMNLP 2024: *LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints*.\n\nThis study dives into an important question: **Do LLMs really do what we ask them to?** We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.\n\nThe authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. **They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow**. In their results **even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested**. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.\n\nTo address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. **Initial tests showed that LLMs couldn’t self-correct reliably alone**, however with weak but minimally reliable auxiliary feedback, **they achieved up to an 8% boost**. **With high-quality “ideal” feedback, DECRIM brought Mistral’s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.**\n\nI think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? \n\nPaper: [https://arxiv.org/pdf/2410.06458](https://arxiv.org/pdf/2410.06458)\n\n[Their post on Linkedin](https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&amp;utm_medium=member_desktop)\n\nhttps://preview.redd.it/techjo8pfazd1.png?width=2794&amp;format=png&amp;auto=webp&amp;s=18155cdbf4ba164f48480d4583c3cfea1d40298e\n\n",
            "author_fullname": "t2_b828vn1w0",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Amazon Researchers Find LLMs do not always follow User Requests and Propose a Self-Correction Pipeline",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": 51,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
                "techjo8pfazd1": {
                    "status": "valid",
                    "e": "Image",
                    "m": "image/png",
                    "p": [
                        {
                            "y": 39,
                            "x": 108,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c628a469998f6bfbd9519fcde5d76c0e92548e8"
                        },
                        {
                            "y": 78,
                            "x": 216,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a0b23de9fbcc6e004fe318a8c6463cfe646a9dab"
                        },
                        {
                            "y": 116,
                            "x": 320,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ba29ea64f5d6071efe5a5c17e48aa5d07718cab8"
                        },
                        {
                            "y": 233,
                            "x": 640,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8ea56ad22a0510b608ac15526df725a6dc76fc45"
                        },
                        {
                            "y": 350,
                            "x": 960,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc1a812d638ba9d5e824f1f60694eb75c3dfed70"
                        },
                        {
                            "y": 393,
                            "x": 1080,
                            "u": "https://preview.redd.it/techjo8pfazd1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c80a5b50719501721eb88a7b6b399cba603a85a8"
                        }
                    ],
                    "s": {
                        "y": 1019,
                        "x": 2794,
                        "u": "https://preview.redd.it/techjo8pfazd1.png?width=2794&amp;format=png&amp;auto=webp&amp;s=18155cdbf4ba164f48480d4583c3cfea1d40298e"
                    },
                    "id": "techjo8pfazd1"
                }
            },
            "name": "t3_1gkzac4",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.84,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 42,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 42,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/bb-ElM03_gsi5MEm-cJniCpLo0hAWXjIwC055MErWL8.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730902001.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Came across this interesting paper being presented next week at EMNLP 2024: &lt;em&gt;LLM Self-Correction with DECRIM: DECOMPOSE, CRITIQUE, AND REFINE for Enhanced Following of Instructions with Multiple Constraints&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;This study dives into an important question: &lt;strong&gt;Do LLMs really do what we ask them to?&lt;/strong&gt; We often rely on LLMs for tasks with specific instructions, but when these instructions get complex and multi-constrained, like requesting specific tones or avoiding certain words, do LLMs actually follow through? This paper suggests that the answer might be more complicated than we think.&lt;/p&gt;\n\n&lt;p&gt;The authors created a new benchmark, RealInstruct, which uses real-world user instructions rather than synthetic prompts. &lt;strong&gt;They estimated that at least 30% of real user requests contain multiple constraints that LLMs must follow&lt;/strong&gt;. In their results &lt;strong&gt;even advanced models like GPT-4 fail to meet at least one requirement over 21% of the instructions tested&lt;/strong&gt;. So, while LLMs perform well in simple cases, their performance drops when handling more intricate, multi-step requests.&lt;/p&gt;\n\n&lt;p&gt;To address these gaps, the authors developed a self-correction pipeline called DECRIM, where the model breaks down each instruction, checks its response against each requirement, and iteratively refines it as needed. Through DECRIM, open-source models like Mistral saw notable improvements, even surpassing GPT-4 on the benchmarks. &lt;strong&gt;Initial tests showed that LLMs couldn’t self-correct reliably alone&lt;/strong&gt;, however with weak but minimally reliable auxiliary feedback, &lt;strong&gt;they achieved up to an 8% boost&lt;/strong&gt;. &lt;strong&gt;With high-quality “ideal” feedback, DECRIM brought Mistral’s performance up by 34%, surpassing GPT-4 on both RealInstruct and IFEval benchmarks.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I think this paper fits in a new trend on LLMs, these System 2 Reasoning models like GPT-o1 that try to mimic some thinking / reflection before outputting their response. Anyway it is shocking that LLMs perform that bad in a task that seems simply the most important ones for the user, following what the users ask. Is this type of model making us closer to AGI? Or is this just proving that this magic AGI that some people talk about is actually much much far away yet? &lt;/p&gt;\n\n&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/pdf/2410.06458\"&gt;https://arxiv.org/pdf/2410.06458&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.linkedin.com/posts/thomasferraz_emnlp2024-ai-llms-activity-7259680754299731968-uLBk?utm_source=share&amp;amp;utm_medium=member_desktop\"&gt;Their post on Linkedin&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/techjo8pfazd1.png?width=2794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18155cdbf4ba164f48480d4583c3cfea1d40298e\"&gt;https://preview.redd.it/techjo8pfazd1.png?width=2794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18155cdbf4ba164f48480d4583c3cfea1d40298e&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkzac4",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Mundane_Sir_7505",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkzac4/r_amazon_researchers_find_llms_do_not_always/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkzac4/r_amazon_researchers_find_llms_do_not_always/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730902001.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi all, I’m working on a self-supervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset, inspired by matrix completion methods.\n\nThe dataset is generated from simulated freeway traffic, discretized in time and space to form a grid of cells. Each cell reflects a traffic density value observed from mobile sensors. I have three core arrays:\n\n1. **actual\\_density\\_values**: Ground truth density, used only for evaluation, not training.\n2. **observed\\_density\\_values**: Traffic density observed from mobile sensors, with some cells unobserved.\n3. **certainty\\_values**: Coverage certainty for each observed cell (range: 0 to 1).\n\nwith dimensions (T, E, S, L), where:\n\n* **T**: Number of time steps\n* **E**: Movement directions (edges) – expected to be 2 (e.g., forward and reverse)\n* **S**: Spatial segments\n* **L**: Lanes\n\n\n\n**Goal**\n\nThe goal here is to build a model that can improve the estimation for cells where the certainty is less than 1. I want the model to capture dependencies over time and space, using self-supervision to “fill in” unobserved or uncertain values more accurately.\n\n\n\n**Proposed Approach**\n\nHere’s what I’m thinking in terms of architecture:\n\n1. **Temporal Dependencies**: Using a CNN to capture time-based dependencies over time steps (T).\n2. **Spatial Dependencies**: Using an RNN to model dependencies across spatial segments (S) and lanes (L).\n3. **Model Structure**:\n   * **Data Masking**: At each time step, mask some of the observed data, especially the lower-certainty cells, so the model learns to predict uncertain values better.\n   * **CNN-RNN Combo**: Combining CNN and RNN layers to learn from both the temporal and spatial aspects.\n   * **Loss Function**: Using a self-supervised loss function that prioritizes accurate reconstruction of observed densities, particularly focusing on uncertain cells. For training, I won’t use the ground truth array (actual\\_density\\_values); it’s only for evaluation.\n4. **Evaluation**: Once trained, I plan to compute the RMSPE (Root Mean Square Percentage Error) between actual\\_density\\_values and the model’s predicted observed\\_density\\_values. I’m especially interested in the improvements on the lower-certainty cells.\n\n\n\n**Question**\n\n1. Does this CNN-RNN combination sound like a good fit for this kind of matrix completion task? Are there alternative approaches or tweaks that might make it more effective?\n2. Any recommendations for loss functions that work well in self-supervised setups, especially where I want to prioritize low-certainty values?\n3. Are there best practices for masking observed values in self-supervised learning setups like this?\n4. Any advice on regularization techniques to prevent overfitting, given the self-supervised nature of the task? Also, any tips on ensuring scalability?",
            "author_fullname": "t2_locz2v76y",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Help with CNN-RNN Architecture for Self-Supervised Matrix Completion",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkxnak",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 1.0,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730897095.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I’m working on a self-supervised learning approach to estimate missing or uncertain data in a freeway traffic density dataset, inspired by matrix completion methods.&lt;/p&gt;\n\n&lt;p&gt;The dataset is generated from simulated freeway traffic, discretized in time and space to form a grid of cells. Each cell reflects a traffic density value observed from mobile sensors. I have three core arrays:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;actual_density_values&lt;/strong&gt;: Ground truth density, used only for evaluation, not training.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;observed_density_values&lt;/strong&gt;: Traffic density observed from mobile sensors, with some cells unobserved.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;certainty_values&lt;/strong&gt;: Coverage certainty for each observed cell (range: 0 to 1).&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;with dimensions (T, E, S, L), where:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;T&lt;/strong&gt;: Number of time steps&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;: Movement directions (edges) – expected to be 2 (e.g., forward and reverse)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;S&lt;/strong&gt;: Spatial segments&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;: Lanes&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The goal here is to build a model that can improve the estimation for cells where the certainty is less than 1. I want the model to capture dependencies over time and space, using self-supervision to “fill in” unobserved or uncertain values more accurately.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Proposed Approach&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Here’s what I’m thinking in terms of architecture:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Temporal Dependencies&lt;/strong&gt;: Using a CNN to capture time-based dependencies over time steps (T).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Spatial Dependencies&lt;/strong&gt;: Using an RNN to model dependencies across spatial segments (S) and lanes (L).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model Structure&lt;/strong&gt;:\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Data Masking&lt;/strong&gt;: At each time step, mask some of the observed data, especially the lower-certainty cells, so the model learns to predict uncertain values better.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CNN-RNN Combo&lt;/strong&gt;: Combining CNN and RNN layers to learn from both the temporal and spatial aspects.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Loss Function&lt;/strong&gt;: Using a self-supervised loss function that prioritizes accurate reconstruction of observed densities, particularly focusing on uncertain cells. For training, I won’t use the ground truth array (actual_density_values); it’s only for evaluation.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Once trained, I plan to compute the RMSPE (Root Mean Square Percentage Error) between actual_density_values and the model’s predicted observed_density_values. I’m especially interested in the improvements on the lower-certainty cells.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Question&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Does this CNN-RNN combination sound like a good fit for this kind of matrix completion task? Are there alternative approaches or tweaks that might make it more effective?&lt;/li&gt;\n&lt;li&gt;Any recommendations for loss functions that work well in self-supervised setups, especially where I want to prioritize low-certainty values?&lt;/li&gt;\n&lt;li&gt;Are there best practices for masking observed values in self-supervised learning setups like this?&lt;/li&gt;\n&lt;li&gt;Any advice on regularization techniques to prevent overfitting, given the self-supervised nature of the task? Also, any tips on ensuring scalability?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkxnak",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "NoTheme6450",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkxnak/r_help_with_cnnrnn_architecture_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkxnak/r_help_with_cnnrnn_architecture_for/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730897095.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi Folks,\n\nI come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.\n\nThus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?\n\nTLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.",
            "author_fullname": "t2_hb1rf8uy",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Want to move away from coding heavy ML but still want to complete the PhD",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkx6o7",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 78,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 78,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1730896518.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730895521.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt;\n\n&lt;p&gt;I come from a tradition electrical engineering background doing things like industrial automation and computer vision. I decided to pursue a PhD in ML as I thought it will be a good field to enter given my past experience. Now I have been doing the PhD for the past three years. While I like my group and research, I am getting discouraged/depressed by (1) The publication rat race (2) post graduation opportunities mostly being coding heavy (3) the inability to carve a name for myself in the field given how crowded the field has become.&lt;/p&gt;\n\n&lt;p&gt;Thus, ideally I would like to complete my PhD and move into a more relaxed paced (even if it is not as high paying as ML jobs) non coding heavy but technical job, where I do not have to constantly up-skill myself. Do you folks have any suggestion on what jobs I can look into or would you suggest dropping the PhD and doing something else?&lt;/p&gt;\n\n&lt;p&gt;TLDR: 4th year ML PhD student unsure of sticking with the PhD as they desire a non coding heavy technical job in the industry post graduation. Seeking advice on what to do.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkx6o7",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Hopeful-Reading-6774",
            "discussion_type": null,
            "num_comments": 53,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkx6o7/d_want_to_move_away_from_coding_heavy_ml_but/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkx6o7/d_want_to_move_away_from_coding_heavy_ml_but/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730895521.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "As AI models continue to scale in both complexity and size, I'm interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?",
            "author_fullname": "t2_akvqkrf0",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Evolving Matrix Computation Techniques for Modern AI: What's New?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkwpht",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 24,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 24,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730893895.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As AI models continue to scale in both complexity and size, I&amp;#39;m interested in how the field of matrix computations is evolving to meet these new challenges. What are some of the latest advancements or strategies in matrix computation that are improving efficiency and adaptability for modern AI systems? Are there any recent breakthroughs or shifts in our approach to these computations that are making a significant impact in AI research and applications?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkwpht",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Glittering_Age7553",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkwpht/d_evolving_matrix_computation_techniques_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkwpht/d_evolving_matrix_computation_techniques_for/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730893895.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "is there any reason to train llms to predict only one token? like wouldnt inference be 2 times faster if it was trained to predict just 2? thats huge gain , sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway, will having llm predict more than 1 token decrease it more?",
            "author_fullname": "t2_19b7tdum2o",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] What if llm's are trained to predict more than 1 token at a time? ",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkuf8y",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.35,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730884299.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;is there any reason to train llms to predict only one token? like wouldnt inference be 2 times faster if it was trained to predict just 2? thats huge gain , sure there can be performance loss but for inference we already do quantization to increase speed which decreases performance anyway, will having llm predict more than 1 token decrease it more?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkuf8y",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "limitless_11111",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkuf8y/d_what_if_llms_are_trained_to_predict_more_than_1/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkuf8y/d_what_if_llms_are_trained_to_predict_more_than_1/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730884299.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Being a PhD student, much of my time is spent on supervising students, project management and writing \"quick and dirty\" code for prototyping. I intend to move to industry after the PhD, but I feel like I'm missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? ",
            "author_fullname": "t2_73ct2mwg",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] As a researcher, how do you become industry-ready?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gksoi7",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.94,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 150,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 150,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730876843.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Being a PhD student, much of my time is spent on supervising students, project management and writing &amp;quot;quick and dirty&amp;quot; code for prototyping. I intend to move to industry after the PhD, but I feel like I&amp;#39;m missing out on key software engineering skills and good coding practices. Does anyone else feel this way? How do you upskill yourself to be industry-ready while doing a PhD? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gksoi7",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "fullgoopy_alchemist",
            "discussion_type": null,
            "num_comments": 42,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gksoi7/d_as_a_researcher_how_do_you_become_industryready/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gksoi7/d_as_a_researcher_how_do_you_become_industryready/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730876843.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I've been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewAI, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what's going behind the curtains even for very simple stuff.\n\n[So I just published this open-source framework GenSphere.](https://github.com/octopus2023-inc/gensphere) The idea is have something like **Docker for LLMs**. You build applications with YAML files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you don't lose control.\n\nYou basically code in YAML, stating what are the tasks that need to be done and how they connect. Other than that, you only write individual python functions to be called during the execution. No new classes and abstractions to learn.\n\nIts all open-source. **Now I'm looking for contributors** to adapt the framework for cycles and conditional nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are tons of things to do!\n\nPS: [you can read the detailed docs here,](https://gensphere.readthedocs.io/en/latest/) And go over this quick [Google Colab tutorial.](https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutorial.ipynb)",
            "author_fullname": "t2_107tk82vkc",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Open-source declarative framework to build LLM applications - looking for contributors",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkpazh",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730864091.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been building LLM-based applications, and was super frustated with all major frameworks - langchain, autogen, crewAI, etc. They also seem to introduce a pile of unnecessary abstractions. It becomes super hard to understand what&amp;#39;s going behind the curtains even for very simple stuff.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/octopus2023-inc/gensphere\"&gt;So I just published this open-source framework GenSphere.&lt;/a&gt; The idea is have something like &lt;strong&gt;Docker for LLMs&lt;/strong&gt;. You build applications with YAML files, that define an execution graph. Nodes can be either LLM API calls, regular function executions or other graphs themselves. Because you can nest graphs easily, building complex applications is not an issue, but at the same time you don&amp;#39;t lose control.&lt;/p&gt;\n\n&lt;p&gt;You basically code in YAML, stating what are the tasks that need to be done and how they connect. Other than that, you only write individual python functions to be called during the execution. No new classes and abstractions to learn.&lt;/p&gt;\n\n&lt;p&gt;Its all open-source. &lt;strong&gt;Now I&amp;#39;m looking for contributors&lt;/strong&gt; to adapt the framework for cycles and conditional nodes - which would allow full-fledged agentic system building! Pls reach out  if you want to contribute, there are tons of things to do!&lt;/p&gt;\n\n&lt;p&gt;PS: &lt;a href=\"https://gensphere.readthedocs.io/en/latest/\"&gt;you can read the detailed docs here,&lt;/a&gt; And go over this quick &lt;a href=\"https://github.com/octopus2023-inc/gensphere/blob/main/examples/gensphere_tutorial.ipynb\"&gt;Google Colab tutorial.&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?auto=webp&amp;s=4736e4889a0299189de6ede11c8ff2703b08be1b",
                            "width": 1200,
                            "height": 600
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=aefdb93baee4cbff3c04f9b32b0b993472516015",
                                "width": 108,
                                "height": 54
                            },
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d33571830d3ff701a14a6a0c2070d8a4b55ccb66",
                                "width": 216,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=8ce95b8cf84ff26b634c6a522fbe9533f4184465",
                                "width": 320,
                                "height": 160
                            },
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=24ca9c7c7cefb3869bddcb6b16f796433d45e1a2",
                                "width": 640,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ac0ea4225107d555c24463e5ef028792486f40d",
                                "width": 960,
                                "height": 480
                            },
                            {
                                "url": "https://external-preview.redd.it/ZE06MUYpgZ_1Q7JLHe7qRMq6pNkUyc2HpqJCiqH3Zoc.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9ef2d0cbe2d01ce32ab60c230dae1f9af199ebd8",
                                "width": 1080,
                                "height": 540
                            }
                        ],
                        "variants": {},
                        "id": "cxDAOl-gRe_JgZo1DupH-7HN3_4KHi6Pa7tc1_8LrOU"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkpazh",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Jazzlike_Tooth929",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkpazh/p_opensource_declarative_framework_to_build_llm/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730864091.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi, i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better, i only applied to full shakespeare\\_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this, i am still going to release a paper since i get good results and done some math work, should i do it?",
            "author_fullname": "t2_1at7vcth63",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "On a successful research with low budget [D]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkn6tw",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730857025.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i got a research idea and applied it on nanogpt repo for lm training and validated transformer generalizes better on validation loss but worse training loss and is more prone to overfitting since like training loss a little worse validation loss a little better, i only applied to full shakespeare_char and a subset on openwebtext bcz 10 usd on runpod only allows me to do this, i am still going to release a paper since i get good results and done some math work, should i do it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkn6tw",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Mean-Force267",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkn6tw/on_a_successful_research_with_low_budget_d/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkn6tw/on_a_successful_research_with_low_budget_d/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730857025.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Just recently saw Autograd(library) by google people that thinly wraps numpy to offer backprop. JAX also does this but rewrites numpy basically. What’s the difference? Is it the gpu tpu support of JAX? is autograd meant for smaller models? ",
            "author_fullname": "t2_ajhvhwf1",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Autograd vs JAX? Both are google products aimed at gradient based methods. What’s the main difference? (GPU/TPU?)",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkms4w",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 14,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 14,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1730860341.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730855713.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently saw Autograd(library) by google people that thinly wraps numpy to offer backprop. JAX also does this but rewrites numpy basically. What’s the difference? Is it the gpu tpu support of JAX? is autograd meant for smaller models? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkms4w",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "MysticalDragoneer",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkms4w/d_autograd_vs_jax_both_are_google_products_aimed/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkms4w/d_autograd_vs_jax_both_are_google_products_aimed/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730855713.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Just read that Kinetic Seas launched a new AI-specific data center—sounds like they’re aiming to make model training and fine-tuning less of a headache. Their setup includes specialized GPUs and CPUs, supposedly built to handle the demands of large, complex models. If traditional data centers feel like running uphill, maybe these AI-specific centers are the downhill version?\n\nWith machine learning models becoming more resource-hungry, I wonder if optimized infrastructure like this might change the game. Think about it: training models faster and with fewer limitations could really boost productivity for researchers and data scientists. Kinetic Seas seems to believe it’s worth building infrastructure just for AI, which feels like a pretty interesting bet.\n\nHas anyone here worked with AI-specific setups like this? Curious to know if it’s really as smooth as it sounds!\n\n[https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html](https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html)",
            "author_fullname": "t2_pz6sh",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] The Role of Dedicated AI Data Centers in Enhancing Model Training and Fine-Tuning",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkmen4",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730854559.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just read that Kinetic Seas launched a new AI-specific data center—sounds like they’re aiming to make model training and fine-tuning less of a headache. Their setup includes specialized GPUs and CPUs, supposedly built to handle the demands of large, complex models. If traditional data centers feel like running uphill, maybe these AI-specific centers are the downhill version?&lt;/p&gt;\n\n&lt;p&gt;With machine learning models becoming more resource-hungry, I wonder if optimized infrastructure like this might change the game. Think about it: training models faster and with fewer limitations could really boost productivity for researchers and data scientists. Kinetic Seas seems to believe it’s worth building infrastructure just for AI, which feels like a pretty interesting bet.&lt;/p&gt;\n\n&lt;p&gt;Has anyone here worked with AI-specific setups like this? Curious to know if it’s really as smooth as it sounds!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html\"&gt;https://www.prnewswire.com/news-releases/kinetic-seas-fka-bellatora-announces-completion-of-phase-i-of-its-data-center-for-ai-302168707.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?auto=webp&amp;s=d491439bb6bc74fec253e3e348460e2b3044fbff",
                            "width": 1500,
                            "height": 785
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd308ca207c0dea078a3a2fb9991745f40e71631",
                                "width": 108,
                                "height": 56
                            },
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6610f506b3a75ed6e040f96b34d0de41c691a502",
                                "width": 216,
                                "height": 113
                            },
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=85e988f5b6c9f3aaf13a7387b670c0b97fb235cc",
                                "width": 320,
                                "height": 167
                            },
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9a4515169cb1e88ee6bda9fe221e1410ed157200",
                                "width": 640,
                                "height": 334
                            },
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a21b3d1c6d3613dc013b51b6977cc15238b2d1fb",
                                "width": 960,
                                "height": 502
                            },
                            {
                                "url": "https://external-preview.redd.it/YQZ-Ph9dFSogH2vammK5Gl9KFg1PMWUgHazsp08R4q8.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=81274cb097a9f7731dafb05d40dca0a1e766ea40",
                                "width": 1080,
                                "height": 565
                            }
                        ],
                        "variants": {},
                        "id": "naNWmRUI_hSeZl5_B2ipdaO86aEo2LkWaHV6f9uxfRc"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkmen4",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "booboo1998",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkmen4/d_the_role_of_dedicated_ai_data_centers_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkmen4/d_the_role_of_dedicated_ai_data_centers_in/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730854559.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi guys!\nI was thinking that, if we could dynamically merge LLM fine-tuning LoRAs depending on type of task at hand, we could fix catastrophic forgetting and maybe even have transformers better able to generalize.\nThe thing is, due to Attention layers being very very non linear on their weights, transformers show poor LMC (linear mode connectivity).\n\nAre you aware of the computational complexity of exact LoRA merging? I have seen quite a lot of papers on the subject of LoRA merging but they seem of poor quality and only empirical, with little mathematical grounding.\n\nSo if you guys have thought of it, I'd be glad to hear about it!",
            "author_fullname": "t2_16aic04ri8",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Is LoRA merging (and non linear mode connectivity) the key to better transformer hypernets?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkhy4n",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.73,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1730934409.0,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730842317.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!\nI was thinking that, if we could dynamically merge LLM fine-tuning LoRAs depending on type of task at hand, we could fix catastrophic forgetting and maybe even have transformers better able to generalize.\nThe thing is, due to Attention layers being very very non linear on their weights, transformers show poor LMC (linear mode connectivity).&lt;/p&gt;\n\n&lt;p&gt;Are you aware of the computational complexity of exact LoRA merging? I have seen quite a lot of papers on the subject of LoRA merging but they seem of poor quality and only empirical, with little mathematical grounding.&lt;/p&gt;\n\n&lt;p&gt;So if you guys have thought of it, I&amp;#39;d be glad to hear about it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkhy4n",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Due-Pangolin325",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkhy4n/d_is_lora_merging_and_non_linear_mode/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkhy4n/d_is_lora_merging_and_non_linear_mode/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730842317.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi all,\n\nApologies if this is the wrong place to post. I'm looking for tools that can help me support my partner, who has been harassed for a number of years by her ex and father of her child.  \n  \n  \nShe is trying to compile evidence for a restraining order but going back through the years of emails and other messages is psychologically draining for her. I was wondering if there are any tools that have a good use case for analysing and classifying emails, either individually or in bulk, so that I can support her by taking over this work for her?",
            "author_fullname": "t2_d0atd",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Tools to classify emails - supporting DV victims [Discussion]",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkgmwo",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730838976.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Apologies if this is the wrong place to post. I&amp;#39;m looking for tools that can help me support my partner, who has been harassed for a number of years by her ex and father of her child.  &lt;/p&gt;\n\n&lt;p&gt;She is trying to compile evidence for a restraining order but going back through the years of emails and other messages is psychologically draining for her. I was wondering if there are any tools that have a good use case for analysing and classifying emails, either individually or in bulk, so that I can support her by taking over this work for her?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkgmwo",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BunsenFurner87",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkgmwo/tools_to_classify_emails_supporting_dv_victims/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkgmwo/tools_to_classify_emails_supporting_dv_victims/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730838976.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone! 😊 I just published an article: [Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models](https://medium.com/@bartek.lazarowicz/mastering-llm-testing-ensuring-accuracy-ethics-and-future-readiness-for-next-gen-ai-models-adc85799efca). I hope I didn’t miss anything important in there!\n\nI’m planning to turn this into a series on AI model testing and testing in general. Hope you enjoy it, and I’m always open for feedback and discussion! 😄",
            "author_fullname": "t2_drp5g",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkfa6n",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730835535.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! 😊 I just published an article: &lt;a href=\"https://medium.com/@bartek.lazarowicz/mastering-llm-testing-ensuring-accuracy-ethics-and-future-readiness-for-next-gen-ai-models-adc85799efca\"&gt;Mastering LLM Testing: Ensuring Accuracy, Ethics, and Future-Readiness for Next-Gen AI Models&lt;/a&gt;. I hope I didn’t miss anything important in there!&lt;/p&gt;\n\n&lt;p&gt;I’m planning to turn this into a series on AI model testing and testing in general. Hope you enjoy it, and I’m always open for feedback and discussion! 😄&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
                "images": [
                    {
                        "source": {
                            "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?auto=webp&amp;s=b78c184c0ab268a5290dede6405abcd3521776de",
                            "width": 1024,
                            "height": 1024
                        },
                        "resolutions": [
                            {
                                "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1237159fec14b521bed9cfa5c925904717900984",
                                "width": 108,
                                "height": 108
                            },
                            {
                                "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=89247ecaf679e95d6ccaad55f3c037f91134bfe2",
                                "width": 216,
                                "height": 216
                            },
                            {
                                "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6d375924475b9f417796022247ad3b9b2043c37",
                                "width": 320,
                                "height": 320
                            },
                            {
                                "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=54b038c4d3574d837c1a911724550b21634c3fe1",
                                "width": 640,
                                "height": 640
                            },
                            {
                                "url": "https://external-preview.redd.it/XM2y_jQbTxSXedCfTboPmRh2E7y9mLQn-tYF_BnXWsg.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f4c03483d0517b1a68b3ce48e81351dd751eaf1",
                                "width": 960,
                                "height": 960
                            }
                        ],
                        "variants": {},
                        "id": "7BF-X2jl75faFrxDfPVFNz4_FOJehjCZPoDVzA6HxXA"
                    }
                ],
                "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkfa6n",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "tukan90",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkfa6n/d_mastering_llm_testing_ensuring_accuracy_ethics/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkfa6n/d_mastering_llm_testing_ensuring_accuracy_ethics/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730835535.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hey r/MachineLearning! 👋\n\nI’m doing some research to understand the key challenges people face when managing multiple AI models—particularly around scaling, monitoring performance, and handling model failures. I’d love to hear from the community to get a better sense of where the pain points are.\n\nHere are a few questions to start:\n\n1. **Scaling and Load Balancing**: Do you find it difficult to scale models for high traffic or load balancing between models?\n2. **Model Observability**: How challenging is it to monitor multiple models in production? \n3. **Fallback and Redundancy**: When a model fails or underperforms, what’s your approach? Do you use fallback models, and if so, what would make managing them easier?\n4. **User and Permission Management**: For those supporting multiple teams or clients, how do you manage access across projects securely? Any struggles with multi-tenant support?\n\nThanks so much for sharing your experiences—I’m excited to hear your insights!",
            "author_fullname": "t2_b6i1si35",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] What Are Your Biggest Pain Points in Managing and Scaling Multiple AI Models?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkdyo8",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.22,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730832233.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/MachineLearning\"&gt;r/MachineLearning&lt;/a&gt;! 👋&lt;/p&gt;\n\n&lt;p&gt;I’m doing some research to understand the key challenges people face when managing multiple AI models—particularly around scaling, monitoring performance, and handling model failures. I’d love to hear from the community to get a better sense of where the pain points are.&lt;/p&gt;\n\n&lt;p&gt;Here are a few questions to start:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Scaling and Load Balancing&lt;/strong&gt;: Do you find it difficult to scale models for high traffic or load balancing between models?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Model Observability&lt;/strong&gt;: How challenging is it to monitor multiple models in production? &lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Fallback and Redundancy&lt;/strong&gt;: When a model fails or underperforms, what’s your approach? Do you use fallback models, and if so, what would make managing them easier?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;User and Permission Management&lt;/strong&gt;: For those supporting multiple teams or clients, how do you manage access across projects securely? Any struggles with multi-tenant support?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thanks so much for sharing your experiences—I’m excited to hear your insights!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkdyo8",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "BuddahJuddah",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkdyo8/r_what_are_your_biggest_pain_points_in_managing/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkdyo8/r_what_are_your_biggest_pain_points_in_managing/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730832233.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi!\n\nSo ElevenLabs has a pretty good audio isolation API but it is really expensive. Are there any opensource models that can be self-hosted and get near the same quality?",
            "author_fullname": "t2_oj9qv",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Voice Isolation",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gkdx19",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730832119.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi!&lt;/p&gt;\n\n&lt;p&gt;So ElevenLabs has a pretty good audio isolation API but it is really expensive. Are there any opensource models that can be self-hosted and get near the same quality?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gkdx19",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "aszx789",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gkdx19/d_voice_isolation/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gkdx19/d_voice_isolation/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730832119.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "",
            "author_fullname": "t2_10skoo",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Ask, and it shall be given: Turing completeness of prompting",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gk9hlk",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.32,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "default",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "mod_note": null,
            "created": 1730821031.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "arxiv.org",
            "allow_live_comments": false,
            "selftext_html": null,
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://arxiv.org/abs/2411.01992",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gk9hlk",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "CommunismDoesntWork",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gk9hlk/r_ask_and_it_shall_be_given_turing_completeness/",
            "stickied": false,
            "url": "https://arxiv.org/abs/2411.01992",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730821031.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "LLMs are usually evaluated on benchmarks that aim to measure broad abilities. However, most publishers of foundational models do not publish the actual cross-entropy loss value that the model achieves at the end of training. I couldn't find any sources on this, but I would like to know what loss value the LLMs can achieve on human language. Is there anyone who knows more about this? Might there be some lower bound?",
            "author_fullname": "t2_oktbx",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] To what cross-entropy loss value can LLMs converge?",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gk92rs",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.9,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 32,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 32,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730819960.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LLMs are usually evaluated on benchmarks that aim to measure broad abilities. However, most publishers of foundational models do not publish the actual cross-entropy loss value that the model achieves at the end of training. I couldn&amp;#39;t find any sources on this, but I would like to know what loss value the LLMs can achieve on human language. Is there anyone who knows more about this? Might there be some lower bound?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gk92rs",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "cbl007",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gk92rs/d_to_what_crossentropy_loss_value_can_llms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gk92rs/d_to_what_crossentropy_loss_value_can_llms/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730819960.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "https://arxiv.org/pdf/2310.02980 \n\nThe authors show that when transformers are pre trained, they can match the performance with S4 on the Long range Arena benchmark. ",
            "author_fullname": "t2_6k7647ey",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[R] Never Train from scratch",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "three",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gk7dny",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.87,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 110,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Research",
            "can_mod_post": false,
            "score": 110,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730815363.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2310.02980\"&gt;https://arxiv.org/pdf/2310.02980&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;The authors show that when transformers are pre trained, they can match the performance with S4 on the Long range Arena benchmark. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gk7dny",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Whatever_635",
            "discussion_type": null,
            "num_comments": 31,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gk7dny/r_never_train_from_scratch/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gk7dny/r_never_train_from_scratch/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730815363.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "Hi everyone,\n\nI'm facing a frustrating issue with my Python script. I'm processing prices and quantities in a DataFrame, using them to calculate unit prices, and saving the result to a CSV file. Everything seems perfect in Python (correct calculations, high precision), but when I open the CSV file, the values—particularly in the `\"Unit Prices\"` column—are incorrect (usually divided by 1000) or rounded, even though I specified high precision.\n\nA few details:\n\n   * I use `pd.to_csv()` with `decimal='.'` to ensure dot-based decimal formatting.\n   * I'm not specifying a `float_format`, aiming to retain maximum precision for `Unit Prices`.\n   * The data preview in Python shows the correct values before saving, but the saved CSV has discrepancies.\n\n**Example Output**: Here’s an example of what I'm seeing:\n\n* Python Output (before saving to CSV): `Unit Prices = 0.696`\n* CSV Output (opened in Excel): `Unit Prices = 696`\n\n  \nThe weird thing is that this does not happen consistently. In some cases, rows are correct.\n\nHas anyone faced this issue before? Any tips on ensuring that the CSV retains the exact precision and format as seen in Python?",
            "author_fullname": "t2_9cjb5thn",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[P] Getting crazy over a simple problem related to csv formatting",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "four",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gk5gr5",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Project",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730809561.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m facing a frustrating issue with my Python script. I&amp;#39;m processing prices and quantities in a DataFrame, using them to calculate unit prices, and saving the result to a CSV file. Everything seems perfect in Python (correct calculations, high precision), but when I open the CSV file, the values—particularly in the &lt;code&gt;&amp;quot;Unit Prices&amp;quot;&lt;/code&gt; column—are incorrect (usually divided by 1000) or rounded, even though I specified high precision.&lt;/p&gt;\n\n&lt;p&gt;A few details:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I use &lt;code&gt;pd.to_csv()&lt;/code&gt; with &lt;code&gt;decimal=&amp;#39;.&amp;#39;&lt;/code&gt; to ensure dot-based decimal formatting.&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m not specifying a &lt;code&gt;float_format&lt;/code&gt;, aiming to retain maximum precision for &lt;code&gt;Unit Prices&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;The data preview in Python shows the correct values before saving, but the saved CSV has discrepancies.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Example Output&lt;/strong&gt;: Here’s an example of what I&amp;#39;m seeing:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Python Output (before saving to CSV): &lt;code&gt;Unit Prices = 0.696&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;CSV Output (opened in Excel): &lt;code&gt;Unit Prices = 696&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The weird thing is that this does not happen consistently. In some cases, rows are correct.&lt;/p&gt;\n\n&lt;p&gt;Has anyone faced this issue before? Any tips on ensuring that the CSV retains the exact precision and format as seen in Python?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gk5gr5",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "No_Possibility_7588",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gk5gr5/p_getting_crazy_over_a_simple_problem_related_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gk5gr5/p_getting_crazy_over_a_simple_problem_related_to/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730809561.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    },
    {
        "kind": "t3",
        "data": {
            "approved_at_utc": null,
            "subreddit": "MachineLearning",
            "selftext": "I need to make a text classifier at work. I have 200 examples for each of the 5 categories. Each example is an email. Two approaches:\n\n* Classifying emails with n-shot prompt classification, possibly with LoRA finetuning.\n* Use a pre-trained text embedder (e.g. a sentence transformer or OpenAI text-embeddings-3) and a classification head. Train the classifier on the text embeddings.\n\nWhich approach is best?",
            "author_fullname": "t2_15tppnl92i",
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[D] Text classification: N-shot prompt classification VS training a linear classifier on top of an embedder",
            "link_flair_richtext": [],
            "subreddit_name_prefixed": "r/MachineLearning",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "one",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1gk4wx1",
            "quarantine": false,
            "link_flair_text_color": null,
            "upvote_ratio": 0.78,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "user_reports": [],
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1730807612.0,
            "link_flair_type": "text",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.MachineLearning",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to make a text classifier at work. I have 200 examples for each of the 5 categories. Each example is an email. Two approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Classifying emails with n-shot prompt classification, possibly with LoRA finetuning.&lt;/li&gt;\n&lt;li&gt;Use a pre-trained text embedder (e.g. a sentence transformer or OpenAI text-embeddings-3) and a classification head. Train the classifier on the text embeddings.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Which approach is best?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_2r3gv",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": null,
            "id": "1gk4wx1",
            "is_robot_indexable": true,
            "report_reasons": null,
            "author": "Aromatic-Oil-4586",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "contest_mode": false,
            "mod_reports": [],
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/MachineLearning/comments/1gk4wx1/d_text_classification_nshot_prompt_classification/",
            "stickied": false,
            "url": "https://www.reddit.com/r/MachineLearning/comments/1gk4wx1/d_text_classification_nshot_prompt_classification/",
            "subreddit_subscribers": 2934446,
            "created_utc": 1730807612.0,
            "num_crossposts": 0,
            "media": null,
            "is_video": false
        }
    }
]